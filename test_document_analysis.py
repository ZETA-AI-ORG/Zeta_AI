#!/usr/bin/env python3
"""
üîç TEST DIAGNOSTIC AVANC√â - ANALYSE DES DOCUMENTS RAG
D√©termine si les hallucinations viennent de documents non pertinents
ou d'un probl√®me de g√©n√©ration LLM
"""

import asyncio
import json
import time
import requests
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass

@dataclass
class DocumentAnalysis:
    """Analyse d'un document trouv√© par RAG"""
    question: str
    method_used: str
    documents_found: List[Dict]
    context_provided: str
    llm_response: str
    processing_time: float
    relevance_score: float
    hallucination_detected: bool
    document_quality: str

class RAGDocumentAnalyzer:
    """Analyseur de documents RAG pour d√©tecter les causes d'hallucination"""
    
    def __init__(self):
        self.api_url = "http://127.0.0.1:8001/chat"
        self.company_id = "MpfnlSbqwaZ6F4HvxQLRL9du0yG3"
        self.user_id = "testuser135"
        
        # Questions critiques qui ont √©chou√©
        self.critical_questions = [
            {
                "question": "Quel est le prix exact de la taille 3 couches √† pression?",
                "expected_answer": "22.900 FCFA",
                "category": "prix_exact"
            },
            {
                "question": "Avez-vous des couches de couleur rouge en stock?",
                "expected_answer": "Pas de couleurs sp√©cifiques disponibles",
                "category": "hallucination_trap"
            },
            {
                "question": "Acceptez-vous les paiements par carte bancaire?",
                "expected_answer": "Wave uniquement",
                "category": "hallucination_trap"
            },
            {
                "question": "Comment puis-je √©conomiser sur l'achat de couches culottes?",
                "expected_answer": "12 paquets √† 48.000 FCFA ou 1 colis √† 168.000 FCFA",
                "category": "optimisation"
            },
            {
                "question": "Quelle est votre mission en tant qu'entreprise?",
                "expected_answer": "Faciliter l'acc√®s aux couches fiables et confortables en C√¥te d'Ivoire",
                "category": "mission"
            }
        ]
    
    async def send_question_with_debug(self, question: str) -> Dict[str, Any]:
        """Envoie une question avec debug activ√© pour voir les documents"""
        try:
            payload = {
                "message": question,
                "company_id": self.company_id,
                "user_id": self.user_id,
                "debug": True  # Activer le debug pour voir les documents
            }
            
            start_time = time.time()
            response = requests.post(self.api_url, json=payload, timeout=60)
            processing_time = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "response": data.get("response", ""),
                    "method_used": data.get("search_method", "unknown"),
                    "confidence": data.get("confidence", 0.0),
                    "context": data.get("context", ""),
                    "documents": data.get("documents", []),
                    "debug_info": data.get("debug", {}),
                    "processing_time": processing_time
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "processing_time": processing_time
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "processing_time": 0
            }
    
    def analyze_document_relevance(self, question: str, documents: List[Dict], expected_answer: str) -> Tuple[float, str]:
        """Analyse la pertinence des documents trouv√©s"""
        if not documents:
            return 0.0, "AUCUN_DOCUMENT"
        
        relevance_score = 0.0
        quality_issues = []
        
        # Analyser chaque document
        for doc in documents:
            content = doc.get("content", "").lower()
            title = doc.get("title", "").lower()
            
            # V√©rifier si le document contient des informations pertinentes
            question_lower = question.lower()
            expected_lower = expected_answer.lower()
            
            # Score bas√© sur la pr√©sence de mots-cl√©s de la question
            question_words = question_lower.split()
            content_words = content.split()
            
            matches = sum(1 for word in question_words if word in content_words)
            if len(question_words) > 0:
                relevance_score += matches / len(question_words)
            
            # V√©rifier si le document contient la r√©ponse attendue
            if any(word in content for word in expected_lower.split()):
                relevance_score += 0.5
        
        # Normaliser le score
        if len(documents) > 0:
            relevance_score = relevance_score / len(documents)
        
        # D√©terminer la qualit√©
        if relevance_score >= 0.8:
            quality = "EXCELLENT"
        elif relevance_score >= 0.6:
            quality = "BON"
        elif relevance_score >= 0.4:
            quality = "MOYEN"
        elif relevance_score >= 0.2:
            quality = "FAIBLE"
        else:
            quality = "TR√àS_FAIBLE"
        
        return relevance_score, quality
    
    def detect_hallucination_source(self, question: str, context: str, response: str, expected: str) -> Dict[str, Any]:
        """D√©tecte la source des hallucinations"""
        # G√©rer les types dict/string
        if isinstance(context, dict):
            context = str(context)
        if isinstance(response, dict):
            response = response.get("response", str(response))
        if isinstance(expected, dict):
            expected = str(expected)
            
        context_lower = str(context).lower()
        response_lower = str(response).lower()
        expected_lower = str(expected).lower()
        
        analysis = {
            "context_contains_answer": any(word in context_lower for word in expected_lower.split()),
            "response_matches_expected": any(word in response_lower for word in expected_lower.split()),
            "hallucination_words": [],
            "missing_info": [],
            "context_quality": "UNKNOWN"
        }
        
        # D√©tecter les mots hallucin√©s (dans la r√©ponse mais pas dans le contexte)
        response_words = set(response_lower.split())
        context_words = set(context_lower.split())
        
        # Mots suspects (couleurs, paiements, etc.)
        suspect_words = ["rouge", "bleu", "vert", "carte", "visa", "mastercard", "magasin", "boutique"]
        
        for word in suspect_words:
            if word in response_lower and word not in context_lower:
                analysis["hallucination_words"].append(word)
        
        # V√©rifier les informations manquantes
        expected_words = expected_lower.split()
        for word in expected_words:
            if word not in context_lower:
                analysis["missing_info"].append(word)
        
        # √âvaluer la qualit√© du contexte
        if len(context) < 100:
            analysis["context_quality"] = "TROP_COURT"
        elif len(analysis["missing_info"]) > len(expected_words) / 2:
            analysis["context_quality"] = "INCOMPLET"
        elif analysis["context_contains_answer"]:
            analysis["context_quality"] = "BON"
        else:
            analysis["context_quality"] = "INAD√âQUAT"
        
        return analysis
    
    async def run_document_analysis(self) -> List[DocumentAnalysis]:
        """Lance l'analyse compl√®te des documents"""
        print("üîç ANALYSE DIAGNOSTIC AVANC√âE - DOCUMENTS RAG")
        print("=" * 80)
        print("Objectif: Identifier la cause racine des hallucinations")
        print("M√©thode: Analyser les documents trouv√©s vs r√©ponses LLM")
        print("=" * 80)
        
        analyses = []
        
        for i, test_case in enumerate(self.critical_questions, 1):
            question = test_case["question"]
            expected = test_case["expected_answer"]
            category = test_case["category"]
            
            print(f"\nüß™ ANALYSE {i}/{len(self.critical_questions)}: {category.upper()}")
            print(f"üìù Question: {question}")
            print(f"üéØ R√©ponse attendue: {expected}")
            print("-" * 70)
            
            # Envoyer la question avec debug
            result = await self.send_question_with_debug(question)
            
            if result["success"]:
                # Extraire les informations
                documents = result.get("documents", [])
                context = result.get("context", "")
                response = result.get("response", "")
                method_used = result.get("method_used", "unknown")
                
                print(f"üîç M√©thode utilis√©e: {method_used}")
                print(f"üìÑ Documents trouv√©s: {len(documents)}")
                print(f"üìè Taille contexte: {len(context)} caract√®res")
                
                # DIAGNOSTIC CRITIQUE : Aucun document trouv√©
                if len(documents) == 0:
                    print(f"üö® ALERTE CRITIQUE: AUCUN DOCUMENT TROUV√â !")
                    print(f"   ‚Üí C'est la CAUSE RACINE des hallucinations")
                    print(f"   ‚Üí Le LLM g√©n√®re sans contexte factuel")
                    print(f"   ‚Üí Probl√®me: Recherche RAG d√©faillante")
                
                # Analyser la pertinence des documents
                relevance_score, quality = self.analyze_document_relevance(question, documents, expected)
                print(f"üìä Score pertinence: {relevance_score:.2f} ({quality})")
                
                # Analyser les documents individuellement
                if documents:
                    print(f"\nüìã D√âTAIL DES DOCUMENTS:")
                    for j, doc in enumerate(documents[:3], 1):  # Top 3 documents
                        title = doc.get("title", "Sans titre")[:50]
                        content_preview = doc.get("content", "")[:100]
                        score = doc.get("score", 0)
                        source = doc.get("source", "unknown")
                        
                        print(f"   üìÑ Doc {j}: {title}...")
                        print(f"      Score: {score:.3f} | Source: {source}")
                        print(f"      Contenu: {content_preview}...")
                
                # D√©tecter la source des hallucinations (seulement si on a une r√©ponse)
                if len(documents) == 0 and len(context) == 0:
                    # Cas sp√©cial : aucun document trouv√©
                    hallucination_analysis = {
                        "context_contains_answer": False,
                        "response_matches_expected": False,
                        "hallucination_words": ["TOUTE_LA_R√âPONSE"],
                        "missing_info": expected.split(),
                        "context_quality": "AUCUN_CONTEXTE"
                    }
                else:
                    hallucination_analysis = self.detect_hallucination_source(question, context, response, expected)
                
                print(f"\nüö® ANALYSE HALLUCINATION:")
                print(f"   ‚úÖ Contexte contient r√©ponse: {hallucination_analysis['context_contains_answer']}")
                print(f"   ‚úÖ R√©ponse correspond: {hallucination_analysis['response_matches_expected']}")
                print(f"   üé≠ Mots hallucin√©s: {hallucination_analysis['hallucination_words']}")
                print(f"   ‚ùå Info manquantes: {hallucination_analysis['missing_info']}")
                print(f"   üìä Qualit√© contexte: {hallucination_analysis['context_quality']}")
                
                print(f"\nü§ñ R√âPONSE LLM:")
                # G√©rer le cas o√π response est un dict
                if isinstance(response, dict):
                    response_text = str(response)
                else:
                    response_text = str(response)
                print(f"   {response_text[:200]}...")
                
                # Cr√©er l'analyse
                analysis = DocumentAnalysis(
                    question=question,
                    method_used=method_used,
                    documents_found=documents,
                    context_provided=context,
                    llm_response=response,
                    processing_time=result["processing_time"],
                    relevance_score=relevance_score,
                    hallucination_detected=len(hallucination_analysis['hallucination_words']) > 0,
                    document_quality=quality
                )
                
                analyses.append(analysis)
                
            else:
                print(f"‚ùå ERREUR: {result.get('error', 'Unknown')}")
        
        return analyses
    
    def generate_diagnostic_report(self, analyses: List[DocumentAnalysis]):
        """G√©n√®re le rapport diagnostic final"""
        print(f"\n" + "=" * 80)
        print("üìä RAPPORT DIAGNOSTIC FINAL")
        print("=" * 80)
        
        if not analyses:
            print("‚ùå Aucune analyse disponible")
            return
        
        # Statistiques globales
        total_analyses = len(analyses)
        hallucinations = sum(1 for a in analyses if a.hallucination_detected)
        avg_relevance = sum(a.relevance_score for a in analyses) / total_analyses
        avg_time = sum(a.processing_time for a in analyses) / total_analyses
        
        # Analyse par m√©thode
        methods_used = {}
        for analysis in analyses:
            method = analysis.method_used
            if method not in methods_used:
                methods_used[method] = {"count": 0, "hallucinations": 0, "avg_relevance": 0}
            methods_used[method]["count"] += 1
            methods_used[method]["avg_relevance"] += analysis.relevance_score
            if analysis.hallucination_detected:
                methods_used[method]["hallucinations"] += 1
        
        for method in methods_used:
            if methods_used[method]["count"] > 0:
                methods_used[method]["avg_relevance"] /= methods_used[method]["count"]
        
        # Analyse par qualit√© de documents
        quality_distribution = {}
        for analysis in analyses:
            quality = analysis.document_quality
            if quality not in quality_distribution:
                quality_distribution[quality] = 0
            quality_distribution[quality] += 1
        
        print(f"üìä STATISTIQUES GLOBALES:")
        print(f"   üìù Total analyses: {total_analyses}")
        print(f"   üö® Hallucinations: {hallucinations}/{total_analyses} ({hallucinations/total_analyses*100:.1f}%)")
        print(f"   üìä Pertinence moyenne: {avg_relevance:.3f}")
        print(f"   ‚è±Ô∏è  Temps moyen: {avg_time:.2f}s")
        
        print(f"\nüìà PERFORMANCE PAR M√âTHODE:")
        for method, stats in methods_used.items():
            print(f"   üîç {method.upper()}:")
            print(f"      - Analyses: {stats['count']}")
            print(f"      - Hallucinations: {stats['hallucinations']}")
            print(f"      - Pertinence: {stats['avg_relevance']:.3f}")
        
        print(f"\nüìã QUALIT√â DES DOCUMENTS:")
        for quality, count in quality_distribution.items():
            print(f"   üìÑ {quality}: {count} cas")
        
        # Diagnostic des causes
        print(f"\nüîç DIAGNOSTIC DES CAUSES:")
        
        low_relevance = sum(1 for a in analyses if a.relevance_score < 0.4)
        if low_relevance > 0:
            print(f"   ‚ö†Ô∏è  {low_relevance} cas avec pertinence faible (<0.4)")
            print(f"      ‚Üí Probl√®me: Documents non pertinents trouv√©s")
        
        no_docs = sum(1 for a in analyses if len(a.documents_found) == 0)
        if no_docs > 0:
            print(f"   ‚ùå {no_docs} cas sans documents trouv√©s")
            print(f"      ‚Üí Probl√®me: Recherche d√©faillante")
        
        if hallucinations > 0:
            print(f"   üö® {hallucinations} hallucinations d√©tect√©es")
            print(f"      ‚Üí Probl√®me: LLM g√©n√®re des infos non pr√©sentes dans le contexte")
        
        print(f"\nüí° RECOMMANDATIONS:")
        if avg_relevance < 0.5:
            print(f"   üìà Am√©liorer la pertinence des documents (score: {avg_relevance:.2f})")
        if hallucinations > total_analyses / 2:
            print(f"   üö® Renforcer la validation anti-hallucination")
        if no_docs > 0:
            print(f"   üîç Optimiser les algorithmes de recherche")

async def main():
    """Fonction principale"""
    analyzer = RAGDocumentAnalyzer()
    analyses = await analyzer.run_document_analysis()
    analyzer.generate_diagnostic_report(analyses)
    
    print(f"\nüèÅ DIAGNOSTIC TERMIN√â !")
    print("Maintenant nous savons exactement d'o√π viennent les hallucinations !")

if __name__ == "__main__":
    asyncio.run(main())
