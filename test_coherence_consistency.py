#!/usr/bin/env python3
"""
üîÑ TESTS DE COH√âRENCE & CONSISTANCE - CHATBOT RUE_DU_GROS
========================================================
Tests de paraphrases et v√©rification de la consistance des r√©ponses
Validation de la coh√©rence s√©mantique
"""

import asyncio
import aiohttp
import time
import json
from datetime import datetime
import difflib
from collections import defaultdict

# Configuration
ENDPOINT_URL = "http://127.0.0.1:8001/chat"
COMPANY_ID = "MpfnlSbqwaZ6F4HvxQLRL9du0yG3"

class CoherenceTester:
    def __init__(self):
        self.results = []
        self.inconsistencies = []
        
    # Tests de paraphrases - m√™me question formul√©e diff√©remment
    PARAPHRASE_TESTS = [
        {
            "name": "Prix Couches Taille 1",
            "variations": [
                "combien co√ªtent les couches taille 1",
                "quel est le prix des couches pour nouveau-n√© taille 1",
                "prix couches taille 1 nouveau-n√©",
                "tarif couches taille 1"
            ],
            "expected_consistency": "price_info",
            "key_elements": ["17.900", "taille 1", "0 √† 4 kg", "300 couches"]
        },
        {
            "name": "Livraison Cocody",
            "variations": [
                "livraison √† Cocody combien √ßa co√ªte",
                "vous livrez √† Cocody quel tarif",
                "frais de livraison Cocody",
                "co√ªt livraison zone Cocody"
            ],
            "expected_consistency": "delivery_info",
            "key_elements": ["cocody", "1500", "fcfa", "livraison"]
        },
        {
            "name": "Paiement Wave",
            "variations": [
                "je peux payer avec wave money",
                "vous acceptez wave comme paiement",
                "paiement par wave possible",
                "wave money accept√©"
            ],
            "expected_consistency": "payment_method",
            "key_elements": ["wave", "accept√©", "paiement", "+2250787360757"]
        },
        {
            "name": "Contact WhatsApp",
            "variations": [
                "num√©ro whatsapp pour commander",
                "votre whatsapp c'est quoi",
                "contact whatsapp commande",
                "num√©ro whatsapp entreprise"
            ],
            "expected_consistency": "contact_info",
            "key_elements": ["+2250160924560", "whatsapp", "commander"]
        }
    ]
    
    # Tests de consistance temporelle - m√™me question √† diff√©rents moments
    TEMPORAL_CONSISTENCY_TESTS = [
        {
            "name": "Stabilit√© Prix Taille 4",
            "query": "prix couches taille 4 pour enfant 10 kg",
            "repetitions": 5,
            "expected_consistency": "stable_pricing",
            "key_elements": ["25.900", "taille 4", "9 √† 14 kg"]
        },
        {
            "name": "Stabilit√© Politique Acompte",
            "query": "faut-il payer un acompte pour commander",
            "repetitions": 5,
            "expected_consistency": "stable_policy",
            "key_elements": ["acompte", "2000", "fcfa", "obligatoire"]
        },
        {
            "name": "Stabilit√© Horaires",
            "query": "√† quelle heure je peux vous contacter",
            "repetitions": 3,
            "expected_consistency": "stable_hours",
            "key_elements": ["toujours ouvert", "contact"]
        }
    ]
    
    # Tests de coh√©rence contextuelle
    CONTEXTUAL_COHERENCE_TESTS = [
        {
            "name": "Calcul Total Commande",
            "queries": [
                "prix 2 paquets couches culottes",
                "livraison √† Marcory combien",
                "total pour 2 paquets culottes + livraison Marcory"
            ],
            "expected_consistency": "mathematical_coherence",
            "validation_type": "calculation"
        },
        {
            "name": "Comparaison Tailles",
            "queries": [
                "prix couches taille 3",
                "prix couches taille 6", 
                "diff√©rence prix entre taille 3 et taille 6"
            ],
            "expected_consistency": "comparative_coherence",
            "validation_type": "comparison"
        }
    ]

    async def run_paraphrase_tests(self, session):
        """Tests de paraphrases pour v√©rifier la consistance"""
        print(f"\nüîÑ TESTS DE PARAPHRASES")
        print("="*50)
        
        paraphrase_results = []
        
        for test_group in self.PARAPHRASE_TESTS:
            print(f"\nüìù {test_group['name']}")
            
            responses = []
            for i, variation in enumerate(test_group['variations'], 1):
                print(f"  {i}. '{variation}'")
                
                success, response, duration = await self.send_request(session, variation)
                
                if success:
                    responses.append({
                        'variation': variation,
                        'response': response,
                        'duration': duration
                    })
                    print(f"     ‚úÖ {duration:.1f}ms")
                else:
                    print(f"     ‚ùå Erreur: {response}")
                
                await asyncio.sleep(0.5)
            
            # Analyse de coh√©rence
            if len(responses) >= 2:
                consistency_score, inconsistencies = self.analyze_paraphrase_consistency(
                    test_group, responses
                )
                
                result = {
                    'test_name': test_group['name'],
                    'type': 'paraphrase',
                    'variations_tested': len(responses),
                    'consistency_score': consistency_score,
                    'responses': responses,
                    'inconsistencies': inconsistencies,
                    'key_elements': test_group['key_elements']
                }
                
                paraphrase_results.append(result)
                
                print(f"  üìä Score coh√©rence: {consistency_score:.1f}%")
                if inconsistencies:
                    print(f"  ‚ö†Ô∏è Incoh√©rences: {len(inconsistencies)}")
                    for inc in inconsistencies:
                        print(f"    ‚Ä¢ {inc}")
        
        return paraphrase_results

    async def run_temporal_consistency_tests(self, session):
        """Tests de consistance temporelle"""
        print(f"\n‚è∞ TESTS DE CONSISTANCE TEMPORELLE")
        print("="*50)
        
        temporal_results = []
        
        for test in self.TEMPORAL_CONSISTENCY_TESTS:
            print(f"\nüîÅ {test['name']} ({test['repetitions']} r√©p√©titions)")
            
            responses = []
            for i in range(test['repetitions']):
                print(f"  R√©p√©tition {i+1}/{test['repetitions']}")
                
                success, response, duration = await self.send_request(session, test['query'])
                
                if success:
                    responses.append({
                        'repetition': i+1,
                        'response': response,
                        'duration': duration,
                        'timestamp': time.time()
                    })
                    print(f"    ‚úÖ {duration:.1f}ms")
                else:
                    print(f"    ‚ùå Erreur: {response}")
                
                await asyncio.sleep(2)  # Pause entre r√©p√©titions
            
            # Analyse de stabilit√©
            if len(responses) >= 2:
                stability_score, variations = self.analyze_temporal_stability(
                    test, responses
                )
                
                result = {
                    'test_name': test['name'],
                    'type': 'temporal',
                    'repetitions': len(responses),
                    'stability_score': stability_score,
                    'responses': responses,
                    'variations': variations,
                    'key_elements': test['key_elements']
                }
                
                temporal_results.append(result)
                
                print(f"  üìä Score stabilit√©: {stability_score:.1f}%")
                if variations:
                    print(f"  üîÑ Variations d√©tect√©es: {len(variations)}")
        
        return temporal_results

    async def run_contextual_coherence_tests(self, session):
        """Tests de coh√©rence contextuelle"""
        print(f"\nüß© TESTS DE COH√âRENCE CONTEXTUELLE")
        print("="*50)
        
        contextual_results = []
        
        for test in self.CONTEXTUAL_COHERENCE_TESTS:
            print(f"\nüîó {test['name']}")
            
            responses = []
            for i, query in enumerate(test['queries'], 1):
                print(f"  {i}. '{query}'")
                
                success, response, duration = await self.send_request(session, query)
                
                if success:
                    responses.append({
                        'query': query,
                        'response': response,
                        'duration': duration
                    })
                    print(f"     ‚úÖ {duration:.1f}ms")
                else:
                    print(f"     ‚ùå Erreur: {response}")
                
                await asyncio.sleep(1)
            
            # Analyse de coh√©rence contextuelle
            if len(responses) >= 2:
                coherence_score, issues = self.analyze_contextual_coherence(
                    test, responses
                )
                
                result = {
                    'test_name': test['name'],
                    'type': 'contextual',
                    'queries_tested': len(responses),
                    'coherence_score': coherence_score,
                    'responses': responses,
                    'coherence_issues': issues,
                    'validation_type': test['validation_type']
                }
                
                contextual_results.append(result)
                
                print(f"  üìä Score coh√©rence: {coherence_score:.1f}%")
                if issues:
                    print(f"  ‚ö†Ô∏è Probl√®mes: {len(issues)}")
        
        return contextual_results

    def analyze_paraphrase_consistency(self, test_group, responses):
        """Analyse la consistance entre paraphrases"""
        consistency_score = 100.0
        inconsistencies = []
        
        # V√©rification des √©l√©ments cl√©s
        key_elements = test_group['key_elements']
        element_presence = defaultdict(list)
        
        for resp in responses:
            response_lower = resp['response'].lower()
            for element in key_elements:
                if element.lower() in response_lower:
                    element_presence[element].append(resp['variation'])
        
        # V√©rifier que tous les √©l√©ments cl√©s sont pr√©sents dans toutes les r√©ponses
        for element in key_elements:
            presence_rate = len(element_presence[element]) / len(responses)
            if presence_rate < 0.8:  # 80% minimum
                inconsistencies.append(f"√âl√©ment '{element}' manquant dans {(1-presence_rate)*100:.0f}% des r√©ponses")
                consistency_score -= 20
        
        # V√©rification de la similarit√© s√©mantique
        response_texts = [r['response'] for r in responses]
        similarity_scores = []
        
        for i in range(len(response_texts)):
            for j in range(i+1, len(response_texts)):
                similarity = difflib.SequenceMatcher(None, response_texts[i], response_texts[j]).ratio()
                similarity_scores.append(similarity)
        
        if similarity_scores:
            avg_similarity = sum(similarity_scores) / len(similarity_scores)
            if avg_similarity < 0.6:  # 60% minimum de similarit√©
                inconsistencies.append(f"Faible similarit√© s√©mantique: {avg_similarity*100:.1f}%")
                consistency_score -= (0.6 - avg_similarity) * 100
        
        return max(0, consistency_score), inconsistencies

    def analyze_temporal_stability(self, test, responses):
        """Analyse la stabilit√© temporelle des r√©ponses"""
        stability_score = 100.0
        variations = []
        
        # Comparaison des r√©ponses successives
        response_texts = [r['response'] for r in responses]
        
        for i in range(1, len(response_texts)):
            similarity = difflib.SequenceMatcher(None, response_texts[0], response_texts[i]).ratio()
            
            if similarity < 0.8:  # 80% minimum de similarit√©
                variations.append({
                    'repetition': i+1,
                    'similarity': similarity,
                    'description': f"Variation significative par rapport √† la premi√®re r√©ponse"
                })
                stability_score -= (0.8 - similarity) * 100
        
        # V√©rification des √©l√©ments cl√©s
        key_elements = test['key_elements']
        for element in key_elements:
            element_count = sum(1 for resp in responses if element.lower() in resp['response'].lower())
            consistency_rate = element_count / len(responses)
            
            if consistency_rate < 1.0:
                variations.append({
                    'element': element,
                    'consistency_rate': consistency_rate,
                    'description': f"√âl√©ment '{element}' absent dans {(1-consistency_rate)*100:.0f}% des r√©p√©titions"
                })
                stability_score -= (1.0 - consistency_rate) * 20
        
        return max(0, stability_score), variations

    def analyze_contextual_coherence(self, test, responses):
        """Analyse la coh√©rence contextuelle"""
        coherence_score = 100.0
        issues = []
        
        validation_type = test['validation_type']
        
        if validation_type == "calculation":
            # V√©rifier la coh√©rence math√©matique
            issues.extend(self.validate_mathematical_coherence(responses))
        
        elif validation_type == "comparison":
            # V√©rifier la coh√©rence comparative
            issues.extend(self.validate_comparative_coherence(responses))
        
        # P√©nalit√© par probl√®me d√©tect√©
        coherence_score -= len(issues) * 25
        
        return max(0, coherence_score), issues

    def validate_mathematical_coherence(self, responses):
        """Valide la coh√©rence math√©matique"""
        issues = []
        
        # Extraction des prix mentionn√©s
        import re
        price_pattern = r'(\d+\.?\d*)\s*(?:fcfa|f\s*cfa)'
        
        extracted_prices = {}
        for resp in responses:
            prices = re.findall(price_pattern, resp['response'].lower())
            if prices:
                extracted_prices[resp['query']] = [float(p.replace('.', '')) for p in prices]
        
        # V√©rification de coh√©rence (exemple basique)
        if len(extracted_prices) >= 3:
            # Logique de validation sp√©cifique selon le contexte
            issues.append("Validation math√©matique n√©cessite impl√©mentation sp√©cifique")
        
        return issues

    def validate_comparative_coherence(self, responses):
        """Valide la coh√©rence comparative"""
        issues = []
        
        # V√©rification que les comparaisons sont logiques
        # Impl√©mentation basique - peut √™tre √©tendue
        for resp in responses:
            if "diff√©rence" in resp['response'].lower():
                if not any(word in resp['response'].lower() for word in ['plus', 'moins', 'sup√©rieur', 'inf√©rieur']):
                    issues.append("Comparaison sans indication claire de direction")
        
        return issues

    async def send_request(self, session, query):
        """Envoie une requ√™te de test"""
        payload = {
            "message": query,
            "company_id": COMPANY_ID,
            "user_id": "coherencetest123"
        }
        
        start_time = time.time()
        try:
            async with session.post(ENDPOINT_URL, json=payload, timeout=30) as response:
                end_time = time.time()
                duration = (end_time - start_time) * 1000
                
                if response.status == 200:
                    response_text = await response.text()
                    try:
                        response_json = json.loads(response_text)
                        actual_response = response_json.get('response', response_text)
                    except:
                        actual_response = response_text
                    return True, actual_response, duration
                else:
                    error_text = await response.text()
                    return False, f"HTTP {response.status}: {error_text}", duration
                    
        except Exception as e:
            end_time = time.time()
            duration = (end_time - start_time) * 1000
            return False, str(e), duration

    async def run_all_tests(self):
        """Lance tous les tests de coh√©rence"""
        print("üîÑ TESTS DE COH√âRENCE & CONSISTANCE - RUE_DU_GROS")
        print("="*60)
        print(f"üéØ URL: {ENDPOINT_URL}")
        print(f"üè¢ Company ID: {COMPANY_ID}")
        print(f"üîç Tests de coh√©rence et consistance...")
        
        async with aiohttp.ClientSession() as session:
            all_results = []
            
            # Tests de paraphrases
            paraphrase_results = await self.run_paraphrase_tests(session)
            all_results.extend(paraphrase_results)
            
            # Tests de consistance temporelle
            temporal_results = await self.run_temporal_consistency_tests(session)
            all_results.extend(temporal_results)
            
            # Tests de coh√©rence contextuelle
            contextual_results = await self.run_contextual_coherence_tests(session)
            all_results.extend(contextual_results)
            
            self.results = all_results
            
        await self.generate_coherence_report()

    async def generate_coherence_report(self):
        """G√©n√®re le rapport de coh√©rence final"""
        print("\n" + "="*60)
        print("üîÑ RAPPORT DE COH√âRENCE FINAL")
        print("="*60)
        
        if not self.results:
            print("‚ùå Aucun r√©sultat √† analyser")
            return
        
        # Calcul des scores moyens par type
        type_scores = defaultdict(list)
        for result in self.results:
            test_type = result['type']
            if 'consistency_score' in result:
                type_scores[test_type].append(result['consistency_score'])
            elif 'stability_score' in result:
                type_scores[test_type].append(result['stability_score'])
            elif 'coherence_score' in result:
                type_scores[test_type].append(result['coherence_score'])
        
        overall_scores = []
        print(f"üìä SCORES PAR CAT√âGORIE:")
        for test_type, scores in type_scores.items():
            avg_score = sum(scores) / len(scores)
            overall_scores.extend(scores)
            status = "‚úÖ" if avg_score >= 90 else "üü°" if avg_score >= 70 else "üî¥"
            print(f"  {status} {test_type.title()}: {avg_score:.1f}% (sur {len(scores)} tests)")
        
        # Score global
        global_score = sum(overall_scores) / len(overall_scores) if overall_scores else 0
        print(f"\nüéØ SCORE GLOBAL DE COH√âRENCE: {global_score:.1f}%")
        
        # Analyse d√©taill√©e
        total_issues = 0
        for result in self.results:
            issues = (result.get('inconsistencies', []) + 
                     result.get('variations', []) + 
                     result.get('coherence_issues', []))
            total_issues += len(issues)
        
        print(f"‚ö†Ô∏è Total des probl√®mes d√©tect√©s: {total_issues}")
        
        # Verdict final
        print(f"\nüèÜ VERDICT COH√âRENCE:")
        if global_score >= 90:
            print("‚úÖ EXCELLENT - Tr√®s haute coh√©rence")
        elif global_score >= 75:
            print("üü° BON - Coh√©rence satisfaisante")
        elif global_score >= 60:
            print("üü† MOYEN - Am√©liorations n√©cessaires")
        else:
            print("üî¥ CRITIQUE - Probl√®mes de coh√©rence majeurs")
        
        # Recommandations
        print(f"\nüí° RECOMMANDATIONS:")
        if global_score >= 90:
            print("  ‚Ä¢ Excellente coh√©rence, maintenir la qualit√©")
        else:
            print("  ‚Ä¢ Am√©liorer la consistance des r√©ponses")
            print("  ‚Ä¢ Standardiser les informations cl√©s")
            print("  ‚Ä¢ Renforcer la validation des calculs")
            print("  ‚Ä¢ Tester r√©guli√®rement la stabilit√©")
        
        # Sauvegarde du rapport
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"coherence_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': {
                    'global_score': global_score,
                    'total_tests': len(self.results),
                    'total_issues': total_issues,
                    'timestamp': timestamp
                },
                'type_scores': {k: sum(v)/len(v) for k, v in type_scores.items()},
                'detailed_results': self.results
            }, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ Rapport sauvegard√©: {report_file}")

async def main():
    tester = CoherenceTester()
    await tester.run_all_tests()

if __name__ == "__main__":
    asyncio.run(main())
