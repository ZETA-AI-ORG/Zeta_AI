#!/usr/bin/env python3
"""
üåç RAG ENGINE UNIVERSEL FRANCOPHONE AVANC√â 2024
Pipeline complet : NLP ‚Üí Recherche Hybride ‚Üí Fusion ‚Üí Prompt Enrichi ‚Üí V√©rification QA
Architecture compl√®tement int√©gr√©e pour la compr√©hension fran√ßaise et l'e-commerce
"""

import asyncio
import time
import logging
import re
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

# Imports des modules avanc√©s
try:
    from .french_nlp_processor import FrenchNLPProcessor, ProcessedQuery
    from .semantic_search_engine import OptimizedSemanticSearchEngine, SearchConfig
    from .intelligent_fusion import IntelligentFusionEngine, FusionConfig, fuse_search_results
    from .prompt_enricher import PromptEnricher, enrich_llm_prompt
    from .response_verifier import ResponseVerifier, VerificationLevel, verify_llm_response
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Modules avanc√©s non disponibles: {e}")
    ADVANCED_MODULES_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class AdvancedRAGResult:
    """R√©sultat du RAG avanc√© avec m√©tadonn√©es compl√®tes"""
    response: str
    confidence: float
    documents_found: bool
    processing_time_ms: float
    search_method: str
    context_used: str
    # M√©tadonn√©es avanc√©es
    nlp_analysis: Optional[Dict[str, Any]] = None
    fusion_metadata: Optional[Dict[str, Any]] = None
    verification_result: Optional[Dict[str, Any]] = None
    quality_score: Optional[float] = None
    pipeline_stages: Optional[List[str]] = None

class AdvancedUniversalRAGEngine:
    """
    üåç RAG ENGINE UNIVERSEL FRANCOPHONE AVANC√â
    
    Architecture compl√®te :
    1. üá´üá∑ Pr√©traitement NLP fran√ßais (normalisation, lemmatisation, intentions, entit√©s)
    2. üîç Recherche hybride (MeiliSearch + S√©mantique + Fuzzy)
    3. üîÄ Fusion intelligente multi-sources
    4. üéØ Prompt enrichi avec intentions/entit√©s
    5. ü§ñ G√©n√©ration LLM optimis√©e
    6. ‚úÖ V√©rification QA et factualit√©
    7. üìä M√©triques et auditabilit√© compl√®tes
    """
    
    def __init__(self):
        self.llm_client = None
        self.embedding_model = None
        
        # Composants avanc√©s
        if ADVANCED_MODULES_AVAILABLE:
            self.nlp_processor = FrenchNLPProcessor()
            self.semantic_engine = OptimizedSemanticSearchEngine()
            self.fusion_engine = IntelligentFusionEngine()
            self.prompt_enricher = PromptEnricher()
            self.response_verifier = ResponseVerifier()
            logger.info("‚úÖ Architecture francophone avanc√©e initialis√©e")
        else:
            self.nlp_processor = None
            self.semantic_engine = None
            self.fusion_engine = None
            self.prompt_enricher = None
            self.response_verifier = None
            logger.warning("‚ö†Ô∏è Mode d√©grad√© - modules avanc√©s non disponibles")
        
        # Configuration par d√©faut
        self.search_config = SearchConfig(
            top_k=8,
            min_score=0.3,
            enable_reranking=True,
            max_context_length=4000
        )
        
        self.fusion_config = FusionConfig(
            meili_weight=0.4,
            semantic_weight=0.4,
            fuzzy_weight=0.2,
            boost_exact_matches=True,
            boost_multi_source=True
        )
        
        self.verification_level = VerificationLevel.STANDARD
    
    async def initialize(self):
        """Initialise tous les composants"""
        try:
            # Import dynamique pour √©viter les erreurs
            from .llm_client import GroqLLMClient
            from embedding_models import get_embedding_model
            
            self.llm_client = GroqLLMClient()
            self.embedding_model = get_embedding_model()
            
            # Initialisation des modules avanc√©s
            if ADVANCED_MODULES_AVAILABLE:
                if self.semantic_engine:
                    self.semantic_engine.initialize()
                logger.info("‚úÖ RAG Universel Francophone Avanc√© initialis√©")
            else:
                logger.info("‚úÖ RAG Universel (mode d√©grad√©) initialis√©")
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation: {e}")
            return False
    
    async def process_query_advanced(
        self, 
        query: str, 
        company_id: str, 
        user_id: str, 
        company_name: str = None
    ) -> AdvancedRAGResult:
        """
        üöÄ TRAITEMENT AVANC√â COMPLET D'UNE REQU√äTE
        
        Pipeline francophone int√©gral :
        1. üá´üá∑ Analyse NLP fran√ßaise compl√®te
        2. üîç Recherche hybride multi-sources
        3. üîÄ Fusion intelligente des r√©sultats
        4. üéØ Enrichissement du prompt
        5. ü§ñ G√©n√©ration LLM optimis√©e
        6. ‚úÖ V√©rification QA et factualit√©
        """
        start_time = time.time()
        pipeline_stages = []
        
        logger.info(f"üöÄ [ADVANCED_RAG] D√©but traitement: '{query[:50]}...'")
        logger.info(f"üè¢ Company: {company_id[:12]}... | User: {user_id[:12]}...")
        
        # Variables de r√©sultats
        nlp_analysis = None
        search_results = None
        fusion_metadata = None
        verification_result = None
        quality_score = None
        response = ""
        context_used = ""
        
        try:
            # Initialisation si n√©cessaire
            if not self.llm_client:
                await self.initialize()
            
            # ========== √âTAPE 1 : ANALYSE NLP FRAN√áAISE ==========
            pipeline_stages.append("nlp_analysis")
            processed_query = None
            
            if ADVANCED_MODULES_AVAILABLE and self.nlp_processor:
                try:
                    logger.info("üá´üá∑ [√âTAPE 1] Analyse NLP fran√ßaise...")
                    processed_query = self.nlp_processor.process_query(query)
                    
                    nlp_analysis = {
                        'original': processed_query.original,
                        'normalized': processed_query.normalized,
                        'lemmatized_words': processed_query.lemmatized_words,
                        'corrected': processed_query.corrected,
                        'intentions': processed_query.intentions,
                        'entities': processed_query.entities,
                        'split_queries': processed_query.split_queries,
                        'confidence': processed_query.confidence
                    }
                    
                    logger.info(f"‚úÖ [NLP] Analyse termin√©e: {len(processed_query.intentions)} intentions, {len(processed_query.entities)} entit√©s")
                    
                except Exception as e:
                    logger.error(f"‚ùå [NLP] Erreur: {e}")
                    processed_query = None
            
            # ========== √âTAPE 2 : RECHERCHE HYBRIDE ==========
            pipeline_stages.append("hybrid_search")
            
            if ADVANCED_MODULES_AVAILABLE and processed_query:
                search_results = await self._advanced_search_pipeline(query, company_id, processed_query)
                
                fusion_metadata = {
                    'search_methods': search_results.get('search_methods_used', []),
                    'total_sources': len(search_results.get('search_methods_used', [])),
                    'documents_by_source': {
                        'meilisearch': len(search_results.get('meili_results', [])),
                        'semantic': len(search_results.get('semantic_results', [])),
                        'fuzzy': len(search_results.get('fuzzy_results', []))
                    },
                    'fusion_score': search_results.get('fusion_score', 0),
                    'total_documents': search_results.get('total_documents', 0)
                }
                
                context_used = search_results.get('final_context', '')
                
            else:
                # Fallback vers recherche classique
                search_results = await self._fallback_search(query, company_id)
                context_used = search_results.get('combined_context', '')
            
            # ========== √âTAPE 3 : G√âN√âRATION AVEC PROMPT ENRICHI ==========
            pipeline_stages.append("enriched_generation")
            
            company_display_name = company_name or f"l'entreprise {company_id[:8]}"
            company_info = {
                'ai_name': 'Jessica',  # Par d√©faut
                'company_name': company_display_name,
                'business_sector': 'Commerce'
            }
            
            if ADVANCED_MODULES_AVAILABLE and self.prompt_enricher and processed_query:
                try:
                    logger.info("üéØ [√âTAPE 3] G√©n√©ration avec prompt enrichi...")
                    
                    # Enrichissement du prompt
                    enriched_prompt = self.prompt_enricher.enrich_prompt(
                        query, context_used, processed_query.intentions, processed_query.entities, company_info
                    )
                    
                    # G√©n√©ration LLM avec prompt enrichi
                    response = await self._generate_llm_response(
                        enriched_prompt.system_prompt,
                        enriched_prompt.user_prompt
                    )
                    
                    logger.info(f"‚úÖ [PROMPT] Template utilis√©: {enriched_prompt.template_used}")
                    
                except Exception as e:
                    logger.error(f"‚ùå [PROMPT] Erreur enrichissement: {e}")
                    # Fallback vers g√©n√©ration classique
                    response = await self._generate_simple_response(query, context_used, company_info)
            else:
                # G√©n√©ration classique
                response = await self._generate_simple_response(query, context_used, company_info)
            
            # ========== √âTAPE 4 : V√âRIFICATION QA ==========
            pipeline_stages.append("qa_verification")
            
            if ADVANCED_MODULES_AVAILABLE and self.response_verifier and response:
                try:
                    logger.info("‚úÖ [√âTAPE 4] V√©rification QA...")
                    
                    verification = self.response_verifier.verify_response(
                        response, context_used, query, 
                        processed_query.intentions if processed_query else [],
                        processed_query.entities if processed_query else []
                    )
                    
                    verification_result = {
                        'is_valid': verification.is_valid,
                        'quality_score': verification.quality_score,
                        'issues_count': len(verification.issues),
                        'issues_by_severity': {},
                        'corrected_available': verification.corrected_response is not None
                    }
                    
                    # Statistiques des issues
                    for issue in verification.issues:
                        severity = issue.severity
                        verification_result['issues_by_severity'][severity] = \
                            verification_result['issues_by_severity'].get(severity, 0) + 1
                    
                    quality_score = verification.quality_score
                    
                    # Utiliser la correction si disponible et n√©cessaire
                    if not verification.is_valid and verification.corrected_response:
                        response = verification.corrected_response
                        logger.info("üîß [QA] R√©ponse corrig√©e automatiquement")
                    
                    logger.info(f"‚úÖ [QA] V√©rification termin√©e: score={quality_score:.2f}, valid={verification.is_valid}")
                    
                except Exception as e:
                    logger.error(f"‚ùå [QA] Erreur v√©rification: {e}")
            
            # ========== CALCUL FINAL ==========
            pipeline_stages.append("finalization")
            
            # Calcul de confiance globale
            confidence = self._calculate_global_confidence(
                nlp_analysis, fusion_metadata, quality_score, len(response)
            )
            
            # M√©thode de recherche
            search_method = "advanced_hybrid" if ADVANCED_MODULES_AVAILABLE else "fallback_classic"
            if search_results:
                search_method = search_results.get('search_method', search_method)
            
            # Documents trouv√©s
            documents_found = bool(context_used and len(context_used.strip()) > 50)
            
            # Temps de traitement
            processing_time = (time.time() - start_time) * 1000
            
            logger.info(f"üéâ [ADVANCED_RAG] Termin√© en {processing_time:.0f}ms | Confiance: {confidence:.2f}")
            
            return AdvancedRAGResult(
                response=response,
                confidence=confidence,
                documents_found=documents_found,
                processing_time_ms=processing_time,
                search_method=search_method,
                context_used=context_used,
                nlp_analysis=nlp_analysis,
                fusion_metadata=fusion_metadata,
                verification_result=verification_result,
                quality_score=quality_score,
                pipeline_stages=pipeline_stages
            )
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            logger.error(f"‚ùå [ADVANCED_RAG] Erreur critique: {str(e)[:100]} | {processing_time:.0f}ms")
            
            # Fallback d'urgence
            return AdvancedRAGResult(
                response="Je rencontre une difficult√© technique. Pouvez-vous r√©essayer votre question ?",
                confidence=0.1,
                documents_found=False,
                processing_time_ms=processing_time,
                search_method="emergency_fallback",
                context_used="",
                pipeline_stages=pipeline_stages
            )
    
    async def _advanced_search_pipeline(self, query: str, company_id: str, processed_query: ProcessedQuery) -> Dict[str, Any]:
        """Pipeline de recherche hybride avanc√©"""
        logger.info("üîç [SEARCH] Pipeline hybride avanc√©...")
        
        all_results = {
            'meili_results': [],
            'semantic_results': [],
            'fuzzy_results': [],
            'final_context': '',
            'total_documents': 0,
            'search_methods_used': [],
            'search_method': 'hybrid_advanced',
            'fusion_score': 0
        }
        
        # 1. Recherche MeiliSearch
        try:
            from database.vector_store_clean_v2 import search_all_indexes_parallel
            
            meili_raw = await search_all_indexes_parallel(
                query=query,
                company_id=company_id,
                limit=self.search_config.top_k
            )
            
            if meili_raw and isinstance(meili_raw, str):
                meili_docs = []
                for i, doc_text in enumerate(meili_raw.split('\n\n')):
                    if doc_text.strip():
                        meili_docs.append({
                            'id': f'meili_{i}',
                            'content': doc_text.strip(),
                            'score': 0.8,
                            'metadata': {'source': 'meilisearch'}
                        })
                
                all_results['meili_results'] = meili_docs
                all_results['search_methods_used'].append('meilisearch')
                logger.info(f"üîç [MEILI] {len(meili_docs)} documents trouv√©s")
        
        except Exception as e:
            logger.error(f"‚ùå [MEILI] Erreur: {e}")
        
        # 2. Recherche s√©mantique
        if self.semantic_engine:
            try:
                semantic_results, _ = await self.semantic_engine.search(
                    query, company_id, self.search_config
                )
                
                semantic_docs = []
                for result in semantic_results:
                    semantic_docs.append({
                        'id': result.id,
                        'content': result.content,
                        'score': result.score,
                        'metadata': {**result.metadata, 'source': 'semantic'}
                    })
                
                all_results['semantic_results'] = semantic_docs
                all_results['search_methods_used'].append('semantic')
                logger.info(f"üß† [SEMANTIC] {len(semantic_docs)} documents trouv√©s")
                
            except Exception as e:
                logger.error(f"‚ùå [SEMANTIC] Erreur: {e}")
        
        # 3. Recherche fuzzy multi-intent
        if processed_query.split_queries and len(processed_query.split_queries) > 1:
            try:
                fuzzy_docs = []
                for split_query in processed_query.split_queries[:3]:
                    sub_results = await self._fuzzy_search_fallback(split_query, company_id)
                    fuzzy_docs.extend(sub_results)
                
                all_results['fuzzy_results'] = fuzzy_docs
                if fuzzy_docs:
                    all_results['search_methods_used'].append('fuzzy_multi_intent')
                    logger.info(f"üîÄ [FUZZY] {len(fuzzy_docs)} documents trouv√©s")
                
            except Exception as e:
                logger.error(f"‚ùå [FUZZY] Erreur: {e}")
        
        # 4. Fusion intelligente
        if self.fusion_engine and any([all_results['meili_results'], all_results['semantic_results'], all_results['fuzzy_results']]):
            try:
                fused_results, formatted_context = fuse_search_results(
                    all_results['meili_results'],
                    all_results['semantic_results'],
                    all_results['fuzzy_results'],
                    query,
                    self.fusion_config
                )
                
                all_results['final_context'] = formatted_context
                all_results['total_documents'] = len(fused_results)
                all_results['fusion_score'] = sum(r.final_score for r in fused_results) / len(fused_results) if fused_results else 0
                
                logger.info(f"üîÄ [FUSION] {len(fused_results)} documents fusionn√©s (score moyen: {all_results['fusion_score']:.2f})")
                
            except Exception as e:
                logger.error(f"‚ùå [FUSION] Erreur: {e}")
                # Fallback simple
                all_contexts = []
                for results in [all_results['meili_results'], all_results['semantic_results'], all_results['fuzzy_results']]:
                    for doc in results[:3]:  # Max 3 par source
                        all_contexts.append(doc.get('content', ''))
                
                all_results['final_context'] = '\n\n'.join(all_contexts)
                all_results['total_documents'] = len(all_contexts)
        
        return all_results
    
    async def _fuzzy_search_fallback(self, query: str, company_id: str) -> List[Dict[str, Any]]:
        """Recherche fuzzy en fallback"""
        try:
            from .supabase_simple import SupabaseSimple
            
            supabase = SupabaseSimple()
            supabase_results = await supabase.search_documents(
                query=query,
                company_id=company_id,
                limit=3
            )
            
            fuzzy_docs = []
            for doc in supabase_results:
                fuzzy_docs.append({
                    'id': doc.get('id', ''),
                    'content': doc.get('content', ''),
                    'score': doc.get('similarity_score', 0.5),
                    'metadata': {**doc.get('metadata', {}), 'source': 'fuzzy_supabase'}
                })
            
            return fuzzy_docs
            
        except Exception as e:
            logger.error(f"‚ùå [FUZZY_FALLBACK] Erreur: {e}")
            return []
    
    async def _fallback_search(self, query: str, company_id: str) -> Dict[str, Any]:
        """Recherche de fallback classique"""
        try:
            # Import de la fonction de recherche existante
            from database.vector_store_clean_v2 import search_all_indexes_parallel
            
            meili_results = await search_all_indexes_parallel(
                query=query,
                company_id=company_id,
                limit=5
            )
            
            return {
                'combined_context': meili_results or '',
                'search_method': 'fallback_meilisearch',
                'total_documents': 1 if meili_results else 0
            }
            
        except Exception as e:
            logger.error(f"‚ùå [FALLBACK_SEARCH] Erreur: {e}")
            return {
                'combined_context': '',
                'search_method': 'fallback_failed',
                'total_documents': 0
            }
    
    async def _generate_llm_response(self, system_prompt: str, user_prompt: str) -> str:
        """G√©n√©ration LLM avec prompts enrichis"""
        try:
            if not self.llm_client:
                await self.initialize()
            
            # Appel LLM avec prompts enrichis
            response = await self.llm_client.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            return response or "Je n'ai pas pu g√©n√©rer une r√©ponse appropri√©e."
            
        except Exception as e:
            logger.error(f"‚ùå [LLM_ENRICHED] Erreur: {e}")
            return "Je rencontre une difficult√© pour traiter votre demande."
    
    async def _generate_simple_response(self, query: str, context: str, company_info: Dict[str, Any]) -> str:
        """G√©n√©ration LLM simple en fallback"""
        try:
            if not self.llm_client:
                await self.initialize()
            
            # Prompt simple
            system_prompt = f"""Tu es {company_info.get('ai_name', 'un assistant IA')} de {company_info.get('company_name', 'notre entreprise')}.
R√©ponds de mani√®re professionnelle et utile en fran√ßais."""
            
            user_prompt = f"""Question: {query}

Contexte disponible:
{context}

R√©ponds de mani√®re pr√©cise et professionnelle."""
            
            response = await self.llm_client.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            return response or "Je n'ai pas pu traiter votre demande."
            
        except Exception as e:
            logger.error(f"‚ùå [LLM_SIMPLE] Erreur: {e}")
            return "Je rencontre une difficult√© technique."
    
    def _calculate_global_confidence(
        self, 
        nlp_analysis: Optional[Dict], 
        fusion_metadata: Optional[Dict], 
        quality_score: Optional[float], 
        response_length: int
    ) -> float:
        """Calcule la confiance globale du pipeline"""
        confidence = 0.5  # Base
        
        # Bonus NLP
        if nlp_analysis:
            nlp_conf = nlp_analysis.get('confidence', 0)
            confidence += nlp_conf * 0.2
        
        # Bonus fusion
        if fusion_metadata:
            sources_count = fusion_metadata.get('total_sources', 0)
            if sources_count > 1:
                confidence += 0.15
            
            docs_count = fusion_metadata.get('total_documents', 0)
            if docs_count > 0:
                confidence += min(docs_count * 0.05, 0.2)
        
        # Bonus qualit√©
        if quality_score:
            confidence += quality_score * 0.3
        
        # Bonus longueur r√©ponse
        if response_length > 100:
            confidence += 0.1
        
        return min(confidence, 1.0)

# Instance globale avanc√©e
advanced_rag_engine = AdvancedUniversalRAGEngine()

# Interface simple pour compatibilit√©
async def get_advanced_rag_response(
    message: str, 
    company_id: str, 
    user_id: str, 
    company_name: str = None,
    conversation_history: str = ""
) -> Dict[str, Any]:
    """
    üöÄ INTERFACE AVANC√âE POUR RAG FRANCOPHONE
    
    Utilise le pipeline complet avec toutes les optimisations
    """
    logger.info(f"üöÄ [ADVANCED_ENTRY] Requ√™te: '{message[:50]}...'")
    
    result = await advanced_rag_engine.process_query_advanced(
        message, company_id, user_id, company_name
    )
    
    return {
        'response': result.response,
        'confidence': result.confidence,
        'documents_found': result.documents_found,
        'processing_time_ms': result.processing_time_ms,
        'search_method': result.search_method,
        'context_used': result.context_used,
        'success': True,
        # M√©tadonn√©es avanc√©es
        'nlp_analysis': result.nlp_analysis,
        'fusion_metadata': result.fusion_metadata,
        'verification_result': result.verification_result,
        'quality_score': result.quality_score,
        'pipeline_stages': result.pipeline_stages
    }

if __name__ == "__main__":
    import sys
    
    async def test_advanced():
        """Test du pipeline avanc√©"""
        company_id = sys.argv[1] if len(sys.argv) > 1 else "XkCn8fjNWEWwqiiKMgJX7OcQrUJ3"
        user_id = sys.argv[2] if len(sys.argv) > 2 else "testuser_advanced"
        company_name = sys.argv[3] if len(sys.argv) > 3 else "Rue du Gros"
        message = sys.argv[4] if len(sys.argv) > 4 else "Je veux 3 paquets de couches taille 2 et combien pour livraison √† Cocody?"
        
        print(f"üß™ Test RAG Engine Avanc√©:")
        print(f"üè¢ Company: {company_name}")
        print(f"üë§ User: {user_id}")
        print(f"üí¨ Message: {message}")
        print("-" * 80)
        
        result = await get_advanced_rag_response(message, company_id, user_id, company_name)
        
        print(f"ü§ñ R√©ponse: {result['response']}")
        print(f"üìä Confiance: {result['confidence']:.2f}")
        print(f"üîç M√©thode: {result['search_method']}")
        print(f"‚è±Ô∏è Temps: {result['processing_time_ms']:.0f}ms")
        
        if result.get('nlp_analysis'):
            nlp = result['nlp_analysis']
            print(f"üá´üá∑ NLP: {len(nlp.get('intentions', []))} intentions, {len(nlp.get('entities', []))} entit√©s")
        
        if result.get('quality_score'):
            print(f"‚úÖ Qualit√©: {result['quality_score']:.2f}")
    
    asyncio.run(test_advanced())
