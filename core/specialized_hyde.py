"""
üß† HYDE SP√âCIALIS√â PAR INTENTION
G√©n√®re des hypoth√®ses ultra-pr√©cises selon l'intention d√©tect√©e
"""

import os
import asyncio
from typing import Dict, Optional
from core.llm_client import GroqLLMClient
from .intention_router import IntentionResult

class SpecializedHyDE:
    """
    G√©n√©rateur d'hypoth√®ses HyDE sp√©cialis√©es par intention e-commerce
    """
    
    def __init__(self):
        self.client = GroqLLMClient()
        
        self.prompts_by_intention = {
            "delivery": {
                "system_prompt": """Tu es un expert en logistique e-commerce fran√ßais. 
                G√©n√®re une r√©ponse d√©taill√©e et r√©aliste sur les frais et d√©lais de livraison.
                Utilise des informations typiques du e-commerce fran√ßais.""",
                
                "user_template": """Question client: {query}

G√©n√®re une r√©ponse hypoth√©tique compl√®te incluant:
- Frais de livraison par zone (France m√©tropolitaine, DOM-TOM, Europe)
- D√©lais de livraison (standard 48-72h, express 24h)
- Conditions de gratuit√© (seuil 50‚Ç¨ typique)
- Transporteurs (Colissimo, Chronopost, Mondial Relay)
- Options de suivi et point relais

R√©ponds comme si tu √©tais le service client de cette boutique.""",
                
                "temperature": 0.3,
                "max_tokens": 200
            },
            
            "product_catalog": {
                "system_prompt": """Tu es un expert produits e-commerce sp√©cialis√© en √©quipements moto.
                G√©n√®re une fiche produit d√©taill√©e avec prix et caract√©ristiques r√©alistes.""",
                
                "user_template": """Question client: {query}

G√©n√®re une fiche produit hypoth√©tique incluant:
- Prix comp√©titif avec √©ventuelles promotions
- Caract√©ristiques techniques d√©taill√©es
- Variantes disponibles (couleurs, tailles, mod√®les)
- Stock et disponibilit√©
- Descriptions marketing attractives
- Comparaisons avec produits similaires

R√©ponds comme un catalogue produit professionnel.""",
                
                "temperature": 0.4,
                "max_tokens": 250
            },
            
            "company_identity": {
                "system_prompt": """Tu es un expert en communication d'entreprise e-commerce.
                G√©n√®re des informations compl√®tes sur l'identit√© et la localisation d'une boutique.""",
                
                "user_template": """Question client: {query}

G√©n√®re des informations d'entreprise incluant:
- Adresse pr√©cise et localisation g√©ographique
- Horaires d'ouverture d√©taill√©s
- Moyens de contact (t√©l√©phone, email, formulaire)
- Pr√©sentation de l'entreprise et son histoire
- Magasin physique vs boutique en ligne
- Services propos√©s et sp√©cialit√©s

R√©ponds comme une page "Qui sommes-nous" professionnelle.""",
                
                "temperature": 0.2,
                "max_tokens": 200
            },
            
            "support": {
                "system_prompt": """Tu es un expert support client e-commerce avec expertise paiement et SAV.
                G√©n√®re une r√©ponse de support technique, commercial et paiement compl√®te.""",
                
                "user_template": """Question client: {query}

G√©n√®re une r√©ponse support incluant:
- Solution d√©taill√©e au probl√®me pos√©
- √âtapes de r√©solution claires et num√©rot√©es
- Politique de retour/remboursement/√©change
- Informations sur garanties et SAV
- M√©thodes de paiement accept√©es (CB, PayPal, Mobile Money MTN/Orange/Moov, Wave, virement, paiement √† la livraison, esp√®ces)
- Gestion des erreurs de paiement et litiges
- Contacts support sp√©cialis√©s si n√©cessaire

R√©ponds comme un service client expert et bienveillant.""",
                
                "temperature": 0.3,
                "max_tokens": 220
            }
        }
        
        # Prompt g√©n√©rique pour fallback
        self.generic_prompt = {
            "system_prompt": """Tu es un assistant e-commerce expert.
            G√©n√®re une r√©ponse d√©taill√©e et utile pour aider le client.""",
            
            "user_template": """Question client: {query}

G√©n√®re une r√©ponse hypoth√©tique compl√®te qui pourrait aider ce client.
Inclus des informations pertinentes sur les produits, services, livraison ou support selon le contexte.

R√©ponds de mani√®re professionnelle et utile.""",
            
            "temperature": 0.4,
            "max_tokens": 200
        }
    
    async def generate_hypothesis(self, query: str, intentions: IntentionResult) -> str:
        """
        G√©n√®re une hypoth√®se HyDE sp√©cialis√©e selon l'intention d√©tect√©e
        
        Args:
            query: Requ√™te utilisateur originale
            intentions: R√©sultat de la d√©tection d'intentions
            
        Returns:
            Hypoth√®se HyDE sp√©cialis√©e
        """
        try:
            if not intentions.primary or intentions.confidence_score < 0.2:
                # Fallback sur HyDE g√©n√©rique
                return await self._generate_generic_hypothesis(query)
            
            if intentions.is_multi_intent and len(intentions.intentions) == 2:
                # Cas sp√©cial : 2 intentions (ex: prix + livraison)
                return await self._generate_multi_intent_hypothesis(query, intentions)
            
            # HyDE sp√©cialis√© pour intention unique
            return await self._generate_specialized_hypothesis(query, intentions.primary)
            
        except Exception as e:
            if os.getenv("DEBUG_HYDE") == "1":
                print(f"[SPECIALIZED_HYDE] Erreur: {e}")
            
            # Fallback s√©curis√©
            return await self._generate_generic_hypothesis(query)
    
    async def _generate_specialized_hypothesis(self, query: str, intention: str) -> str:
        """G√©n√®re une hypoth√®se pour une intention sp√©cifique"""
        
        if intention not in self.prompts_by_intention:
            return await self._generate_generic_hypothesis(query)
        
        config = self.prompts_by_intention[intention]
        
        prompt = f"{config['system_prompt']}\n\n{config['user_template'].format(query=query)}"
        
        # Utilise GROQ_HYDE_MODEL du .env pour les hypoth√®ses
        hyde_model = os.getenv("GROQ_HYDE_MODEL", "llama-3.1-8b-instant")
        hypothesis = await self.client.complete(
            prompt=prompt,
            model_name=hyde_model,  # Mini LLM 8B pour HyDE
            temperature=config["temperature"],
            max_tokens=config["max_tokens"]
        )
        
        if os.getenv("DEBUG_HYDE") == "1":
            print(f"[SPECIALIZED_HYDE] Intention: {intention}")
            print(f"[SPECIALIZED_HYDE] Query: {query}")
            print(f"[SPECIALIZED_HYDE] Hypothesis: {hypothesis[:100]}...")
        
        return hypothesis
    
    async def _generate_multi_intent_hypothesis(self, query: str, intentions: IntentionResult) -> str:
        """G√©n√®re une hypoth√®se pour requ√™tes multi-intentions"""
        
        # Prendre les 2 intentions principales
        top_intentions = list(intentions.intentions.keys())[:2]
        
        # Prompt hybride combinant les 2 domaines
        combined_system = f"""Tu es un expert e-commerce. Cette question concerne {' et '.join(top_intentions)}.
        G√©n√®re une r√©ponse compl√®te couvrant ces deux aspects."""
        
        combined_user = f"""Question client: {query}

Cette question concerne plusieurs sujets. G√©n√®re une r√©ponse hypoth√©tique qui couvre:
- Les aspects li√©s √† {top_intentions[0]}
- Les aspects li√©s √† {top_intentions[1]}

R√©ponds de mani√®re structur√©e et compl√®te."""
        
        prompt = f"{combined_system}\n\n{combined_user}"
        
        # Utilise GROQ_HYDE_MODEL pour multi-intentions
        hyde_model = os.getenv("GROQ_HYDE_MODEL", "llama-3.1-8b-instant")
        response = await self.client.complete(
            prompt=prompt,
            model_name=hyde_model,  # Mini LLM 8B pour HyDE
            temperature=0.3,
            max_tokens=250
        )
        
        return response
    
    async def _generate_generic_hypothesis(self, query: str) -> str:
        """G√©n√®re une hypoth√®se g√©n√©rique en fallback"""
        
        prompt = f"{self.generic_prompt['system_prompt']}\n\n{self.generic_prompt['user_template'].format(query=query)}"
        
        # Utilise GROQ_HYDE_MODEL pour fallback g√©n√©rique
        hyde_model = os.getenv("GROQ_HYDE_MODEL", "llama-3.1-8b-instant")
        response = await self.client.complete(
            prompt=prompt,
            model_name=hyde_model,  # Mini LLM 8B pour HyDE
            temperature=self.generic_prompt["temperature"],
            max_tokens=self.generic_prompt["max_tokens"]
        )
        
        return response

# Instance globale pour import facile (lazy loading)
specialized_hyde = None

def get_specialized_hyde():
    """Factory pour l'instance SpecializedHyDE avec lazy loading"""
    global specialized_hyde
    if specialized_hyde is None:
        specialized_hyde = SpecializedHyDE()
    return specialized_hyde
