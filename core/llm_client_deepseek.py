"""
Client LLM pour DeepSeek-V3.2 (DeepSeek)
Production-ready avec retry logic, rate limiting et context caching
"""
import os
import asyncio
from openai import AsyncOpenAI
from typing import Optional
import time
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")
DEEPSEEK_BASE_URL = "https://api.deepseek.com"
DEFAULT_MODEL = "deepseek-chat"  # V3.2-Exp sans r√©flexion
DEFAULT_REASONER_MODEL = "deepseek-reasoner"  # V3.2-Exp avec r√©flexion
DEFAULT_MAX_TOKENS = 1000
DEFAULT_TEMPERATURE = 0.7

# Client global
_client: Optional[AsyncOpenAI] = None

def get_client() -> AsyncOpenAI:
    """R√©cup√®re ou cr√©e le client DeepSeek"""
    global _client
    if _client is None:
        if not DEEPSEEK_API_KEY:
            raise ValueError("‚ùå DEEPSEEK_API_KEY non d√©finie dans .env")
        _client = AsyncOpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL
        )
        print("‚úÖ [DEEPSEEK] Client configur√©")
    return _client


async def complete(
    prompt: str,
    model_name: str = DEFAULT_MODEL,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    temperature: float = DEFAULT_TEMPERATURE,
    use_reasoning: bool = False,
    max_retries: int = 3
) -> str:
    """
    G√©n√®re une r√©ponse avec DeepSeek-V3.2
    
    Args:
        prompt: Le prompt utilisateur
        model_name: Mod√®le √† utiliser (d√©faut: deepseek-chat)
        max_tokens: Nombre max de tokens de sortie
        temperature: Cr√©ativit√© (0=d√©terministe, 1=cr√©atif)
        use_reasoning: Utiliser le mode raisonnement (deepseek-reasoner)
        max_retries: Nombre de tentatives en cas d'erreur
        
    Returns:
        str: R√©ponse g√©n√©r√©e par le LLM
    """
    client = get_client()
    
    # S√©lection du mod√®le
    if use_reasoning:
        model_name = DEFAULT_REASONER_MODEL
        print("üß† [DEEPSEEK] Mode raisonnement activ√©")
    
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            
            # Log complet de l'input envoy√© au LLM
            if attempt == 0:  # Afficher seulement √† la premi√®re tentative
                print("\n" + "="*100)
                print("\033[95müíú [DEEPSEEK INPUT] PROMPT COMPLET ENVOY√â AU LLM:\033[0m")
                print("\033[95m" + "="*100 + "\033[0m")
                print(f"\033[95m{prompt}\033[0m")
                print("\033[95m" + "="*100 + "\033[0m")
                print(f"\033[95müìä Longueur: {len(prompt)} caract√®res\033[0m")
                print("="*100 + "\n")
            
            response = await client.chat.completions.create(
                model=model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=60.0
            )
            
            elapsed = time.time() - start_time
            
            if response.choices and response.choices[0].message.content:
                text = response.choices[0].message.content.strip()
                
                # Logs d√©taill√©s
                usage = response.usage
                
                # Calcul du co√ªt (approximatif)
                input_cost = (usage.prompt_tokens / 1_000_000) * 0.28
                output_cost = (usage.completion_tokens / 1_000_000) * 0.42
                total_cost = input_cost + output_cost
                
                print(f"‚úÖ [DEEPSEEK] R√©ponse en {elapsed:.2f}s")
                print(f"üìä [DEEPSEEK] Tokens: {usage.prompt_tokens} input + {usage.completion_tokens} output = {usage.total_tokens} total")
                print(f"üí∞ [DEEPSEEK] Co√ªt: ${total_cost:.6f} (${input_cost:.6f} input + ${output_cost:.6f} output)")
                
                # Extraire le raisonnement si mode reasoner
                if use_reasoning and hasattr(response.choices[0].message, 'reasoning_content'):
                    reasoning = response.choices[0].message.reasoning_content
                    if reasoning:
                        print(f"üß† [DEEPSEEK] Raisonnement: {reasoning[:100]}...")
                
                return text
            else:
                print(f"‚ö†Ô∏è [DEEPSEEK] R√©ponse vide (tentative {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                continue
                
        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå [DEEPSEEK] Erreur (tentative {attempt + 1}/{max_retries}): {error_msg}")
            
            # Gestion des erreurs sp√©cifiques
            if "rate_limit" in error_msg.lower() or "429" in error_msg:
                print("‚è≥ [DEEPSEEK] Rate limit, attente 10s...")
                await asyncio.sleep(10)
            elif "api_key" in error_msg.lower() or "authentication" in error_msg.lower() or "401" in error_msg:
                raise ValueError("‚ùå Cl√© API DeepSeek invalide")
            elif "timeout" in error_msg.lower():
                print("‚è≥ [DEEPSEEK] Timeout, nouvelle tentative...")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
            else:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
    
    raise Exception(f"‚ùå [DEEPSEEK] √âchec apr√®s {max_retries} tentatives")


async def complete_with_history(
    messages: list[dict],
    model_name: str = DEFAULT_MODEL,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    temperature: float = DEFAULT_TEMPERATURE,
    use_reasoning: bool = False,
    enable_cache: bool = True,
    max_retries: int = 3
) -> str:
    """
    G√©n√®re une r√©ponse avec historique de conversation
    
    Args:
        messages: Liste de messages [{"role": "user/assistant/system", "content": "..."}]
        model_name: Mod√®le √† utiliser
        max_tokens: Nombre max de tokens de sortie
        temperature: Cr√©ativit√©
        use_reasoning: Utiliser le mode raisonnement
        enable_cache: Activer le context caching (√©conomise 10x sur tokens r√©p√©t√©s)
        max_retries: Nombre de tentatives
        
    Returns:
        str: R√©ponse g√©n√©r√©e
    """
    client = get_client()
    
    # S√©lection du mod√®le
    if use_reasoning:
        model_name = DEFAULT_REASONER_MODEL
    
    # Activer le cache pour les messages syst√®me/historique longs
    if enable_cache and len(messages) > 1:
        # Marquer les premiers messages pour le cache
        for i, msg in enumerate(messages[:-1]):  # Tous sauf le dernier
            if "cache_control" not in msg:
                msg["cache_control"] = {"type": "ephemeral"}
    
    for attempt in range(max_retries):
        try:
            start_time = time.time()
            
            # Log complet de l'input envoy√© au LLM
            if attempt == 0:  # Afficher seulement √† la premi√®re tentative
                print("\n" + "="*100)
                print("\033[95müíú [DEEPSEEK INPUT] MESSAGES COMPLETS ENVOY√âS AU LLM:\033[0m")
                print("\033[95m" + "="*100 + "\033[0m")
                for i, msg in enumerate(messages):
                    print(f"\033[95m[Message {i+1}] Role: {msg.get('role', 'unknown')}\033[0m")
                    content = msg.get('content', '')
                    print(f"\033[95m{content}\033[0m")
                    print("\033[95m" + "-"*100 + "\033[0m")
                print(f"\033[95müìä Total messages: {len(messages)}\033[0m")
                total_chars = sum(len(msg.get('content', '')) for msg in messages)
                print(f"\033[95müìä Longueur totale: {total_chars} caract√®res\033[0m")
                print("="*100 + "\n")
            
            response = await client.chat.completions.create(
                model=model_name,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                timeout=60.0
            )
            
            elapsed = time.time() - start_time
            
            if response.choices and response.choices[0].message.content:
                text = response.choices[0].message.content.strip()
                usage = response.usage
                
                # Calcul du co√ªt avec cache
                cache_hit_tokens = getattr(usage, 'prompt_cache_hit_tokens', 0)
                cache_miss_tokens = usage.prompt_tokens - cache_hit_tokens
                
                cache_hit_cost = (cache_hit_tokens / 1_000_000) * 0.028
                cache_miss_cost = (cache_miss_tokens / 1_000_000) * 0.28
                output_cost = (usage.completion_tokens / 1_000_000) * 0.42
                total_cost = cache_hit_cost + cache_miss_cost + output_cost
                
                print(f"‚úÖ [DEEPSEEK] R√©ponse avec historique en {elapsed:.2f}s")
                print(f"üìä [DEEPSEEK] Tokens: {usage.total_tokens} total")
                if cache_hit_tokens > 0:
                    print(f"üéØ [DEEPSEEK] Cache: {cache_hit_tokens} hits (√©conomie: ${(cache_miss_tokens * 0.252 / 1_000_000):.6f})")
                print(f"üí∞ [DEEPSEEK] Co√ªt: ${total_cost:.6f}")
                
                return text
            else:
                if attempt < max_retries - 1:
                    await asyncio.sleep(2 ** attempt)
                continue
                
        except Exception as e:
            print(f"‚ùå [DEEPSEEK] Erreur historique (tentative {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)
    
    raise Exception(f"‚ùå [DEEPSEEK] √âchec apr√®s {max_retries} tentatives")


# Test rapide
if __name__ == "__main__":
    async def test():
        print("üß™ Test DeepSeek-V3.2...")
        
        # Test simple
        response = await complete(
            "R√©ponds en une phrase : Quelle est la capitale de la C√¥te d'Ivoire ?",
            max_tokens=100
        )
        print(f"\n‚úÖ R√©ponse: {response}\n")
        
        # Test avec balises XML
        response2 = await complete(
            """R√©ponds avec ce format exact:
<thinking>
Analyse rapide
</thinking>

<response>
R√©ponse courte
</response>

Question: Quel est 2+2 ?""",
            max_tokens=200
        )
        print(f"‚úÖ R√©ponse structur√©e:\n{response2}\n")
        
        # Test mode raisonnement
        response3 = await complete(
            "R√©sous ce probl√®me √©tape par √©tape : Si un produit co√ªte 15000 FCFA et j'ai une remise de 20%, combien je paie ?",
            max_tokens=500,
            use_reasoning=True
        )
        print(f"‚úÖ R√©ponse avec raisonnement:\n{response3}")
    
    asyncio.run(test())
