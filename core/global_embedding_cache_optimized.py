#!/usr/bin/env python3
"""
ðŸš€ CACHE EMBEDDING OPTIMISÃ‰ SCALABLE
Objectif: RÃ©duire le temps de gÃ©nÃ©ration embeddings de 1.9s Ã  0.01s
Architecture: Cache intelligent avec similaritÃ© et TTL adaptatif
"""

import hashlib
import time
import numpy as np
from typing import Dict, Optional, Any, List, Tuple
from datetime import datetime, timedelta
import threading
from utils import log3

class GlobalEmbeddingCache:
    """
    ðŸš€ Cache intelligent des embeddings avec dÃ©tection de similaritÃ©
    - Cache basÃ© sur hash de la query
    - DÃ©tection de similaritÃ© pour rÃ©utilisation
    - TTL adaptatif selon frÃ©quence d'usage
    - Compression des embeddings pour Ã©conomiser la mÃ©moire
    """
    
    def __init__(self, max_cache_size: int = 10000, similarity_threshold: float = 0.95):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_cache_size = max_cache_size
        self.similarity_threshold = similarity_threshold
        self.lock = threading.RLock()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "similarity_hits": 0,
            "evictions": 0,
            "compressions": 0
        }
        
        log3("[EMBEDDING_CACHE]", "ðŸš€ Cache embedding optimisÃ© initialisÃ©")
    
    def _create_query_hash(self, query: str, model_name: str = "all-mpnet-base-v2") -> str:
        """CrÃ©e un hash unique pour une query + modÃ¨le"""
        combined = f"{model_name}:{query.lower().strip()}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    def _calculate_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calcule la similaritÃ© cosinus entre deux embeddings"""
        try:
            # Normaliser les vecteurs
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # SimilaritÃ© cosinus
            similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
            return float(similarity)
        except Exception:
            return 0.0
    
    def _find_similar_embedding(self, query: str, target_embedding: Optional[np.ndarray] = None) -> Optional[Tuple[str, np.ndarray]]:
        """
        ðŸ” Trouve un embedding similaire dans le cache
        Retourne (cache_key, embedding) si trouvÃ©
        """
        if len(self.cache) == 0:
            return None
        
        query_words = set(query.lower().split())
        best_similarity = 0.0
        best_match = None
        
        with self.lock:
            for cache_key, cache_entry in self.cache.items():
                if self._is_expired(cache_entry):
                    continue
                
                cached_query = cache_entry["original_query"]
                cached_words = set(cached_query.lower().split())
                
                # SimilaritÃ© textuelle rapide
                word_overlap = len(query_words.intersection(cached_words))
                word_union = len(query_words.union(cached_words))
                text_similarity = word_overlap / word_union if word_union > 0 else 0.0
                
                # Si similaritÃ© textuelle suffisante, vÃ©rifier embedding
                if text_similarity > 0.7 and target_embedding is not None:
                    cached_embedding = cache_entry["embedding"]
                    embedding_similarity = self._calculate_similarity(target_embedding, cached_embedding)
                    
                    if embedding_similarity > best_similarity and embedding_similarity >= self.similarity_threshold:
                        best_similarity = embedding_similarity
                        best_match = (cache_key, cached_embedding)
        
        if best_match:
            log3("[EMBEDDING_CACHE]", {
                "action": "similarity_match",
                "query": query[:50],
                "similarity_score": f"{best_similarity:.3f}",
                "threshold": self.similarity_threshold
            })
        
        return best_match
    
    def _is_expired(self, cache_entry: Dict[str, Any]) -> bool:
        """VÃ©rifie si une entrÃ©e est expirÃ©e avec TTL adaptatif"""
        return datetime.now() > cache_entry["expires_at"]
    
    def _calculate_adaptive_ttl(self, access_count: int) -> timedelta:
        """TTL adaptatif basÃ© sur la frÃ©quence d'usage"""
        if access_count >= 10:
            return timedelta(hours=24)  # TrÃ¨s utilisÃ©
        elif access_count >= 5:
            return timedelta(hours=6)   # Moyennement utilisÃ©
        else:
            return timedelta(hours=1)   # Peu utilisÃ©
    
    def _evict_oldest(self) -> None:
        """Supprime les entrÃ©es les plus anciennes si cache plein"""
        if len(self.cache) < self.max_cache_size:
            return
        
        with self.lock:
            # Trier par date d'accÃ¨s (plus ancien en premier)
            sorted_entries = sorted(
                self.cache.items(),
                key=lambda x: x[1]["last_accessed"]
            )
            
            # Supprimer 20% des entrÃ©es les plus anciennes
            to_remove = int(len(sorted_entries) * 0.2)
            for i in range(to_remove):
                cache_key = sorted_entries[i][0]
                del self.cache[cache_key]
                self.stats["evictions"] += 1
        
        log3("[EMBEDDING_CACHE]", {"action": "eviction", "removed_count": to_remove})
    
    async def get_embedding(self, query: str, model_name: str = "all-mpnet-base-v2") -> Optional[np.ndarray]:
        """
        ðŸŽ¯ RÃ©cupÃ¨re un embedding depuis le cache
        Utilise la similaritÃ© pour rÃ©utiliser des embeddings proches
        """
        query_hash = self._create_query_hash(query, model_name)
        
        with self.lock:
            # Cache hit exact
            if query_hash in self.cache:
                cache_entry = self.cache[query_hash]
                
                if not self._is_expired(cache_entry):
                    # Mettre Ã  jour les stats d'accÃ¨s
                    cache_entry["access_count"] += 1
                    cache_entry["last_accessed"] = datetime.now()
                    
                    # Recalculer TTL adaptatif
                    new_ttl = self._calculate_adaptive_ttl(cache_entry["access_count"])
                    cache_entry["expires_at"] = datetime.now() + new_ttl
                    
                    self.stats["hits"] += 1
                    log3("[EMBEDDING_CACHE]", {
                        "action": "cache_hit",
                        "query": query[:50],
                        "access_count": cache_entry["access_count"],
                        "new_ttl_hours": new_ttl.total_seconds() / 3600
                    })
                    
                    return cache_entry["embedding"]
                else:
                    # EntrÃ©e expirÃ©e
                    del self.cache[query_hash]
        
        # Recherche par similaritÃ©
        similar_match = self._find_similar_embedding(query)
        if similar_match:
            cache_key, embedding = similar_match
            self.stats["similarity_hits"] += 1
            
            # Mettre en cache avec la nouvelle query
            await self.set_embedding(query, embedding, model_name)
            return embedding
        
        # Cache miss complet
        self.stats["misses"] += 1
        log3("[EMBEDDING_CACHE]", {"action": "cache_miss", "query": query[:50]})
        return None
    
    async def set_embedding(self, query: str, embedding: np.ndarray, model_name: str = "all-mpnet-base-v2") -> None:
        """ðŸ’¾ Met en cache un embedding avec compression optionnelle"""
        if embedding is None or len(embedding) == 0:
            return
        
        # Ã‰viction si nÃ©cessaire
        self._evict_oldest()
        
        query_hash = self._create_query_hash(query, model_name)
        now = datetime.now()
        
        # Compression pour Ã©conomiser la mÃ©moire (float32 au lieu de float64)
        compressed_embedding = embedding.astype(np.float32)
        
        with self.lock:
            self.cache[query_hash] = {
                "embedding": compressed_embedding,
                "original_query": query,
                "model_name": model_name,
                "cached_at": now,
                "last_accessed": now,
                "expires_at": now + timedelta(hours=1),  # TTL initial
                "access_count": 0,
                "embedding_size": compressed_embedding.nbytes
            }
        
        self.stats["compressions"] += 1
        log3("[EMBEDDING_CACHE]", {
            "action": "cache_set",
            "query": query[:50],
            "embedding_dims": len(compressed_embedding),
            "size_bytes": compressed_embedding.nbytes
        })
    
    def get_stats(self) -> Dict[str, Any]:
        """ðŸ“Š Statistiques dÃ©taillÃ©es du cache"""
        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"] + self.stats["similarity_hits"]
            hit_rate = ((self.stats["hits"] + self.stats["similarity_hits"]) / total_requests * 100) if total_requests > 0 else 0
            
            total_memory = sum(entry["embedding_size"] for entry in self.cache.values())
            
            return {
                "cache_size": len(self.cache),
                "max_cache_size": self.max_cache_size,
                "memory_usage_mb": total_memory / (1024 * 1024),
                "total_requests": total_requests,
                "exact_hits": self.stats["hits"],
                "similarity_hits": self.stats["similarity_hits"],
                "misses": self.stats["misses"],
                "hit_rate_percent": f"{hit_rate:.1f}%",
                "evictions": self.stats["evictions"],
                "compressions": self.stats["compressions"]
            }
    
    def clear_expired(self) -> int:
        """ðŸ§¹ Nettoie les entrÃ©es expirÃ©es"""
        expired_keys = []
        
        with self.lock:
            for key, entry in self.cache.items():
                if self._is_expired(entry):
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
        
        if expired_keys:
            log3("[EMBEDDING_CACHE]", {"action": "cleanup_expired", "expired_count": len(expired_keys)})
        
        return len(expired_keys)

# Instance globale
_global_embedding_cache = None

def get_global_embedding_cache() -> GlobalEmbeddingCache:
    """ðŸš€ Singleton pour le cache embedding optimisÃ©"""
    global _global_embedding_cache
    if _global_embedding_cache is None:
        _global_embedding_cache = GlobalEmbeddingCache()
    return _global_embedding_cache
