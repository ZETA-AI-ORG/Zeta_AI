"""
üí∞ OPTIMISEUR DE TOKENS PROFESSIONNEL
R√©duction intelligente des co√ªts LLM
"""

import re
import math
from typing import List, Dict, Any, Tuple
import tiktoken
from collections import defaultdict
import logging

logger = logging.getLogger(__name__)

class TokenOptimizerPro:
    """
    üéØ OPTIMISEUR DE TOKENS PROFESSIONNEL
    
    Techniques d'optimisation:
    - Analyse d'importance des phrases par ML
    - Compression s√©mantique intelligente
    - D√©duplication avanc√©e
    - Reformatage optimal
    - Budget dynamique par type de requ√™te
    """
    
    def __init__(self):
        # Mod√®le de tokenisation
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # Configuration d'optimisation
        self.config = {
            'target_compression_ratio': 0.6,  # R√©duire de 40%
            'min_sentence_score': 0.3,        # Score minimum pour garder une phrase
            'max_context_tokens': 2000,       # Maximum absolu
            'deduplication_threshold': 0.8,   # Seuil de similarit√© pour d√©duplication
        }
        
        # Patterns de compression
        self.compression_patterns = {
            # Suppression des r√©p√©titions
            r'\b(\w+)\s+\1\b': r'\1',
            # Raccourcissement des listes longues
            r'(\w+),\s*(\w+),\s*(\w+),\s*(\w+),.*?(\w+)': r'\1, \2, \3... \5',
            # Simplification des formats
            r'POUR\s*\([^)]+\)\s*-\s*Index:\s*[^-]+\s*-\s*': '',
            r'Document\s+\d+/\d+\s*:': '',
            r'voici\s+le\s+document\s+trouv√©\s*:': '',
        }
        
        # Mots vides √©tendus
        self.stop_words = {
            'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'et', 'ou', 'mais',
            'donc', 'car', 'ni', 'or', 'ce', 'cette', 'ces', 'cet', 'voici', 'voil√†',
            'tr√®s', 'plus', 'moins', 'aussi', 'encore', 'd√©j√†', 'toujours', 'jamais'
        }
        
        logger.info("[TOKEN_OPTIMIZER] ‚úÖ Optimiseur de tokens initialis√©")
    
    def estimate_tokens(self, text: str) -> int:
        """Estime le nombre de tokens"""
        try:
            return len(self.encoding.encode(text))
        except Exception:
            # Estimation approximative si erreur
            return len(text) // 4
    
    def optimize_context(self, context: str, target_tokens: int = None, query: str = "") -> Dict[str, Any]:
        """
        Optimise un contexte pour r√©duire les tokens
        
        Args:
            context: Contexte √† optimiser
            target_tokens: Nombre de tokens cible (optionnel)
            query: Requ√™te utilisateur pour prioriser le contenu pertinent
        
        Returns:
            Dict avec contexte optimis√© et statistiques
        """
        
        if not context or len(context.strip()) == 0:
            return {
                'optimized_context': '',
                'original_tokens': 0,
                'optimized_tokens': 0,
                'compression_ratio': 0,
                'techniques_applied': []
            }
        
        original_tokens = self.estimate_tokens(context)
        
        # D√©finir la cible si pas fournie
        if target_tokens is None:
            target_tokens = min(
                int(original_tokens * self.config['target_compression_ratio']),
                self.config['max_context_tokens']
            )
        
        logger.info(f"[TOKEN_OPTIMIZER] Optimisation: {original_tokens} ‚Üí {target_tokens} tokens")
        
        # Pipeline d'optimisation
        optimized_context = context
        techniques_applied = []
        
        # 1. Nettoyage et formatage
        optimized_context, clean_applied = self._apply_cleaning(optimized_context)
        techniques_applied.extend(clean_applied)
        
        # 2. D√©duplication intelligente
        optimized_context, dedup_applied = self._apply_deduplication(optimized_context)
        techniques_applied.extend(dedup_applied)
        
        # 3. Analyse d'importance et s√©lection
        if self.estimate_tokens(optimized_context) > target_tokens:
            optimized_context, selection_applied = self._apply_smart_selection(
                optimized_context, target_tokens, query
            )
            techniques_applied.extend(selection_applied)
        
        # 4. Compression finale si n√©cessaire
        if self.estimate_tokens(optimized_context) > target_tokens:
            optimized_context, compression_applied = self._apply_final_compression(
                optimized_context, target_tokens
            )
            techniques_applied.extend(compression_applied)
        
        optimized_tokens = self.estimate_tokens(optimized_context)
        compression_ratio = 1 - (optimized_tokens / max(original_tokens, 1))
        
        logger.info(f"[TOKEN_OPTIMIZER] R√©sultat: {compression_ratio:.1%} compression")
        
        return {
            'optimized_context': optimized_context,
            'original_tokens': original_tokens,
            'optimized_tokens': optimized_tokens,
            'compression_ratio': compression_ratio,
            'techniques_applied': techniques_applied,
            'savings_tokens': original_tokens - optimized_tokens,
            'savings_cost_estimate_usd': (original_tokens - optimized_tokens) * 0.00002  # ~$0.02/1K tokens
        }
    
    def _apply_cleaning(self, text: str) -> Tuple[str, List[str]]:
        """Nettoyage et formatage basique"""
        techniques = []
        original_text = text
        
        # Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text)
        if text != original_text:
            techniques.append('espaces_multiples')
        
        # Application des patterns de compression
        for pattern, replacement in self.compression_patterns.items():
            new_text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
            if new_text != text:
                techniques.append(f'pattern_{pattern[:20]}...')
                text = new_text
        
        # Suppression des lignes vides
        lines = text.split('\n')
        lines = [line.strip() for line in lines if line.strip()]
        text = '\n'.join(lines)
        
        if len(lines) < len(text.split('\n')):
            techniques.append('lignes_vides')
        
        return text, techniques
    
    def _apply_deduplication(self, text: str) -> Tuple[str, List[str]]:
        """D√©duplication intelligente des contenus similaires"""
        techniques = []
        
        # S√©parer en blocs (par sections ou paragraphes)
        blocks = self._split_into_blocks(text)
        
        if len(blocks) <= 1:
            return text, techniques
        
        # D√©tecter et supprimer les doublons
        unique_blocks = []
        seen_content = set()
        
        for block in blocks:
            # Normaliser pour comparaison
            normalized = self._normalize_for_comparison(block)
            content_hash = hash(normalized)
            
            # V√©rifier la similarit√© avec les blocs existants
            is_duplicate = False
            for seen_hash in seen_content:
                similarity = self._calculate_similarity(content_hash, seen_hash)
                if similarity > self.config['deduplication_threshold']:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_blocks.append(block)
                seen_content.add(content_hash)
            else:
                techniques.append('deduplication')
        
        deduped_text = '\n\n'.join(unique_blocks)
        
        return deduped_text, techniques
    
    def _apply_smart_selection(self, text: str, target_tokens: int, query: str) -> Tuple[str, List[str]]:
        """S√©lection intelligente bas√©e sur l'importance des phrases"""
        techniques = []
        
        # S√©parer en phrases
        sentences = self._split_into_sentences(text)
        
        if len(sentences) <= 1:
            return text, techniques
        
        # Scorer chaque phrase
        sentence_scores = []
        query_keywords = self._extract_keywords(query.lower())
        
        for sentence in sentences:
            score = self._score_sentence_importance(sentence, query_keywords)
            sentence_scores.append({
                'sentence': sentence,
                'score': score,
                'tokens': self.estimate_tokens(sentence)
            })
        
        # Trier par score d√©croissant
        sentence_scores.sort(key=lambda x: x['score'], reverse=True)
        
        # S√©lection optimale sous contrainte de tokens
        selected_sentences = []
        current_tokens = 0
        
        for sentence_data in sentence_scores:
            sentence_tokens = sentence_data['tokens']
            
            # V√©rifier si on peut ajouter cette phrase
            if current_tokens + sentence_tokens <= target_tokens:
                if sentence_data['score'] >= self.config['min_sentence_score']:
                    selected_sentences.append(sentence_data)
                    current_tokens += sentence_tokens
            else:
                # Plus de place, arr√™ter
                break
        
        if len(selected_sentences) < len(sentences):
            techniques.append(f'selection_phrases_{len(selected_sentences)}/{len(sentences)}')
        
        # R√©organiser par ordre d'apparition original pour coh√©rence
        selected_sentences.sort(key=lambda x: sentences.index(x['sentence']))
        
        optimized_text = ' '.join([s['sentence'] for s in selected_sentences])
        
        return optimized_text, techniques
    
    def _apply_final_compression(self, text: str, target_tokens: int) -> Tuple[str, List[str]]:
        """Compression finale si toujours trop long"""
        techniques = []
        current_tokens = self.estimate_tokens(text)
        
        if current_tokens <= target_tokens:
            return text, techniques
        
        # Compression par troncature intelligente
        # Garder le d√©but et la fin, couper le milieu si n√©cessaire
        words = text.split()
        target_words = int(len(words) * (target_tokens / current_tokens))
        
        if target_words < len(words):
            # Garder 70% du d√©but, 30% de la fin
            start_words = int(target_words * 0.7)
            end_words = target_words - start_words
            
            if end_words > 0:
                compressed_text = (
                    ' '.join(words[:start_words]) + 
                    ' [...] ' + 
                    ' '.join(words[-end_words:])
                )
            else:
                compressed_text = ' '.join(words[:target_words])
            
            techniques.append(f'troncature_{target_words}_mots')
            return compressed_text, techniques
        
        return text, techniques
    
    def _split_into_blocks(self, text: str) -> List[str]:
        """S√©pare le texte en blocs logiques"""
        # S√©parer par double retour √† la ligne ou sections
        blocks = re.split(r'\n\s*\n', text)
        return [block.strip() for block in blocks if block.strip()]
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """S√©pare le texte en phrases"""
        # Patterns pour d√©tecter les fins de phrase
        sentence_endings = r'[.!?]+\s+'
        sentences = re.split(sentence_endings, text)
        
        # Nettoyer et filtrer
        sentences = [s.strip() for s in sentences if s.strip() and len(s.strip()) > 10]
        
        return sentences
    
    def _normalize_for_comparison(self, text: str) -> str:
        """Normalise le texte pour comparaison de similarit√©"""
        # Minuscules, suppression ponctuation, mots vides
        text = text.lower()
        text = re.sub(r'[^\w\s]', '', text)
        words = text.split()
        words = [w for w in words if w not in self.stop_words and len(w) > 2]
        return ' '.join(sorted(words))
    
    def _calculate_similarity(self, hash1: int, hash2: int) -> float:
        """Calcule la similarit√© entre deux hashes (approximatif)"""
        # M√©thode simple bas√©e sur les bits communs
        xor_result = hash1 ^ hash2
        common_bits = bin(xor_result).count('0')
        total_bits = 64  # hash Python 64 bits
        return common_bits / total_bits
    
    def _extract_keywords(self, query: str) -> List[str]:
        """Extrait les mots-cl√©s importants d'une requ√™te"""
        words = re.findall(r'\b\w+\b', query.lower())
        keywords = [w for w in words if w not in self.stop_words and len(w) > 2]
        return keywords
    
    def _score_sentence_importance(self, sentence: str, query_keywords: List[str]) -> float:
        """Score l'importance d'une phrase"""
        sentence_lower = sentence.lower()
        score = 0.0
        
        # 1. Score bas√© sur les mots-cl√©s de la requ√™te
        keyword_matches = 0
        for keyword in query_keywords:
            if keyword in sentence_lower:
                keyword_matches += 1
                # Bonus pour correspondances exactes
                score += 1.0
                # Bonus pour multiples occurrences
                occurrences = sentence_lower.count(keyword)
                score += min(occurrences - 1, 2) * 0.5
        
        # 2. Score de longueur (ni trop court ni trop long)
        length = len(sentence.split())
        if 5 <= length <= 30:
            score += 0.5
        elif length > 30:
            score -= 0.2
        
        # 3. Score de position de mots importants
        important_words = ['prix', 'co√ªt', 'tarif', 'livraison', 'disponible', 'stock']
        for word in important_words:
            if word in sentence_lower:
                score += 0.3
        
        # 4. P√©nalit√© pour phrases g√©n√©riques
        generic_phrases = ['informations g√©n√©rales', 'voici le document', 'document trouv√©']
        for phrase in generic_phrases:
            if phrase in sentence_lower:
                score -= 0.5
        
        # Normaliser le score entre 0 et 1
        max_possible_score = len(query_keywords) + 1.0  # Score maximum th√©orique
        normalized_score = min(score / max_possible_score, 1.0) if max_possible_score > 0 else 0.0
        
        return max(normalized_score, 0.0)

# Instance globale
token_optimizer = TokenOptimizerPro()




