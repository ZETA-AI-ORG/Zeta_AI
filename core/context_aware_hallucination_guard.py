#!/usr/bin/env python3
"""
ðŸ›¡ï¸ GARDE-FOU ANTI-HALLUCINATION CONTEXTUEL 2024
SystÃ¨me de validation avancÃ© basÃ© sur les meilleures pratiques enterprise
"""

import asyncio
import time
import json
import re
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import logging
from datetime import datetime

from .advanced_intent_classifier import IntentType, IntentResult
from .llm_client import GroqLLMClient

logger = logging.getLogger(__name__)

class ValidationLevel(Enum):
    """Niveaux de validation"""
    STRICT = "strict"        # Validation stricte (intentions critiques)
    MODERATE = "moderate"    # Validation modÃ©rÃ©e (intentions mÃ©tier)
    LENIENT = "lenient"      # Validation permissive (intentions sociales)
    BYPASS = "bypass"        # Pas de validation (conversations gÃ©nÃ©rales)

@dataclass
class ValidationResult:
    """RÃ©sultat de validation contextuelle"""
    is_safe: bool
    confidence_score: float
    validation_level: ValidationLevel
    intent_detected: IntentType
    requires_documents: bool
    documents_found: bool
    context_relevance: float
    factual_accuracy: float
    safety_score: float
    suggested_response: Optional[str] = None
    rejection_reason: Optional[str] = None
    processing_time_ms: float = 0.0
    validation_details: Dict[str, Any] = None

class ContextAwareHallucinationGuard:
    """
    ðŸ›¡ï¸ GARDE-FOU ANTI-HALLUCINATION CONTEXTUEL 2024
    
    FonctionnalitÃ©s avancÃ©es :
    1. Validation contextuelle intelligente
    2. Seuils de confiance adaptatifs
    3. Gestion des intentions sociales vs mÃ©tier
    4. Validation factuelle multi-niveaux
    5. SystÃ¨me de fallback intelligent
    """
    
    def __init__(self):
        self.llm_client = GroqLLMClient()
        self.start_time = datetime.now()
        
        # Seuils de validation adaptatifs
        self.validation_thresholds = {
            ValidationLevel.STRICT: {
                'min_confidence': 0.9,
                'min_documents': 1,
                'min_relevance': 0.8,
                'min_factual': 0.9
            },
            ValidationLevel.MODERATE: {
                'min_confidence': 0.7,
                'min_documents': 1,
                'min_relevance': 0.6,
                'min_factual': 0.7
            },
            ValidationLevel.LENIENT: {
                'min_confidence': 0.5,
                'min_documents': 0,
                'min_relevance': 0.4,
                'min_factual': 0.5
            },
            ValidationLevel.BYPASS: {
                'min_confidence': 0.0,
                'min_documents': 0,
                'min_relevance': 0.0,
                'min_factual': 0.0
            }
        }
        
        # Patterns de validation
        self.validation_patterns = self._build_validation_patterns()
        
        # Cache des validations
        self.validation_cache = {}
        self.cache_ttl = 1800  # 30 minutes
        
        logger.info("[CONTEXT_GUARD] SystÃ¨me de validation contextuel initialisÃ©")
    
    def _build_validation_patterns(self) -> Dict[str, List[str]]:
        """Construit les patterns de validation"""
        return {
            'dangerous_claims': [
                r'(?i)(nous\s+garantissons|we\s+guarantee)(?!\s+(selon|d\'aprÃ¨s))',
                r'(?i)(prix\s+fixe|fixed\s+price)(?!\s+(mentionnÃ©|indiquÃ©))',
                r'(?i)(disponible\s+immÃ©diatement|available\s+now)(?!\s+(selon))',
                r'(?i)(livraison\s+gratuite|free\s+shipping)(?!\s+(dans|pour))',
                r'(?i)(promotion\s+exclusive|exclusive\s+offer)(?!\s+(valable))'
            ],
            'factual_claims': [
                r'(?i)(prix|price)\s+est\s+(\d+)',
                r'(?i)(stock|inventory)\s+est\s+(\d+)',
                r'(?i)(livraison|delivery)\s+en\s+(\d+)\s+jours',
                r'(?i)(garantie|warranty)\s+de\s+(\d+)\s+ans',
                r'(?i)(dimensions|size)\s+sont\s+(\d+)'
            ],
            'social_responses': [
                r'(?i)(je\s+m\'appelle|my\s+name\s+is)',
                r'(?i)(bonjour|hello|hi)',
                r'(?i)(merci|thank\s+you)',
                r'(?i)(comment\s+allez-vous|how\s+are\s+you)',
                r'(?i)(de\s+rien|you\'re\s+welcome)'
            ]
        }
    
    async def validate_response(
        self,
        user_query: str,
        ai_response: str,
        intent_result: IntentResult,
        supabase_results: List[Dict] = None,
        meili_results: List[Dict] = None,
        supabase_context: str = "",
        meili_context: str = "",
        company_id: str = ""
    ) -> ValidationResult:
        """
        ðŸŽ¯ VALIDATION CONTEXTUELLE AVANCÃ‰E
        
        Pipeline de validation adaptatif basÃ© sur l'intention dÃ©tectÃ©e
        """
        start_time = time.time()
        
        logger.info(f"[CONTEXT_GUARD] Validation: '{ai_response[:50]}...' (intent: {intent_result.primary_intent.value})")
        
        # DÃ©termination du niveau de validation
        validation_level = self._determine_validation_level(intent_result)
        
        # VÃ©rification du cache
        cache_key = self._generate_cache_key(user_query, ai_response, intent_result.primary_intent)
        if cache_key in self.validation_cache:
            cached_result = self.validation_cache[cache_key]
            if (datetime.now() - cached_result['timestamp']).seconds < self.cache_ttl:
                return cached_result['result']
        
        # Pipeline de validation adaptatif
        if validation_level == ValidationLevel.BYPASS:
            result = self._bypass_validation(intent_result, ai_response)
        elif validation_level == ValidationLevel.LENIENT:
            result = await self._lenient_validation(
                user_query, ai_response, intent_result, 
                supabase_results, meili_results, supabase_context, meili_context
            )
        elif validation_level == ValidationLevel.MODERATE:
            result = await self._moderate_validation(
                user_query, ai_response, intent_result,
                supabase_results, meili_results, supabase_context, meili_context
            )
        else:  # STRICT
            result = await self._strict_validation(
                user_query, ai_response, intent_result,
                supabase_results, meili_results, supabase_context, meili_context
            )
        
        # Calcul du temps de traitement
        result.processing_time_ms = (time.time() - start_time) * 1000
        
        # Mise en cache
        self.validation_cache[cache_key] = {
            'result': result,
            'timestamp': datetime.now()
        }
        
        logger.info(f"[CONTEXT_GUARD] Validation terminÃ©e: {result.is_safe} (conf: {result.confidence_score:.2f})")
        
        return result
    
    def _determine_validation_level(self, intent_result: IntentResult) -> ValidationLevel:
        """DÃ©termine le niveau de validation basÃ© sur l'intention"""
        intent = intent_result.primary_intent
        confidence = intent_result.confidence
        
        # Intentions critiques â†’ Validation stricte
        if intent in [IntentType.TECHNICAL_SPECS, IntentType.MEDICAL_ADVICE, IntentType.FINANCIAL_ADVICE]:
            return ValidationLevel.STRICT
        
        # Intentions mÃ©tier avec confiance Ã©levÃ©e â†’ Validation modÃ©rÃ©e
        if intent in [IntentType.PRODUCT_INQUIRY, IntentType.PRICING_INFO, IntentType.DELIVERY_INFO]:
            if confidence >= 0.7:
                return ValidationLevel.MODERATE
            else:
                return ValidationLevel.STRICT
        
        # Intentions sociales avec confiance Ã©levÃ©e â†’ Validation permissive
        if intent in [IntentType.SOCIAL_GREETING, IntentType.POLITENESS, IntentType.GENERAL_CONVERSATION]:
            if confidence >= 0.6:
                return ValidationLevel.LENIENT
            else:
                return ValidationLevel.BYPASS
        
        # Intentions hors-sujet â†’ Pas de validation
        if intent == IntentType.OFF_TOPIC:
            return ValidationLevel.BYPASS
        
        # Fallback â†’ Validation permissive
        return ValidationLevel.LENIENT
    
    def _bypass_validation(self, intent_result: IntentResult, ai_response: str) -> ValidationResult:
        """Validation bypass pour les conversations sociales"""
        return ValidationResult(
            is_safe=True,
            confidence_score=0.9,
            validation_level=ValidationLevel.BYPASS,
            intent_detected=intent_result.primary_intent,
            requires_documents=False,
            documents_found=False,
            context_relevance=0.8,
            factual_accuracy=0.7,
            safety_score=0.9,
            processing_time_ms=0.0,
            validation_details={'bypass_reason': 'social_conversation'}
        )
    
    async def _lenient_validation(
        self, user_query: str, ai_response: str, intent_result: IntentResult,
        supabase_results: List[Dict], meili_results: List[Dict], 
        supabase_context: str, meili_context: str
    ) -> ValidationResult:
        """Validation permissive pour les intentions sociales"""
        
        # VÃ©rification des patterns dangereux
        dangerous_patterns = self.validation_patterns['dangerous_claims']
        has_dangerous_claims = any(re.search(pattern, ai_response) for pattern in dangerous_patterns)
        
        # VÃ©rification des patterns sociaux
        social_patterns = self.validation_patterns['social_responses']
        has_social_patterns = any(re.search(pattern, ai_response) for pattern in social_patterns)
        
        # Score de sÃ©curitÃ© basÃ© sur les patterns
        safety_score = 0.8 if has_social_patterns else 0.6
        if has_dangerous_claims:
            safety_score = 0.2
        
        is_safe = safety_score >= 0.5 and not has_dangerous_claims
        
        return ValidationResult(
            is_safe=is_safe,
            confidence_score=0.7,
            validation_level=ValidationLevel.LENIENT,
            intent_detected=intent_result.primary_intent,
            requires_documents=False,
            documents_found=False,
            context_relevance=0.6,
            factual_accuracy=0.6,
            safety_score=safety_score,
            suggested_response=None if is_safe else "Je ne peux pas rÃ©pondre Ã  cette question de maniÃ¨re appropriÃ©e.",
            rejection_reason=None if is_safe else "Patterns dangereux dÃ©tectÃ©s",
            validation_details={
                'has_dangerous_claims': has_dangerous_claims,
                'has_social_patterns': has_social_patterns,
                'safety_score': safety_score
            }
        )
    
    async def _moderate_validation(
        self, user_query: str, ai_response: str, intent_result: IntentResult,
        supabase_results: List[Dict], meili_results: List[Dict], 
        supabase_context: str, meili_context: str
    ) -> ValidationResult:
        """Validation modÃ©rÃ©e pour les intentions mÃ©tier"""
        
        # VÃ©rification de la prÃ©sence de documents
        documents_found = bool(
            (supabase_results and len(supabase_results) > 0) or
            (meili_results and len(meili_results) > 0) or
            (supabase_context and len(supabase_context.strip()) > 10) or
            (meili_context and len(meili_context.strip()) > 10)
        )
        
        # Validation LLM simplifiÃ©e
        llm_validation = await self._llm_validation(user_query, ai_response, supabase_context, meili_context, "moderate")
        
        # Score de confiance combinÃ©
        confidence_score = 0.6
        if documents_found:
            confidence_score += 0.2
        if llm_validation['is_safe']:
            confidence_score += 0.2
        
        is_safe = confidence_score >= 0.7 and llm_validation['is_safe']
        
        return ValidationResult(
            is_safe=is_safe,
            confidence_score=confidence_score,
            validation_level=ValidationLevel.MODERATE,
            intent_detected=intent_result.primary_intent,
            requires_documents=True,
            documents_found=documents_found,
            context_relevance=0.7,
            factual_accuracy=0.7,
            safety_score=0.7,
            suggested_response=llm_validation.get('suggested_response'),
            rejection_reason=llm_validation.get('rejection_reason'),
            validation_details={
                'documents_found': documents_found,
                'llm_validation': llm_validation
            }
        )
    
    async def _strict_validation(
        self, user_query: str, ai_response: str, intent_result: IntentResult,
        supabase_results: List[Dict], meili_results: List[Dict], 
        supabase_context: str, meili_context: str
    ) -> ValidationResult:
        """Validation stricte pour les intentions critiques"""
        
        # VÃ©rification de la prÃ©sence de documents
        documents_found = bool(
            (supabase_results and len(supabase_results) > 0) or
            (meili_results and len(meili_results) > 0) or
            (supabase_context and len(supabase_context.strip()) > 10) or
            (meili_context and len(meili_context.strip()) > 10)
        )
        
        # Validation LLM stricte
        llm_validation = await self._llm_validation(user_query, ai_response, supabase_context, meili_context, "strict")
        
        # Score de confiance strict
        confidence_score = 0.3
        if documents_found:
            confidence_score += 0.3
        if llm_validation['is_safe']:
            confidence_score += 0.4
        
        is_safe = confidence_score >= 0.8 and llm_validation['is_safe'] and documents_found
        
        return ValidationResult(
            is_safe=is_safe,
            confidence_score=confidence_score,
            validation_level=ValidationLevel.STRICT,
            intent_detected=intent_result.primary_intent,
            requires_documents=True,
            documents_found=documents_found,
            context_relevance=0.8,
            factual_accuracy=0.8,
            safety_score=0.8,
            suggested_response=llm_validation.get('suggested_response'),
            rejection_reason=llm_validation.get('rejection_reason'),
            validation_details={
                'documents_found': documents_found,
                'llm_validation': llm_validation,
                'strict_mode': True
            }
        )
    
    async def _llm_validation(
        self, user_query: str, ai_response: str, context: str, meili_context: str, mode: str
    ) -> Dict[str, Any]:
        """Validation par LLM"""
        
        # Prompt adaptatif selon le mode
        if mode == "strict":
            prompt = self._create_strict_validation_prompt(user_query, ai_response, context, meili_context)
        else:
            prompt = self._create_moderate_validation_prompt(user_query, ai_response, context, meili_context)
        
        try:
            # Appel LLM avec timeout
            judge_response = await asyncio.wait_for(
                self.llm_client.complete(prompt, temperature=0.0, max_tokens=200),
                timeout=10.0
            )
            
            # Analyse de la rÃ©ponse
            is_accepted = "ACCEPTER" in judge_response.upper()
            
            return {
                'is_safe': is_accepted,
                'judge_response': judge_response,
                'suggested_response': None if is_accepted else self._generate_safe_response(user_query),
                'rejection_reason': None if is_accepted else "Validation LLM Ã©chouÃ©e"
            }
            
        except asyncio.TimeoutError:
            logger.warning("[CONTEXT_GUARD] Timeout validation LLM")
            return {
                'is_safe': False,
                'judge_response': "TIMEOUT",
                'suggested_response': "Je ne peux pas valider cette rÃ©ponse actuellement.",
                'rejection_reason': "Timeout validation LLM"
            }
        except Exception as e:
            logger.error(f"[CONTEXT_GUARD] Erreur validation LLM: {e}")
            return {
                'is_safe': False,
                'judge_response': f"ERROR: {str(e)}",
                'suggested_response': "Erreur de validation.",
                'rejection_reason': f"Erreur LLM: {str(e)}"
            }
    
    def _create_strict_validation_prompt(self, user_query: str, ai_response: str, context: str, meili_context: str) -> str:
        """CrÃ©e un prompt de validation stricte"""
        return f"""Tu es un juge expert anti-hallucination TRÃˆS STRICT.

CONTEXTE:
- Question utilisateur: "{user_query}"
- Status documents: {"DOCUMENTS TROUVÃ‰S" if context or meili_context else "AUCUN DOCUMENT TROUVÃ‰"}
- Mode: VALIDATION STRICTE

RÃ‰PONSE Ã€ JUGER:
{ai_response}

DOCUMENTS DE RÃ‰FÃ‰RENCE:
{context[:1000] if context else "AUCUN CONTEXTE DISPONIBLE"}

RÃˆGLES STRICTES:
1. Si AUCUN DOCUMENT: REJETER immÃ©diatement
2. Tous les faits doivent Ãªtre dans les documents
3. AUCUNE invention de prix, stock, ou dÃ©tails techniques
4. Les chiffres doivent Ãªtre exacts

DÃ‰CISION: RÃ©ponds UNIQUEMENT par:
- "ACCEPTER" si la rÃ©ponse est fidÃ¨le aux documents
- "REJETER: [raison prÃ©cise]" si elle invente ou dÃ©forme des informations

Sois IMPITOYABLE - en cas de doute, REJETER."""
    
    def _create_moderate_validation_prompt(self, user_query: str, ai_response: str, context: str, meili_context: str) -> str:
        """CrÃ©e un prompt de validation modÃ©rÃ©e"""
        return f"""Tu es un juge expert anti-hallucination MODÃ‰RÃ‰.

CONTEXTE:
- Question utilisateur: "{user_query}"
- Status documents: {"DOCUMENTS TROUVÃ‰S" if context or meili_context else "AUCUN DOCUMENT TROUVÃ‰"}

RÃ‰PONSE Ã€ JUGER:
{ai_response}

DOCUMENTS DE RÃ‰FÃ‰RENCE:
{context[:500] if context else "AUCUN CONTEXTE DISPONIBLE"}

RÃˆGLES MODÃ‰RÃ‰ES:
1. Si documents disponibles: vÃ©rifier la cohÃ©rence
2. Si pas de documents: permettre rÃ©ponse gÃ©nÃ©rale
3. Rejeter seulement les affirmations manifestement fausses

DÃ‰CISION: RÃ©ponds UNIQUEMENT par:
- "ACCEPTER" si la rÃ©ponse est appropriÃ©e
- "REJETER: [raison prÃ©cise]" si elle contient des erreurs flagrantes"""
    
    def _generate_safe_response(self, user_query: str) -> str:
        """GÃ©nÃ¨re une rÃ©ponse sÃ©curisÃ©e de fallback"""
        safe_responses = {
            "greeting": "Bonjour ! Comment puis-je vous aider aujourd'hui ?",
            "question": "Je ne peux pas rÃ©pondre Ã  cette question avec certitude. Pouvez-vous contacter notre service client ?",
            "general": "Je ne suis pas sÃ»r de la rÃ©ponse. Pouvez-vous reformuler votre question ?"
        }
        
        # DÃ©tection simple du type de question
        if any(word in user_query.lower() for word in ['bonjour', 'salut', 'hello']):
            return safe_responses['greeting']
        elif '?' in user_query:
            return safe_responses['question']
        else:
            return safe_responses['general']
    
    def _generate_cache_key(self, user_query: str, ai_response: str, intent: IntentType) -> str:
        """GÃ©nÃ¨re une clÃ© de cache"""
        import hashlib
        key_data = f"{user_query}_{ai_response}_{intent.value}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def clear_cache(self):
        """Vide le cache de validation"""
        self.validation_cache.clear()
        logger.info("[CONTEXT_GUARD] Cache de validation vidÃ©")

# Instance globale
context_guard = ContextAwareHallucinationGuard()

# Fonction d'interface pour compatibilitÃ©
async def validate_response_contextual(
    user_query: str,
    ai_response: str,
    intent_result: IntentResult,
    supabase_results: List[Dict] = None,
    meili_results: List[Dict] = None,
    supabase_context: str = "",
    meili_context: str = "",
    company_id: str = ""
) -> ValidationResult:
    """Interface simplifiÃ©e pour la validation contextuelle"""
    return await context_guard.validate_response(
        user_query, ai_response, intent_result,
        supabase_results, meili_results, supabase_context, meili_context, company_id
    )

if __name__ == "__main__":
    # Test du garde-fou contextuel
    async def test_guard():
        from .advanced_intent_classifier import IntentType, IntentResult
        
        # Test avec intention sociale
        social_intent = IntentResult(
            primary_intent=IntentType.SOCIAL_GREETING,
            confidence=0.9,
            all_intents={},
            requires_documents=False,
            is_critical=False,
            fallback_required=False,
            context_hints=[],
            processing_time_ms=0.0
        )
        
        result = await validate_response_contextual(
            "Comment tu t'appelles ?",
            "Je m'appelle Gamma, votre assistant client.",
            social_intent
        )
        
        print(f"Validation sociale: {result.is_safe} (conf: {result.confidence_score:.2f})")
    
    asyncio.run(test_guard())
