#!/usr/bin/env python3
"""
üöÄ SYST√àME DE CACHE UNIFI√â SCALABLE
Objectif: R√©duire le temps total de 19.6s √† 7.3s
Architecture: Int√©gration de tous les caches avec monitoring unifi√©
"""

import asyncio
import time
from typing import Dict, Any, Optional
from datetime import datetime
import threading
from utils import log3

# Import des caches sp√©cialis√©s
from core.global_prompt_cache import get_global_prompt_cache
from core.global_embedding_cache_optimized import get_global_embedding_cache
from core.global_model_cache import get_global_model_cache

class UnifiedCacheSystem:
    """
    üöÄ Syst√®me de cache unifi√© pour toutes les entreprises
    - Coordination entre tous les caches
    - Monitoring global des performances
    - Pr√©chargement intelligent
    - Nettoyage automatique
    """
    
    def __init__(self):
        self.prompt_cache = get_global_prompt_cache()
        self.embedding_cache = get_global_embedding_cache()
        self.model_cache = get_global_model_cache()
        
        self.global_stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "total_time_saved_seconds": 0.0
        }
        
        self.lock = threading.RLock()
        log3("[UNIFIED_CACHE]", "üöÄ Syst√®me de cache unifi√© initialis√©")
    
    async def get_cached_prompt(self, company_id: str) -> Optional[str]:
        """
        üéØ R√©cup√®re un prompt avec cache optimis√©
        Gain attendu: 2.1s ‚Üí 0.01s
        """
        start_time = time.time()
        
        try:
            prompt = await self.prompt_cache.get_prompt(company_id)
            
            if prompt:
                time_saved = 2.1 - (time.time() - start_time)  # 2.1s = temps DB normal
                with self.lock:
                    self.global_stats["cache_hits"] += 1
                    self.global_stats["total_time_saved_seconds"] += time_saved
                
                # LOG SIMPLIFI√â - Une seule ligne
                log3("[UNIFIED_CACHE]", f"‚úÖ Prompt hit: {company_id[:8]}... | Saved: {time_saved:.3f}s")
            else:
                with self.lock:
                    self.global_stats["cache_misses"] += 1
            
            with self.lock:
                self.global_stats["total_requests"] += 1
            
            return prompt
            
        except Exception as e:
            log3("[UNIFIED_CACHE]", f"‚ùå Prompt error: {company_id[:8]}... | {str(e)}")
            return None
    
    async def get_cached_embedding(self, query: str, model_name: str = "sentence-transformers/all-mpnet-base-v2") -> Optional:
        """
        üéØ R√©cup√®re un embedding avec cache optimis√©
        Gain attendu: 1.9s ‚Üí 0.01s
        """
        start_time = time.time()
        
        try:
            embedding = await self.embedding_cache.get_embedding(query, model_name)
            
            if embedding is not None:
                time_saved = 1.9 - (time.time() - start_time)  # 1.9s = temps g√©n√©ration normal
                with self.lock:
                    self.global_stats["cache_hits"] += 1
                    self.global_stats["total_time_saved_seconds"] += time_saved
                
                log3("[UNIFIED_CACHE]", f"‚úÖ Embedding hit: '{query[:30]}...' | Saved: {time_saved:.3f}s")
            else:
                with self.lock:
                    self.global_stats["cache_misses"] += 1
            
            with self.lock:
                self.global_stats["total_requests"] += 1
            
            return embedding
            
        except Exception as e:
            log3("[UNIFIED_CACHE]", f"‚ùå Embedding error: '{query[:30]}...' | {str(e)}")
            return None
    
    def get_cached_model(self, model_name: str = "sentence-transformers/all-mpnet-base-v2"):
        """
        üéØ R√©cup√®re un mod√®le avec cache persistant
        Gain attendu: 8.3s ‚Üí 0s (apr√®s premier chargement)
        """
        start_time = time.time()
        
        try:
            model = self.model_cache.get_model(model_name)
            
            if model is not None:
                load_time = time.time() - start_time
                
                # Si chargement rapide = cache hit
                if load_time < 1.0:  # Moins d'1 seconde = cache hit
                    time_saved = 8.3 - load_time  # 8.3s = temps chargement normal
                    with self.lock:
                        self.global_stats["cache_hits"] += 1
                        self.global_stats["total_time_saved_seconds"] += time_saved
                    
                    log3("[UNIFIED_CACHE]", f"‚úÖ Model hit: {model_name.split('/')[-1]} | Saved: {time_saved:.3f}s")
                else:
                    # Premier chargement
                    with self.lock:
                        self.global_stats["cache_misses"] += 1
                    
                    log3("[UNIFIED_CACHE]", f"üÜï Model loaded: {model_name.split('/')[-1]} | Time: {load_time:.3f}s")
            
            with self.lock:
                self.global_stats["total_requests"] += 1
            
            return model
            
        except Exception as e:
            log3("[UNIFIED_CACHE]", f"‚ùå Model error: {model_name.split('/')[-1]} | {str(e)}")
            return None
    
    async def preload_company_resources(self, company_id: str, common_queries: list = None) -> Dict[str, bool]:
        """
        üöÄ Pr√©charge toutes les ressources d'une entreprise
        Id√©al pour les entreprises fr√©quemment utilis√©es
        """
        results = {
            "prompt_preloaded": False,
            "model_preloaded": False,
            "embeddings_preloaded": 0
        }
        
        try:
            # Pr√©charger le prompt
            prompt = await self.prompt_cache.get_prompt(company_id)
            if prompt:
                results["prompt_preloaded"] = True
            
            # Pr√©charger le mod√®le
            model = self.model_cache.get_model()
            if model:
                results["model_preloaded"] = True
            
            # Pr√©charger des embeddings pour queries communes
            if common_queries and model:
                for query in common_queries[:5]:  # Limiter √† 5 queries
                    try:
                        # G√©n√©rer et cacher l'embedding
                        embedding = model.encode([query])
                        if len(embedding) > 0:
                            await self.embedding_cache.set_embedding(query, embedding[0])
                            results["embeddings_preloaded"] += 1
                    except Exception:
                        continue
            
            log3("[UNIFIED_CACHE]", {
                "action": "preload_complete",
                "company_id": company_id,
                "results": results
            })
            
        except Exception as e:
            log3("[UNIFIED_CACHE]", {
                "action": "preload_error",
                "company_id": company_id,
                "error": str(e)
            })
        
        return results
    
    def get_global_stats(self) -> Dict[str, Any]:
        """üìä Statistiques globales de tous les caches"""
        with self.lock:
            total_requests = self.global_stats["total_requests"]
            hit_rate = (self.global_stats["cache_hits"] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                "unified_cache": {
                    "total_requests": total_requests,
                    "cache_hits": self.global_stats["cache_hits"],
                    "cache_misses": self.global_stats["cache_misses"],
                    "hit_rate_percent": f"{hit_rate:.1f}%",
                    "total_time_saved_seconds": f"{self.global_stats['total_time_saved_seconds']:.1f}s",
                    "average_time_saved_per_hit": f"{self.global_stats['total_time_saved_seconds'] / max(1, self.global_stats['cache_hits']):.3f}s"
                },
                "prompt_cache": self.prompt_cache.get_stats(),
                "embedding_cache": self.embedding_cache.get_stats(),
                "model_cache": self.model_cache.get_stats()
            }
    
    async def cleanup_all_caches(self) -> Dict[str, int]:
        """üßπ Nettoyage global de tous les caches"""
        results = {
            "prompt_expired": 0,
            "embedding_expired": 0,
            "model_cleanup": 0
        }
        
        try:
            # Nettoyer les caches expir√©s
            results["prompt_expired"] = self.prompt_cache.clear_expired()
            results["embedding_expired"] = self.embedding_cache.clear_expired()
            
            # Nettoyer les mod√®les peu utilis√©s
            model_cleanup = self.model_cache.force_cleanup()
            results["model_cleanup"] = model_cleanup.get("removed_models", 0)
            
            log3("[UNIFIED_CACHE]", {
                "action": "global_cleanup",
                "results": results
            })
            
        except Exception as e:
            log3("[UNIFIED_CACHE]", {
                "action": "cleanup_error",
                "error": str(e)
            })
        
        return results

# Instance globale singleton
_unified_cache_system = None

def get_unified_cache_system() -> UnifiedCacheSystem:
    """üöÄ Singleton pour le syst√®me de cache unifi√©"""
    global _unified_cache_system
    if _unified_cache_system is None:
        _unified_cache_system = UnifiedCacheSystem()
    return _unified_cache_system

# Fonction de commodit√© pour l'int√©gration facile
async def get_optimized_resources(company_id: str, query: str, model_name: str = "sentence-transformers/all-mpnet-base-v2"):
    """
    üéØ Fonction tout-en-un pour r√©cup√©rer toutes les ressources optimis√©es
    Retourne: (prompt, embedding, model)
    """
    cache_system = get_unified_cache_system()
    
    # R√©cup√©ration en parall√®le quand possible
    prompt_task = cache_system.get_cached_prompt(company_id)
    embedding_task = cache_system.get_cached_embedding(query, model_name)
    
    # Mod√®le en premier (synchrone)
    model = cache_system.get_cached_model(model_name)
    
    # Attendre les t√¢ches async
    prompt, embedding = await asyncio.gather(prompt_task, embedding_task, return_exceptions=True)
    
    # G√©rer les exceptions
    if isinstance(prompt, Exception):
        prompt = None
    if isinstance(embedding, Exception):
        embedding = None
    
    return prompt, embedding, model
