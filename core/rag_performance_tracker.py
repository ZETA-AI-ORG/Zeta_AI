#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â±ï¸ SYSTÃˆME DE TRACKING PERFORMANCE RAG
Mesure temps d'exÃ©cution et tokens rÃ©els de chaque Ã©tape
âœ… OPTIMISATION: Mode production avec tracking minimal
"""

import time
import os
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, field
import logging

logger = logging.getLogger(__name__)

# âœ… OPTIMISATION: DÃ©sactiver tracking dÃ©taillÃ© en production
ENABLE_DETAILED_TRACKING = os.getenv("ENABLE_DETAILED_TRACKING", "false").lower() == "true"


@dataclass
class StepTiming:
    """Timing d'une Ã©tape du RAG"""
    name: str
    start_time: float
    end_time: Optional[float] = None
    duration_ms: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def finish(self):
        """Termine le timing"""
        self.end_time = time.time()
        self.duration_ms = (self.end_time - self.start_time) * 1000


@dataclass
class LLMUsage:
    """Usage tokens et coÃ»t LLM"""
    model: str
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    prompt_cost: float
    completion_cost: float
    total_cost: float


class RAGPerformanceTracker:
    """
    Tracker de performance pour pipeline RAG complet
    Mesure chaque Ã©tape et calcule coÃ»ts rÃ©els
    """
    
    # CoÃ»ts par modÃ¨le ($/1M tokens)
    MODEL_COSTS = {
        "llama-3.3-70b-versatile": {
            "input": 0.59,
            "output": 0.79
        },
        "llama-3.1-8b-instant": {
            "input": 0.05,
            "output": 0.08
        },
        "openai/gpt-oss-120b": {
            "input": 0.50,
            "output": 0.70
        }
    }
    
    def __init__(self, request_id: str):
        self.request_id = request_id
        self.start_time = time.time()
        self.steps: List[StepTiming] = []
        self.current_step: Optional[StepTiming] = None
        self.llm_usage: Optional[LLMUsage] = None
        self.total_duration_ms: Optional[float] = None
    
    def start_step(self, name: str, **metadata):
        """DÃ©marre une nouvelle Ã©tape"""
        # âœ… OPTIMISATION: Skip si tracking dÃ©sactivÃ©
        if not ENABLE_DETAILED_TRACKING:
            return
            
        # Finir l'Ã©tape prÃ©cÃ©dente si existe
        if self.current_step and self.current_step.end_time is None:
            self.current_step.finish()
        
        # CrÃ©er nouvelle Ã©tape
        self.current_step = StepTiming(
            name=name,
            start_time=time.time(),
            metadata=metadata
        )
        self.steps.append(self.current_step)
        
        logger.debug(f"â±ï¸ [{self.request_id}] DÃ©but: {name}")
    
    def end_step(self, **metadata):
        """Termine l'Ã©tape courante"""
        # âœ… OPTIMISATION: Skip si tracking dÃ©sactivÃ©
        if not ENABLE_DETAILED_TRACKING:
            return
            
        if self.current_step:
            self.current_step.metadata.update(metadata)
            self.current_step.finish()
            logger.debug(
                f"â±ï¸ [{self.request_id}] Fin: {self.current_step.name} "
                f"({self.current_step.duration_ms:.2f}ms)"
            )
            self.current_step = None
    
    def record_llm_usage(
        self,
        model: str,
        prompt_tokens: int,
        completion_tokens: int
    ):
        """Enregistre l'usage LLM avec calcul de coÃ»t"""
        total_tokens = prompt_tokens + completion_tokens
        
        # RÃ©cupÃ©rer les coÃ»ts du modÃ¨le
        costs = self.MODEL_COSTS.get(model, {"input": 0, "output": 0})
        
        # Calculer coÃ»ts ($/1M tokens)
        prompt_cost = (prompt_tokens / 1_000_000) * costs["input"]
        completion_cost = (completion_tokens / 1_000_000) * costs["output"]
        total_cost = prompt_cost + completion_cost
        
        self.llm_usage = LLMUsage(
            model=model,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens,
            prompt_cost=prompt_cost,
            completion_cost=completion_cost,
            total_cost=total_cost
        )
        
        logger.info(
            f"ðŸ’° [{self.request_id}] LLM: {model} | "
            f"{prompt_tokens}+{completion_tokens}={total_tokens} tokens | "
            f"${total_cost:.6f}"
        )
    
    def finish(self):
        """Termine le tracking complet"""
        # Finir l'Ã©tape courante si existe
        if self.current_step and self.current_step.end_time is None:
            self.current_step.finish()
        
        # Calculer durÃ©e totale
        self.total_duration_ms = (time.time() - self.start_time) * 1000
    
    def get_summary(self) -> Dict[str, Any]:
        """Retourne un rÃ©sumÃ© complet"""
        return {
            "request_id": self.request_id,
            "total_duration_ms": round(self.total_duration_ms, 2) if self.total_duration_ms else None,
            "steps": [
                {
                    "name": step.name,
                    "duration_ms": round(step.duration_ms, 2) if step.duration_ms else None,
                    "metadata": step.metadata
                }
                for step in self.steps
            ],
            "llm_usage": {
                "model": self.llm_usage.model,
                "prompt_tokens": self.llm_usage.prompt_tokens,
                "completion_tokens": self.llm_usage.completion_tokens,
                "total_tokens": self.llm_usage.total_tokens,
                "prompt_cost_usd": round(self.llm_usage.prompt_cost, 6),
                "completion_cost_usd": round(self.llm_usage.completion_cost, 6),
                "total_cost_usd": round(self.llm_usage.total_cost, 6)
            } if self.llm_usage else None
        }
    
    def print_summary_red(self):
        """Affiche le rÃ©sumÃ© en ROUGE dans les logs"""
        # âœ… OPTIMISATION: Skip si tracking dÃ©sactivÃ©
        if not ENABLE_DETAILED_TRACKING:
            return
            
        RED = '\033[91m'
        BOLD = '\033[1m'
        RESET = '\033[0m'
        
        summary = self.get_summary()
        
        print(f"\n{RED}{BOLD}{'='*80}{RESET}")
        print(f"{RED}{BOLD}â±ï¸  PERFORMANCE TRACKING - {self.request_id}{RESET}")
        print(f"{RED}{BOLD}{'='*80}{RESET}")
        
        # DurÃ©e totale
        if summary['total_duration_ms']:
            print(f"{RED}ðŸ• DURÃ‰E TOTALE: {summary['total_duration_ms']:.2f}ms ({summary['total_duration_ms']/1000:.2f}s){RESET}")
        
        # Ã‰tapes
        print(f"\n{RED}{BOLD}ðŸ“Š Ã‰TAPES DU PIPELINE:{RESET}")
        for step in summary['steps']:
            duration = step['duration_ms'] if step['duration_ms'] else 'N/A'
            metadata_str = ""
            if step['metadata']:
                metadata_str = f" | {step['metadata']}"
            print(f"{RED}  â”œâ”€ {step['name']}: {duration}ms{metadata_str}{RESET}")
        
        # LLM Usage
        if summary['llm_usage']:
            llm = summary['llm_usage']
            print(f"\n{RED}{BOLD}ðŸ¤– USAGE LLM:{RESET}")
            print(f"{RED}  â”œâ”€ ModÃ¨le: {llm['model']}{RESET}")
            print(f"{RED}  â”œâ”€ Tokens prompt: {llm['prompt_tokens']:,}{RESET}")
            print(f"{RED}  â”œâ”€ Tokens completion: {llm['completion_tokens']:,}{RESET}")
            print(f"{RED}  â”œâ”€ Tokens total: {llm['total_tokens']:,}{RESET}")
            print(f"{RED}  â”œâ”€ CoÃ»t prompt: ${llm['prompt_cost_usd']:.6f}{RESET}")
            print(f"{RED}  â”œâ”€ CoÃ»t completion: ${llm['completion_cost_usd']:.6f}{RESET}")
            print(f"{RED}  â””â”€ COÃ›T TOTAL: ${llm['total_cost_usd']:.6f}{RESET}")
        
        # Analyse performance
        print(f"\n{RED}{BOLD}ðŸ“ˆ ANALYSE:{RESET}")
        if summary['steps']:
            slowest = max(summary['steps'], key=lambda x: x['duration_ms'] or 0)
            print(f"{RED}  â”œâ”€ Ã‰tape la plus lente: {slowest['name']} ({slowest['duration_ms']:.2f}ms){RESET}")
            
            total_steps_time = sum(s['duration_ms'] or 0 for s in summary['steps'])
            overhead = (summary['total_duration_ms'] or 0) - total_steps_time
            if overhead > 0:
                print(f"{RED}  â””â”€ Overhead systÃ¨me: {overhead:.2f}ms{RESET}")
        
        print(f"{RED}{BOLD}{'='*80}{RESET}\n")


# ============================================================================
# SINGLETON GLOBAL PAR REQUÃŠTE
# ============================================================================

_trackers: Dict[str, RAGPerformanceTracker] = {}


def get_tracker(request_id: str) -> RAGPerformanceTracker:
    """RÃ©cupÃ¨re ou crÃ©e un tracker pour une requÃªte"""
    if request_id not in _trackers:
        _trackers[request_id] = RAGPerformanceTracker(request_id)
    return _trackers[request_id]


def cleanup_tracker(request_id: str):
    """Nettoie un tracker aprÃ¨s utilisation"""
    if request_id in _trackers:
        del _trackers[request_id]


def get_all_trackers() -> Dict[str, RAGPerformanceTracker]:
    """Retourne tous les trackers actifs"""
    return _trackers.copy()
