#!/usr/bin/env python3
"""
üéØ HYDE D'INGESTION - ANALYSEUR INTELLIGENT DE DOCUMENTS
Analyse les documents lors de l'ingestion pour cr√©er un cache de mots-scores
Transforme le chatbot en expert sp√©cialis√© du domaine business
"""

import asyncio
import json
import re
from typing import Dict, List, Set, Tuple
from datetime import datetime
from groq import Groq
import os
from collections import Counter, defaultdict

# Import du nouveau syst√®me de scoring HyDE
from .hyde_word_scorer import HydeWordScorer

def log_hyde(message, data=None, level="INFO"):
    """Log ultra-d√©taill√© pour HyDE d'ingestion"""
    timestamp = datetime.now().strftime("%H:%M:%S")
    icons = {"INFO": "‚ÑπÔ∏è", "SUCCESS": "‚úÖ", "WARNING": "‚ö†Ô∏è", "ERROR": "‚ùå", "PROCESS": "üîÑ", "ANALYSIS": "üß†"}
    icon = icons.get(level, "üìù")
    print(f"{icon} [HYDE_ANALYZER][{timestamp}] {message}")
    if data:
        print(f"   üìä {json.dumps(data, indent=2, ensure_ascii=False)}")

class IngestionHydeAnalyzer:
    """
    Analyseur HyDE d'ingestion - Cr√©e un cache intelligent de mots-scores
    """
    
    def __init__(self, groq_api_key: str = None):
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        
        # Cache des scores de mots par entreprise
        self.word_scores_cache = {}
        
        # Statistiques d'analyse
        self.analysis_stats = {
            "documents_analyzed": 0,
            "words_scored": 0,
            "business_terms_found": 0,
            "stopwords_removed": 0
        }
    
    def get_word_score(self, word: str, company_id: str) -> float:
        """
        üéØ M√âTHODE MANQUANTE - R√©cup√®re le score d'un mot pour une entreprise
        """
        if company_id not in self.word_scores_cache:
            log_hyde(f"‚ö†Ô∏è Aucun cache trouv√© pour {company_id}, score par d√©faut", level="WARNING")
            return 5.0
        
        word_lower = word.lower()
        score = self.word_scores_cache[company_id].get(word_lower, 5.0)
        
        log_hyde(f"üìä Score r√©cup√©r√©: '{word}' = {score}")
        return score
        
        # Mots vides fran√ßais √©tendus
        self.french_stopwords = {
            # Articles
            "le", "la", "les", "un", "une", "des", "du", "de", "d", "l",
            # Pr√©positions
            "√†", "au", "aux", "avec", "dans", "par", "pour", "sur", "sous", "vers", "chez",
            # Conjonctions
            "et", "ou", "mais", "donc", "car", "ni", "que", "qui", "quoi", "dont", "o√π",
            # Pronoms
            "je", "tu", "il", "elle", "nous", "vous", "ils", "elles", "me", "te", "se", "nous", "vous",
            "mon", "ma", "mes", "ton", "ta", "tes", "son", "sa", "ses", "notre", "votre", "leur", "leurs",
            "ce", "cette", "ces", "cet", "celui", "celle", "ceux", "celles",
            # Verbes auxiliaires/fr√©quents
            "√™tre", "avoir", "faire", "aller", "venir", "voir", "savoir", "pouvoir", "vouloir", "devoir",
            "est", "sont", "√©tait", "√©taient", "sera", "seront", "a", "ont", "avait", "avaient", "aura", "auront",
            "fait", "font", "faisait", "faisaient", "fera", "feront",
            # Mots de liaison
            "alors", "ainsi", "aussi", "bien", "encore", "m√™me", "plus", "tr√®s", "tout", "tous", "toute", "toutes",
            "peut", "peuvent", "pourrait", "pourraient", "doit", "doivent", "devrait", "devraient",
            # Mots conversationnels
            "bonjour", "bonsoir", "salut", "merci", "svp", "sil", "vous", "plait", "pla√Æt",
            "euh", "bon", "voil√†", "voici", "donc", "enfin", "bref"
        }

    async def analyze_documents(self, documents: List[Dict], company_id: str):
        """
        üß† ANALYSE CORPUS DE DOCUMENTS POUR CACHE MOTS-SCORES
        """
        log_hyde(f"üß† D√âBUT ANALYSE CORPUS", {
            "nb_documents": len(documents),
            "company_id": company_id
        }, "ANALYSIS")
        
        # Extraire tout le texte des documents
        log_hyde(f"üìù EXTRACTION TEXTE EN COURS...", level="PROCESS")
        all_text = self._extract_all_text(documents)
        log_hyde(f"üìù TEXTE EXTRAIT", {
            "caract√®res_total": len(all_text),
            "taille_ko": f"{len(all_text)/1024:.1f} Ko"
        })
        
        # Tokeniser et nettoyer
        log_hyde(f"üßπ TOKENISATION ET NETTOYAGE...", level="PROCESS")
        words = self._tokenize_and_clean(all_text)
        log_hyde(f"üßπ NETTOYAGE TERMIN√â", {
            "mots_uniques": len(words),
            "stopwords_supprim√©s": self.analysis_stats["stopwords_removed"],
            "√©chantillon_mots": list(words)[:10]
        })
        
        # Analyser par batches avec Groq
        log_hyde(f"ü§ñ ANALYSE GROQ EN COURS...", level="PROCESS")
        word_scores = await self._analyze_words_with_groq(words, company_id)
        
        # Stocker dans le cache
        self.word_scores_cache[company_id] = word_scores
        
        # Mettre √† jour les statistiques
        self.analysis_stats["documents_analyzed"] += len(documents)
        self.analysis_stats["words_scored"] += len(word_scores)
        
        # Analyser les scores obtenus
        if word_scores:
            scores_values = list(word_scores.values())
            top_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:10]
            
            log_hyde(f"üéØ ANALYSE TERMIN√âE AVEC SUCC√àS", {
                "mots_scor√©s": len(word_scores),
                "score_moyen": f"{sum(scores_values)/len(scores_values):.2f}",
                "score_max": max(scores_values),
                "score_min": min(scores_values),
                "top_10_mots": dict(top_words)
            }, "SUCCESS")
        else:
            log_hyde(f"‚ö†Ô∏è AUCUN SCORE G√âN√âR√â", level="WARNING")

    def _extract_all_text(self, documents: List[Dict]) -> str:
        """Extrait tout le texte des documents"""
        all_text = ""
        for doc in documents:
            all_text += doc.get('searchable_text', '')
        return all_text

    def _tokenize_and_clean(self, text: str) -> List[str]:
        """Tokenise et nettoie le texte avec d√©tection automatique des nombres"""
        # Conversion en minuscules
        text = text.lower()
        
        # Suppression de la ponctuation et caract√®res sp√©ciaux
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Tokenisation
        words = text.split()
        
        # Cr√©er un scorer pour d√©tecter les nombres
        scorer = HydeWordScorer()
        
        # Filtrage avec d√©tection automatique des nombres
        filtered_words = []
        numbers_detected = 0
        
        for w in words:
            if len(w) <= 2:
                continue
            if w in self.french_stopwords:
                self.analysis_stats["stopwords_removed"] += 1
                continue
            # NOUVELLE LOGIQUE: Exclure automatiquement les nombres
            if scorer._is_numeric(w):
                numbers_detected += 1
                log_hyde(f"üî¢ Nombre exclu de l'analyse: '{w}'", level="INFO")
                continue
            
            filtered_words.append(w)
        
        log_hyde(f"üßπ NETTOYAGE AVEC D√âTECTION NOMBRES", {
            "mots_initiaux": len(words),
            "mots_filtr√©s": len(filtered_words),
            "stopwords_supprim√©s": self.analysis_stats["stopwords_removed"],
            "nombres_exclus": numbers_detected
        })
        
        return filtered_words

    async def _analyze_words_with_groq(self, words: Set[str], company_id: str) -> Dict[str, float]:
        """
        ü§ñ ANALYSE MOTS AVEC NOUVEAU SYST√àME HYDE + API GROQ
        """
        if not words:
            log_hyde(f"‚ùå AUCUN MOT √Ä ANALYSER", level="ERROR")
            return {}
        
        # Utiliser le nouveau syst√®me HyDE pour le scoring de base
        hyde_scorer = HydeWordScorer(self.groq_client)
        word_scores = {}
        words_list = list(words)
        
        log_hyde(f"ü§ñ ANALYSE HYBRIDE HYDE + GROQ", {
            "total_mots": len(words_list),
            "company_id": company_id,
            "m√©thode": "HyDE_scorer + Groq_fallback"
        }, "PROCESS")
        
        # Analyser chaque mot avec le nouveau syst√®me
        for word in words_list:
            try:
                # Utiliser le syst√®me HyDE am√©lior√©
                word_dict = await hyde_scorer.score_query_words(word, "e-commerce")
                score = word_dict.get(word, 5.0)  # Score par d√©faut si non trouv√©
                
                word_scores[word] = float(score)
                
            except Exception as e:
                log_hyde(f"‚ùå ERREUR SCORING '{word}'", {"erreur": str(e)}, "ERROR")
                word_scores[word] = 5.0  # Score neutre en cas d'erreur
        
        # Analyser les r√©sultats avec logs d√©taill√©s
        if word_scores:
            scores_values = list(word_scores.values())
            
            # Classement par cat√©gories de score (comme dans le nouveau syst√®me)
            score_categories = {
                "ESSENTIELS_10": [(w, s) for w, s in word_scores.items() if s == 10],
                "TRES_PERTINENTS_8_9": [(w, s) for w, s in word_scores.items() if 8 <= s < 10],
                "CONTEXTUELS_6_7": [(w, s) for w, s in word_scores.items() if 6 <= s < 8],
                "FAIBLE_PERTINENCE_3_5": [(w, s) for w, s in word_scores.items() if 3 <= s < 6],
                "STOP_WORDS_0_2": [(w, s) for w, s in word_scores.items() if s < 3]
            }
            
            log_hyde(f"üéØ ANALYSE HYBRIDE TERMIN√âE", {
                "total_mots_scor√©s": len(word_scores),
                "score_moyen": f"{sum(scores_values)/len(scores_values):.2f}",
                "r√©partition_scores": {
                    "üî• ESSENTIELS (10)": len(score_categories["ESSENTIELS_10"]),
                    "‚úÖ TR√àS PERTINENTS (8-9)": len(score_categories["TRES_PERTINENTS_8_9"]),
                    "‚ö†Ô∏è CONTEXTUELS (6-7)": len(score_categories["CONTEXTUELS_6_7"]),
                    "üî∏ FAIBLE PERTINENCE (3-5)": len(score_categories["FAIBLE_PERTINENCE_3_5"]),
                    "‚ùå STOP WORDS (0-2)": len(score_categories["STOP_WORDS_0_2"])
                }
            }, "SUCCESS")
            
            # Afficher les top mots par cat√©gorie
            for category, words_scores in score_categories.items():
                if words_scores:
                    top_words = dict(sorted(words_scores, key=lambda x: x[1], reverse=True)[:5])
                    log_hyde(f"üìä {category}", {"top_mots": top_words})
        
        return word_scores

    async def _score_words_batch(self, words: List[str], company_id: str) -> Dict[str, float]:
        """
        ü§ñ SCORE MOTS PAR BATCH AVEC API GROQ
        """
        if not words:
            log_hyde(f"‚ùå AUCUN MOT √Ä ANALYSER", level="ERROR")
            return {}
        
        prompt = f"""
Tu es un expert en analyse s√©mantique pour une entreprise avec ce profil:
- Secteur: e-commerce
- Produits: produits g√©n√©riques
- Zones: abidjan

Score ces mots de 0 √† 10 selon leur importance business:
- 10: Mots critiques (produits phares, prix, contact, zones principales)
- 8-9: Mots tr√®s importants (marques, services, paiement)
- 6-7: Mots utiles (caract√©ristiques, couleurs, mod√®les)
- 3-5: Mots peu importants
- 0-2: Mots sans int√©r√™t business

MOTS √Ä SCORER: {', '.join(words)}

‚ö†Ô∏è IMPORTANT: Les nombres/chiffres/montants doivent TOUJOURS avoir un score de 0.
Exemples: "3500", "5000", "1500fcfa" = score 0

R√©ponds UNIQUEMENT en JSON:
{{"mot1": score, "mot2": score, ...}}
"""
        
        try:
            response = self.groq_client.chat.completions.create(
                model="llama-3.1-8b-instant",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=500
            )
            
            # R√©cup√©rer et nettoyer la r√©ponse
            response_content = response.choices[0].message.content.strip()
            log_hyde(f"ü§ñ R√âPONSE GROQ BRUTE", {"content": response_content[:200]})
            
            # V√âRIFIER SI LA R√âPONSE EST VIDE
            if not response_content:
                log_hyde(f"‚ùå R√âPONSE GROQ VIDE", {"mots_batch": words[:3]}, "ERROR")
                return {}
            
            # Extraire le JSON de la r√©ponse
            try:
                # Chercher le JSON dans la r√©ponse
                import re
                json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    # NETTOYAGE SUPPL√âMENTAIRE DU JSON
                    json_str = json_str.replace('\n', '').replace('\r', '')
                    batch_scores = json.loads(json_str)
                    log_hyde(f"‚úÖ JSON EXTRAIT", {"nb_scores": len(batch_scores)})
                    return batch_scores
                else:
                    log_hyde(f"‚ùå AUCUN JSON TROUV√â", {"response": response_content}, "ERROR")
                    # FALLBACK: G√âN√âRATION DE SCORES PAR D√âFAUT
                    fallback_scores = {word: 5.0 for word in words[:10]}  # Score neutre
                    log_hyde(f"üîÑ FALLBACK SCORES", {"nb_scores": len(fallback_scores)}, "WARNING")
                    return fallback_scores
            except json.JSONDecodeError as je:
                log_hyde(f"‚ùå ERREUR PARSING JSON", {
                    "erreur": str(je),
                    "content": response_content[:100]
                }, "ERROR")
                # FALLBACK: G√âN√âRATION DE SCORES PAR D√âFAUT
                fallback_scores = {word: 5.0 for word in words[:10]}  # Score neutre
                log_hyde(f"üîÑ FALLBACK SCORES", {"nb_scores": len(fallback_scores)}, "WARNING")
                return fallback_scores
            
        except Exception as e:
            log_hyde(f"‚ùå ERREUR SCORING BATCH", {
                "erreur": str(e),
                "mots_batch": words[:3]
            }, "ERROR")
            return {}

    def save_word_cache(self, company_id: str) -> str:
        """
        üíæ SAUVEGARDE CACHE MOTS-SCORES DANS FICHIER JSON
        """
        if company_id not in self.word_scores_cache:
            log_hyde(f"‚ùå AUCUN CACHE TROUV√â POUR {company_id}", level="ERROR")
            return None
        
        cache_dir = "word_caches"
        os.makedirs(cache_dir, exist_ok=True)
        
        cache_file = os.path.join(cache_dir, f"word_scores_{company_id}.json")
        
        word_scores = self.word_scores_cache[company_id]
        
        # Analyser le cache avant sauvegarde
        if word_scores:
            scores_values = list(word_scores.values())
            top_words = sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:15]
            low_words = sorted(word_scores.items(), key=lambda x: x[1])[:5]
            
            cache_analysis = {
                "total_mots": len(word_scores),
                "score_moyen": f"{sum(scores_values)/len(scores_values):.2f}",
                "score_max": max(scores_values),
                "score_min": min(scores_values),
                "mots_haute_valeur": len([s for s in scores_values if s >= 8.0]),
                "mots_moyenne_valeur": len([s for s in scores_values if 5.0 <= s < 8.0]),
                "mots_faible_valeur": len([s for s in scores_values if s < 5.0])
            }
            
            log_hyde(f"üìä ANALYSE CACHE AVANT SAUVEGARDE", cache_analysis, "ANALYSIS")
            log_hyde(f"üèÜ TOP 15 MOTS BUSINESS", dict(top_words))
            log_hyde(f"üìâ 5 MOTS SCORE FAIBLE", dict(low_words))
        
        cache_data = {
            "company_id": company_id,
            "created_at": datetime.now().isoformat(),
            "word_scores": word_scores,
            "stats": self.analysis_stats.copy(),
            "cache_analysis": cache_analysis if word_scores else {}
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            file_size = os.path.getsize(cache_file)
            
            log_hyde(f"üíæ CACHE SAUVEGARD√â AVEC SUCC√àS", {
                "fichier": cache_file,
                "taille_fichier": f"{file_size/1024:.1f} Ko",
                "mots_sauvegard√©s": len(word_scores),
                "company_id": company_id
            }, "SUCCESS")
            
            return cache_file
            
        except Exception as e:
            log_hyde(f"‚ùå ERREUR SAUVEGARDE CACHE", {
                "erreur": str(e),
                "fichier_cible": cache_file
            }, "ERROR")
            return None

# Fonction utilitaire pour l'int√©gration
async def create_company_word_cache(documents: List[Dict], company_id: str) -> Dict:
    """
    Fonction principale pour cr√©er le cache de mots d'une entreprise
    """
    analyzer = IngestionHydeAnalyzer()
    return await analyzer.analyze_documents(documents, company_id)

if __name__ == "__main__":
    print("üéØ ANALYSEUR HYDE D'INGESTION")
    print("=" * 50)
    print("Utilisation: from core.ingestion_hyde_analyzer import create_company_word_cache")
