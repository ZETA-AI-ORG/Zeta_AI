"""
üöÄ RAG ENGINE OPTIMIS√â - ARCHITECTURE NOUVELLE G√âN√âRATION
Remplace compl√®tement l'ancien rag_engine.py avec une approche moderne et performante
"""
import asyncio
import time
import uuid
from typing import Dict, List, Optional, Any, Tuple

from core.semantic_search_engine import semantic_search, SearchConfig, SearchStrategy
from core.llm_client import GroqLLMClient
from database.supabase_client import get_company_system_prompt, get_company_context
from core.conversation import get_history, save_message
from core.cache_manager import cache_manager
from utils import log3, timing_metric, validate_context


class OptimizedRAGEngine:
    """
    üéØ MOTEUR RAG OPTIMIS√â
    
    Caract√©ristiques :
    - Architecture simplifi√©e et modulaire
    - Performance maximis√©e
    - Cache intelligent
    - Logs structur√©s
    - Gestion d'erreurs robuste
    """
    
    def __init__(self):
        self.llm_client = GroqLLMClient()
        self.request_cache: Dict[str, Any] = {}
    
    async def preprocess_query(self, message: str, chat_history: List[Dict]) -> str:
        """
        üîÑ PR√âTRAITEMENT INTELLIGENT DE LA REQU√äTE
        Enrichit la requ√™te avec le contexte conversationnel
        """
        try:
            # Si historique disponible, enrichir la requ√™te
            if chat_history and len(chat_history) > 0:
                # Prendre les 3 derniers √©changes pour le contexte
                recent_context = []
                for exchange in chat_history[-3:]:
                    if exchange.get('user_message'):
                        recent_context.append(f"User: {exchange['user_message']}")
                    if exchange.get('assistant_message'):
                        recent_context.append(f"Assistant: {exchange['assistant_message'][:100]}...")
                
                context_str = "\n".join(recent_context)
                
                # Enrichissement contextuel simple
                enriched_query = f"Contexte r√©cent:\n{context_str}\n\nNouvelle question: {message}"
                
                log3("[QUERY_PREPROCESSING]", f"‚úÖ Requ√™te enrichie avec {len(chat_history)} √©changes d'historique")
                return enriched_query
            
            return message
            
        except Exception as e:
            log3("[QUERY_PREPROCESSING]", f"‚ö†Ô∏è Erreur pr√©traitement: {str(e)}")
            return message
    
    async def generate_response(
        self, 
        query: str, 
        context: str, 
        company_id: str
    ) -> str:
        """
        ü§ñ G√âN√âRATION DE R√âPONSE LLM OPTIMIS√âE
        """
        try:
            # R√©cup√©ration du prompt syst√®me
            system_prompt = await get_company_system_prompt(company_id)
            company_context = await get_company_context(company_id)
            
            # Construction du prompt optimis√©
            company_name = company_context.get('companyName', 'Notre entreprise')
            ai_name = company_context.get('aiName', 'Assistant')
            
            # Prompt structur√©
            full_prompt = f"""Tu es {ai_name}, l'assistant IA de {company_name}.

CONTEXTE DOCUMENTAIRE:
{context}

INSTRUCTIONS:
- R√©ponds uniquement en te basant sur le contexte fourni
- Si l'information n'est pas dans le contexte, dis-le clairement
- Sois pr√©cis, utile et professionnel
- Utilise le nom de l'entreprise quand c'est pertinent

QUESTION: {query}

R√âPONSE:"""

            # G√©n√©ration avec le LLM
            response = await self.llm_client.complete(
                prompt=full_prompt,
                max_tokens=500,
                temperature=0.1  # R√©ponses plus d√©terministes
            )
            
            log3("[LLM_RESPONSE]", f"‚úÖ R√©ponse g√©n√©r√©e: {len(response)} caract√®res")
            return response
            
        except Exception as e:
            log3("[LLM_RESPONSE]", f"üí• Erreur g√©n√©ration: {type(e).__name__}: {str(e)}")
            return "Je rencontre une difficult√© technique. Pouvez-vous reformuler votre question ?"
    
    def should_use_cache(self, query: str, company_id: str, user_id: str) -> Optional[str]:
        """
        üóÑÔ∏è V√âRIFICATION CACHE INTELLIGENT
        """
        try:
            # Cl√© de cache bas√©e sur la requ√™te normalis√©e
            normalized_query = query.lower().strip()
            cache_key = f"rag_response:{company_id}:{hash(normalized_query)}"
            
            cached_response = cache_manager.get(cache_key)
            if cached_response:
                log3("[CACHE]", f"‚úÖ Cache hit pour requ√™te: {query[:50]}...")
                return cached_response
            
            return None
            
        except Exception as e:
            log3("[CACHE]", f"‚ö†Ô∏è Erreur cache: {str(e)}")
            return None
    
    def cache_response(self, query: str, company_id: str, response: str):
        """
        üíæ MISE EN CACHE DE LA R√âPONSE
        """
        try:
            normalized_query = query.lower().strip()
            cache_key = f"rag_response:{company_id}:{hash(normalized_query)}"
            
            # Cache pour 1 heure
            cache_manager.set(cache_key, response, ttl_seconds=3600)
            log3("[CACHE]", f"‚úÖ R√©ponse mise en cache: {cache_key}")
            
        except Exception as e:
            log3("[CACHE]", f"‚ö†Ô∏è Erreur mise en cache: {str(e)}")
    
    @timing_metric("rag_total_optimized")
    async def process_request(
        self, 
        company_id: str, 
        user_id: str, 
        message: str
    ) -> str:
        """
        üöÄ TRAITEMENT PRINCIPAL OPTIMIS√â
        Point d'entr√©e unique pour le traitement RAG
        """
        request_id = str(uuid.uuid4())[:8]
        
        log3("[RAG_OPTIMIZED]", {
            "request_id": request_id,
            "company_id": company_id,
            "user_id": user_id,
            "message_preview": message[:100],
            "message_length": len(message)
        })
        
        try:
            # 1. Validation des entr√©es
            if not message or not message.strip():
                return "Veuillez poser une question."
            
            # 2. V√©rification cache
            cached_response = self.should_use_cache(message, company_id, user_id)
            if cached_response:
                return cached_response
            
            # 3. R√©cup√©ration historique
            start_time = time.time()
            chat_history = await get_history(company_id, user_id)
            history_time = time.time() - start_time
            
            # 4. Pr√©traitement de la requ√™te
            start_time = time.time()
            processed_query = await self.preprocess_query(message, chat_history)
            preprocess_time = time.time() - start_time
            
            # 5. Recherche s√©mantique
            start_time = time.time()
            search_results, formatted_context = await semantic_search(
                query=processed_query,
                company_id=company_id,
                top_k=5,
                min_score=0.3
            )
            search_time = time.time() - start_time
            
            # 6. V√©rification des r√©sultats
            if not formatted_context or len(search_results) == 0:
                log3("[RAG_OPTIMIZED]", f"‚ùå Aucun contexte trouv√© pour: {message[:50]}...")
                
                # Gestion de la relance
                relance_key = f"no_context:{company_id}:{user_id}"
                if cache_manager.get(relance_key):
                    cache_manager.delete(relance_key)
                    return "Je vous mets en relation avec un conseiller qui pourra mieux vous aider."
                
                cache_manager.set(relance_key, "1", ttl_seconds=600)
                return "Je n'ai pas trouv√© d'information pr√©cise. Pouvez-vous reformuler votre question ?"
            
            # 7. G√©n√©ration de la r√©ponse
            start_time = time.time()
            response = await self.generate_response(
                query=message,  # Requ√™te originale pour la r√©ponse
                context=formatted_context,
                company_id=company_id
            )
            generation_time = time.time() - start_time
            
            # 8. Sauvegarde en cache
            self.cache_response(message, company_id, response)
            
            # 9. Sauvegarde de la conversation
            try:
                await save_message(company_id, user_id, message, response)
            except Exception as e:
                log3("[CONVERSATION_SAVE]", f"‚ö†Ô∏è Erreur sauvegarde: {str(e)}")
            
            # 10. M√©triques finales
            total_time = history_time + preprocess_time + search_time + generation_time
            
            log3("[RAG_OPTIMIZED]", {
                "request_id": request_id,
                "success": True,
                "results_count": len(search_results),
                "context_length": len(formatted_context),
                "response_length": len(response),
                "timing": {
                    "history_ms": round(history_time * 1000, 2),
                    "preprocess_ms": round(preprocess_time * 1000, 2),
                    "search_ms": round(search_time * 1000, 2),
                    "generation_ms": round(generation_time * 1000, 2),
                    "total_ms": round(total_time * 1000, 2)
                }
            })
            
            return response
            
        except Exception as e:
            log3("[RAG_OPTIMIZED]", {
                "request_id": request_id,
                "error": str(e),
                "error_type": type(e).__name__,
                "success": False
            })
            
            return "Je rencontre une difficult√© technique momentan√©e. Veuillez r√©essayer."


# Instance globale
_rag_engine: Optional[OptimizedRAGEngine] = None

def get_rag_engine() -> OptimizedRAGEngine:
    """Factory pour obtenir l'instance du moteur RAG"""
    global _rag_engine
    if _rag_engine is None:
        _rag_engine = OptimizedRAGEngine()
    return _rag_engine


# API de compatibilit√© avec l'ancien syst√®me
async def get_rag_response(company_id: str, user_id: str, message: str) -> str:
    """
    üéØ API PRINCIPALE POUR COMPATIBILIT√â
    Remplace l'ancienne fonction get_rag_response
    """
    engine = get_rag_engine()
    return await engine.process_request(company_id, user_id, message)


# Fonction de nettoyage
async def cleanup_rag_engine():
    """Nettoyage des ressources"""
    global _rag_engine
    if _rag_engine:
        # Nettoyage si n√©cessaire
        pass
