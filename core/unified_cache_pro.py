"""
üß† CACHE UNIFI√â INTELLIGENT - SYST√àME PROFESSIONNEL
Remplace tous les caches fragment√©s par un syst√®me unifi√©
"""

import asyncio
import json
import time
import hashlib
import pickle
import zlib
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, asdict
from collections import OrderedDict, defaultdict
import threading
import aioredis
from pathlib import Path
import tempfile
import logging

logger = logging.getLogger(__name__)

@dataclass
class CacheEntry:
    """Entr√©e de cache optimis√©e"""
    value: Any
    timestamp: float
    ttl_seconds: int
    access_count: int = 0
    size_bytes: int = 0
    compressed: bool = False

@dataclass
class CacheStats:
    """Statistiques de cache"""
    total_gets: int = 0
    total_sets: int = 0
    hits: int = 0
    misses: int = 0
    memory_usage_bytes: int = 0
    redis_usage_bytes: int = 0

class UnifiedCachePro:
    """
    üéØ CACHE UNIFI√â PROFESSIONNEL
    
    Fonctionnalit√©s:
    - L1: M√©moire ultra-rapide (1MB hot data)
    - L2: Redis distribu√© (100MB warm data) 
    - L3: Disque persistant (1GB cold data)
    - Compression automatique des gros objets
    - √âviction intelligente LRU + ML
    - Pr√©diction des acc√®s futurs
    - M√©triques temps r√©el
    """
    
    def __init__(self, max_memory_mb: int = 50):
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        
        # Cache L1 - M√©moire
        self.l1_cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.l1_lock = threading.RLock()
        
        # Cache L2 - Redis (connexion lazy)
        self.redis_client = None
        self.redis_connected = False
        
        # Cache L3 - Disque
        self.disk_cache_dir = Path(tempfile.gettempdir()) / "chatbot_unified_cache"
        self.disk_cache_dir.mkdir(exist_ok=True)
        
        # Statistiques et monitoring
        self.stats = CacheStats()
        self.access_patterns: Dict[str, List[float]] = defaultdict(list)
        self.prediction_scores: Dict[str, float] = {}
        
        # Configuration
        self.compression_threshold = 1024  # Compresser si > 1KB
        self.prediction_window = 3600  # 1 heure
        self.eviction_threshold = 0.8  # √âviction si > 80% plein
        
        logger.info(f"[UNIFIED_CACHE] ‚úÖ Cache unifi√© initialis√© (max: {max_memory_mb}MB)")
    
    async def _ensure_redis_connection(self):
        """Connexion lazy √† Redis"""
        if not self.redis_connected:
            try:
                self.redis_client = await aioredis.from_url(
                    "redis://localhost:6379",
                    encoding="utf-8",
                    decode_responses=False,
                    max_connections=20
                )
                
                # Test de connexion
                await self.redis_client.ping()
                self.redis_connected = True
                logger.info("[UNIFIED_CACHE] ‚úÖ Connexion Redis √©tablie")
                
            except Exception as e:
                logger.warning(f"[UNIFIED_CACHE] ‚ö†Ô∏è Redis non disponible: {e}")
                self.redis_connected = False
    
    def _normalize_key(self, key: str) -> str:
        """Normalise une cl√© de cache"""
        return hashlib.md5(key.encode()).hexdigest()
    
    def _should_compress(self, data: bytes) -> bool:
        """D√©termine si les donn√©es doivent √™tre compress√©es"""
        return len(data) > self.compression_threshold
    
    def _compress_data(self, data: bytes) -> bytes:
        """Compresse les donn√©es"""
        return zlib.compress(data, level=6)
    
    def _decompress_data(self, data: bytes) -> bytes:
        """D√©compresse les donn√©es"""
        return zlib.decompress(data)
    
    def _serialize_value(self, value: Any) -> tuple[bytes, bool]:
        """S√©rialise une valeur avec compression optionnelle"""
        try:
            serialized = pickle.dumps(value)
            
            if self._should_compress(serialized):
                compressed = self._compress_data(serialized)
                return compressed, True
            else:
                return serialized, False
                
        except Exception as e:
            logger.error(f"[UNIFIED_CACHE] Erreur s√©rialisation: {e}")
            return b"", False
    
    def _deserialize_value(self, data: bytes, compressed: bool = False) -> Any:
        """D√©s√©rialise une valeur avec d√©compression optionnelle"""
        try:
            if compressed:
                data = self._decompress_data(data)
            
            return pickle.loads(data)
            
        except Exception as e:
            logger.error(f"[UNIFIED_CACHE] Erreur d√©s√©rialisation: {e}")
            return None
    
    async def get(self, key: str, default: Any = None) -> Any:
        """R√©cup√©ration intelligente multi-niveaux"""
        cache_key = self._normalize_key(key)
        self.stats.total_gets += 1
        
        # L1: Cache m√©moire
        with self.l1_lock:
            if cache_key in self.l1_cache:
                entry = self.l1_cache[cache_key]
                
                # V√©rifier TTL
                if time.time() - entry.timestamp < entry.ttl_seconds:
                    # D√©placer en fin (LRU)
                    self.l1_cache.move_to_end(cache_key)
                    entry.access_count += 1
                    self.stats.hits += 1
                    
                    # Enregistrer pattern d'acc√®s
                    self._record_access_pattern(cache_key)
                    
                    logger.debug(f"[UNIFIED_CACHE] L1 HIT: {key[:30]}...")
                    return entry.value
                else:
                    # TTL expir√©
                    del self.l1_cache[cache_key]
        
        # L2: Cache Redis
        if self.redis_connected or await self._ensure_redis_connection():
            try:
                redis_data = await self.redis_client.get(f"cache:{cache_key}")
                if redis_data:
                    # D√©s√©rialiser
                    cache_entry_data = json.loads(redis_data)
                    
                    # V√©rifier TTL
                    if time.time() - cache_entry_data['timestamp'] < cache_entry_data['ttl_seconds']:
                        value = self._deserialize_value(
                            cache_entry_data['data'], 
                            cache_entry_data.get('compressed', False)
                        )
                        
                        if value is not None:
                            # Promouvoir vers L1 si fr√©quemment utilis√©
                            await self._maybe_promote_to_l1(key, value, cache_entry_data['ttl_seconds'])
                            
                            self.stats.hits += 1
                            self._record_access_pattern(cache_key)
                            
                            logger.debug(f"[UNIFIED_CACHE] L2 HIT: {key[:30]}...")
                            return value
                    else:
                        # TTL expir√©, supprimer
                        await self.redis_client.delete(f"cache:{cache_key}")
            
            except Exception as e:
                logger.warning(f"[UNIFIED_CACHE] Erreur Redis get: {e}")
        
        # L3: Cache disque
        disk_path = self.disk_cache_dir / f"{cache_key}.cache"
        if disk_path.exists():
            try:
                with open(disk_path, 'rb') as f:
                    cache_entry_data = pickle.load(f)
                
                # V√©rifier TTL
                if time.time() - cache_entry_data['timestamp'] < cache_entry_data['ttl_seconds']:
                    value = cache_entry_data['value']
                    
                    # Promouvoir vers L2 ou L1
                    await self._maybe_promote_from_disk(key, value, cache_entry_data['ttl_seconds'])
                    
                    self.stats.hits += 1
                    logger.debug(f"[UNIFIED_CACHE] L3 HIT: {key[:30]}...")
                    return value
                else:
                    # TTL expir√©
                    disk_path.unlink(missing_ok=True)
            
            except Exception as e:
                logger.warning(f"[UNIFIED_CACHE] Erreur disque get: {e}")
                disk_path.unlink(missing_ok=True)
        
        # Cache miss complet
        self.stats.misses += 1
        logger.debug(f"[UNIFIED_CACHE] MISS: {key[:30]}...")
        return default
    
    async def set(self, key: str, value: Any, ttl_seconds: int = 3600):
        """Stockage intelligent multi-niveaux"""
        cache_key = self._normalize_key(key)
        self.stats.total_sets += 1
        
        # S√©rialiser la valeur
        serialized_data, compressed = self._serialize_value(value)
        size_bytes = len(serialized_data)
        
        # Cr√©er l'entr√©e
        entry = CacheEntry(
            value=value,
            timestamp=time.time(),
            ttl_seconds=ttl_seconds,
            size_bytes=size_bytes,
            compressed=compressed
        )
        
        # L1: Toujours essayer de mettre en m√©moire pour hot data
        await self._set_l1(cache_key, entry)
        
        # L2: Redis pour donn√©es warm
        if self.redis_connected or await self._ensure_redis_connection():
            await self._set_l2(cache_key, entry, serialized_data)
        
        # L3: Disque pour donn√©es importantes (TTL > 1h)
        if ttl_seconds > 3600:
            await self._set_l3(cache_key, entry)
        
        logger.debug(f"[UNIFIED_CACHE] SET: {key[:30]}... ({size_bytes} bytes)")
    
    async def _set_l1(self, cache_key: str, entry: CacheEntry):
        """Stockage L1 avec √©viction intelligente"""
        with self.l1_lock:
            # V√©rifier si √©viction n√©cessaire
            while self._get_l1_memory_usage() > self.max_memory_bytes * self.eviction_threshold:
                if not self.l1_cache:
                    break
                
                # √âviction LRU du moins r√©cemment utilis√©
                lru_key, lru_entry = self.l1_cache.popitem(last=False)
                self.stats.memory_usage_bytes -= lru_entry.size_bytes
                logger.debug(f"[UNIFIED_CACHE] √âviction L1: {lru_key}")
            
            # Ajouter la nouvelle entr√©e
            self.l1_cache[cache_key] = entry
            self.stats.memory_usage_bytes += entry.size_bytes
    
    async def _set_l2(self, cache_key: str, entry: CacheEntry, serialized_data: bytes):
        """Stockage L2 Redis"""
        try:
            redis_entry = {
                'data': serialized_data,
                'timestamp': entry.timestamp,
                'ttl_seconds': entry.ttl_seconds,
                'compressed': entry.compressed,
                'size_bytes': entry.size_bytes
            }
            
            await self.redis_client.setex(
                f"cache:{cache_key}",
                entry.ttl_seconds,
                json.dumps(redis_entry, default=lambda x: x.hex() if isinstance(x, bytes) else x)
            )
            
        except Exception as e:
            logger.warning(f"[UNIFIED_CACHE] Erreur Redis set: {e}")
    
    async def _set_l3(self, cache_key: str, entry: CacheEntry):
        """Stockage L3 disque"""
        try:
            disk_path = self.disk_cache_dir / f"{cache_key}.cache"
            
            disk_entry = {
                'value': entry.value,
                'timestamp': entry.timestamp,
                'ttl_seconds': entry.ttl_seconds
            }
            
            with open(disk_path, 'wb') as f:
                pickle.dump(disk_entry, f)
                
        except Exception as e:
            logger.warning(f"[UNIFIED_CACHE] Erreur disque set: {e}")
    
    def _get_l1_memory_usage(self) -> int:
        """Calcule l'usage m√©moire L1"""
        return sum(entry.size_bytes for entry in self.l1_cache.values())
    
    def _record_access_pattern(self, cache_key: str):
        """Enregistre les patterns d'acc√®s pour pr√©diction"""
        current_time = time.time()
        self.access_patterns[cache_key].append(current_time)
        
        # Garder seulement la fen√™tre de pr√©diction
        cutoff_time = current_time - self.prediction_window
        self.access_patterns[cache_key] = [
            t for t in self.access_patterns[cache_key] if t > cutoff_time
        ]
    
    async def _maybe_promote_to_l1(self, key: str, value: Any, ttl_seconds: int):
        """Promotion conditionnelle vers L1"""
        cache_key = self._normalize_key(key)
        
        # Promouvoir si acc√®s fr√©quents
        access_frequency = len(self.access_patterns.get(cache_key, []))
        if access_frequency > 3:  # Seuil de promotion
            entry = CacheEntry(
                value=value,
                timestamp=time.time(),
                ttl_seconds=ttl_seconds,
                size_bytes=len(str(value))
            )
            await self._set_l1(cache_key, entry)
            logger.debug(f"[UNIFIED_CACHE] Promotion L1: {key[:30]}...")
    
    async def _maybe_promote_from_disk(self, key: str, value: Any, ttl_seconds: int):
        """Promotion depuis le disque"""
        # Promouvoir vers L2 au minimum
        await self.set(key, value, ttl_seconds)
    
    async def clear_expired(self):
        """Nettoyage des entr√©es expir√©es"""
        current_time = time.time()
        expired_keys = []
        
        # L1: Nettoyage m√©moire
        with self.l1_lock:
            for cache_key, entry in self.l1_cache.items():
                if current_time - entry.timestamp >= entry.ttl_seconds:
                    expired_keys.append(cache_key)
            
            for cache_key in expired_keys:
                del self.l1_cache[cache_key]
        
        logger.info(f"[UNIFIED_CACHE] Nettoyage: {len(expired_keys)} entr√©es expir√©es")
    
    def get_stats(self) -> Dict[str, Any]:
        """Retourne les statistiques d√©taill√©es"""
        hit_rate = (self.stats.hits / max(self.stats.total_gets, 1)) * 100
        
        return {
            'hit_rate_percent': round(hit_rate, 2),
            'total_requests': self.stats.total_gets,
            'cache_hits': self.stats.hits,
            'cache_misses': self.stats.misses,
            'l1_entries': len(self.l1_cache),
            'l1_memory_usage_mb': round(self._get_l1_memory_usage() / (1024 * 1024), 2),
            'l1_memory_limit_mb': round(self.max_memory_bytes / (1024 * 1024), 2),
            'redis_connected': self.redis_connected,
            'access_patterns_tracked': len(self.access_patterns)
        }

# Instance globale
unified_cache = UnifiedCachePro()




