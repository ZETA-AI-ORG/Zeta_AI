#!/usr/bin/env python3
"""
üåç RAG ENGINE UNIVERSEL 2024 - ARCHITECTURE FRANCOPHONE AVANC√âE
Pipeline complet : NLP ‚Üí Recherche Hybride ‚Üí Fusion ‚Üí Prompt Enrichi ‚Üí V√©rification QA
Optimis√© pour la compr√©hension fran√ßaise et l'e-commerce
"""

import asyncio
import time
import logging
import re
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

# Imports Live Mode (r√©els)
from .live_mode_manager import LiveModeManager

# Imports des nouveaux modules
try:
    from .french_nlp_processor import FrenchNLPProcessor, ProcessedQuery
    from .semantic_search_engine import OptimizedSemanticSearchEngine, SearchConfig
    from .intelligent_fusion import IntelligentFusionEngine, FusionConfig, fuse_search_results
    from .prompt_enricher import PromptEnricher, enrich_llm_prompt
    from .response_verifier import ResponseVerifier, VerificationLevel, verify_llm_response
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    logging.warning(f"‚ö†Ô∏è Modules avanc√©s non disponibles: {e}")
    ADVANCED_MODULES_AVAILABLE = False

logger = logging.getLogger(__name__)

@dataclass
class UniversalRAGResult:
    """R√©sultat du RAG universel enrichi"""
    response: str
    confidence: float
    documents_found: bool
    processing_time_ms: float
    search_method: str
    context_used: str
    # Nouveaux champs pour l'architecture avanc√©e
    nlp_analysis: Optional[Dict[str, Any]] = None
    fusion_metadata: Optional[Dict[str, Any]] = None
    verification_result: Optional[Dict[str, Any]] = None
    quality_score: Optional[float] = None
    # Champs pour debugging et validation
    thinking: str = ""
    validation: Optional[Dict[str, Any]] = None

class UniversalRAGEngine:
    """
    üåç RAG ENGINE UNIVERSEL FRANCOPHONE AVANC√â
    
    Architecture compl√®te :
    1. üá´üá∑ Pr√©traitement NLP fran√ßais (normalisation, lemmatisation, intentions, entit√©s)
    2. üîç Recherche hybride (MeiliSearch + S√©mantique + Fuzzy)
    3. üîÄ Fusion intelligente multi-sources
    4. üéØ Prompt enrichi avec intentions/entit√©s
    5. ü§ñ G√©n√©ration LLM optimis√©e
    6. ‚úÖ V√©rification QA et factualit√©
    7. üìä M√©triques et auditabilit√© compl√®tes
    """
    
    def __init__(self):
        self.llm_client = None
        self.embedding_model = None
        
        # Initialiser Supabase pour les prompts dynamiques
        try:
            from database.supabase_client import get_supabase_client
            self.supabase = get_supabase_client()
            logger.info("‚úÖ Connexion Supabase initialis√©e (prompts dynamiques)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Supabase non disponible: {e}")
            self.supabase = None
        
        # Nouveaux composants avanc√©s
        if ADVANCED_MODULES_AVAILABLE:
            self.nlp_processor = FrenchNLPProcessor()
            self.semantic_engine = OptimizedSemanticSearchEngine()
            self.fusion_engine = IntelligentFusionEngine()
            self.prompt_enricher = PromptEnricher()
            self.response_verifier = ResponseVerifier()
            logger.info("‚úÖ Architecture francophone avanc√©e initialis√©e")
        else:
            self.nlp_processor = None
            self.semantic_engine = None
            self.fusion_engine = None
            self.prompt_enricher = None
            self.response_verifier = None
            logger.warning("‚ö†Ô∏è Mode compatibilit√© - modules avanc√©s d√©sactiv√©s")
        
    async def initialize(self):
        """Initialise les composants"""
        try:
            # Import dynamique pour √©viter les erreurs
            from .llm_client import GroqLLMClient
            from embedding_models import get_embedding_model
            
            self.llm_client = GroqLLMClient()
            self.embedding_model = get_embedding_model()
            
            # Initialisation des modules avanc√©s
            if ADVANCED_MODULES_AVAILABLE:
                if self.semantic_engine:
                    self.semantic_engine.initialize()
                logger.info("‚úÖ RAG Universel Francophone Avanc√© initialis√©")
            else:
                logger.info("‚úÖ RAG Universel (mode compatibilit√©) initialis√©")
            
            return True
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation: {e}")
            return False
    
    async def advanced_search_pipeline(self, query: str, company_id: str, processed_query: Any = None) -> Dict[str, Any]:
        """
        üöÄ PIPELINE DE RECHERCHE HYBRIDE AVANC√â
        Combine MeiliSearch, recherche s√©mantique et fuzzy matching
        """
        if not ADVANCED_MODULES_AVAILABLE:
            return await self.search_sequential_sources(query, company_id)
        
        logger.info(f"üöÄ [ADVANCED_SEARCH] D√©but pipeline hybride: '{query[:50]}...'")
        
        # R√©sultats consolid√©s
        all_results = {
            'meili_results': [],
            'semantic_results': [],
            'fuzzy_results': [],
            'fused_results': [],
            'final_context': '',
            'total_documents': 0,
            'search_methods_used': [],
            'search_method': 'hybrid_advanced'
        }
        
        # 1. Recherche MeiliSearch (existante)
        try:
            from database.vector_store_clean_v2 import search_all_indexes_parallel
            
            meili_raw = await search_all_indexes_parallel(
                query=query,
                company_id=company_id,
                max_results=10
            )
            
            # Conversion au format standard
            if meili_raw and isinstance(meili_raw, str):
                # Parser les r√©sultats MeiliSearch
                meili_docs = []
                for i, doc_text in enumerate(meili_raw.split('\n\n')):
                    if doc_text.strip():
                        meili_docs.append({
                            'id': f'meili_{i}',
                            'content': doc_text.strip(),
                            'score': 0.8,  # Score par d√©faut
                            'metadata': {'source': 'meilisearch'}
                        })
                all_results['meili_results'] = meili_docs
                all_results['search_methods_used'].append('meilisearch')
                logger.info(f"üîç [MEILI] {len(meili_docs)} documents trouv√©s")
            
        except Exception as e:
            logger.error(f"‚ùå [MEILI] Erreur: {e}")
        
        # 2. Recherche s√©mantique avanc√©e
        if self.semantic_engine:
            try:
                config = SearchConfig(top_k=5, min_score=0.3, enable_reranking=True)
                semantic_results, _ = await self.semantic_engine.search(query, company_id, config)
                
                # Conversion au format standard
                semantic_docs = []
                for result in semantic_results:
                    semantic_docs.append({
                        'id': result.id,
                        'content': result.content,
                        'score': result.score,
                        'metadata': {**result.metadata, 'source': 'semantic'}
                    })
                
                all_results['semantic_results'] = semantic_docs
                all_results['search_methods_used'].append('semantic')
                logger.info(f"üß† [SEMANTIC] {len(semantic_docs)} documents trouv√©s")
                
            except Exception as e:
                logger.error(f"‚ùå [SEMANTIC] Erreur: {e}")
        
        # 3. Recherche fuzzy sur requ√™tes reformul√©es
        if processed_query.split_queries and len(processed_query.split_queries) > 1:
            try:
                fuzzy_docs = []
                for split_query in processed_query.split_queries[:3]:  # Max 3 sous-requ√™tes
                    # Recherche simple sur chaque sous-requ√™te
                    sub_results = await self._fuzzy_search_fallback(split_query, company_id)
                    fuzzy_docs.extend(sub_results)
                
                all_results['fuzzy_results'] = fuzzy_docs
                if fuzzy_docs:
                    all_results['search_methods_used'].append('fuzzy_multi_intent')
                    logger.info(f"üîÄ [FUZZY] {len(fuzzy_docs)} documents trouv√©s")
                
            except Exception as e:
                logger.error(f"‚ùå [FUZZY] Erreur: {e}")
        
        # 4. Fusion intelligente des r√©sultats
        if self.fusion_engine and any([all_results['meili_results'], all_results['semantic_results'], all_results['fuzzy_results']]):
            try:
                fused_results, formatted_context = fuse_search_results(
                    all_results['meili_results'],
                    all_results['semantic_results'],
                    all_results['fuzzy_results'],
                    query
                )
                
                all_results['fused_results'] = [r.__dict__ for r in fused_results]  # Conversion dataclass
                all_results['final_context'] = formatted_context
                all_results['total_documents'] = len(fused_results)
                
                logger.info(f"üîÄ [FUSION] {len(fused_results)} documents fusionn√©s")
                
            except Exception as e:
                logger.error(f"‚ùå [FUSION] Erreur: {e}")
                # Fallback : concat√©nation simple
                all_contexts = []
                for results in [all_results['meili_results'], all_results['semantic_results'], all_results['fuzzy_results']]:
                    for doc in results:
                        all_contexts.append(doc.get('content', ''))
                all_results['final_context'] = '\n\n'.join(all_contexts[:5])
                all_results['total_documents'] = len(all_contexts)
        
        return all_results
    
    async def _fuzzy_search_fallback(self, query: str, company_id: str) -> List[Dict[str, Any]]:
        """Recherche fuzzy simple en fallback"""
        try:
            # Utilisation de la recherche Supabase OPTIMIS√âE
            from .supabase_optimized import search_documents
            
            supabase_results = await search_documents(
                query=query,
                company_id=company_id,
                limit=3
            )
            
            fuzzy_docs = []
            for doc in supabase_results:
                fuzzy_docs.append({
                    'id': doc.get('id', ''),
                    'content': doc.get('content', ''),
                    'score': doc.get('similarity_score', 0.5),
                    'metadata': {**doc.get('metadata', {}), 'source': 'fuzzy_supabase'}
                })
            
            return fuzzy_docs
            
        except Exception as e:
            logger.error(f"‚ùå [FUZZY_FALLBACK] Erreur: {e}")
            return []

    async def search_sequential_sources(self, query: str, company_id: str, last_user_message: str = None, user_id: str = None) -> Dict[str, Any]:
        """üéØ RECHERCHE S√âQUENTIELLE CLASSIQUE (Mode compatibilit√©)"""
        # ========== TRACKING ==========
        try:
            from core.rag_performance_tracker import get_tracker
            tracker = get_tracker(getattr(self, '_request_id', 'unknown'))
            tracker.start_step("search_sources", query_length=len(query))
        except:
            tracker = None
        
        results = {
            'supabase_context': '',
            'meili_context': '',
            'total_documents': 0,
            'search_methods_used': [],
            'search_method': 'sequential_classic'
        }
        
        # === APPEL HYDE POUR REFORMULATION DOCUMENTAIRE ===
        clarified_query = query
        try:
            from core.hyde_word_scorer import clarify_request_with_hyde
            if last_user_message:
                clarified = await clarify_request_with_hyde(query, last_user_message)
            else:
                clarified = await clarify_request_with_hyde(query, "")
            if clarified:
                print(f"[HYDE] Requ√™te documentaire clarifi√©e : {clarified}")
                clarified_query = clarified
            else:
                print("[HYDE] Pas de clarification, on garde la requ√™te d'origine.")
        except Exception as e:
            print(f"[HYDE] Erreur appel Hyde : {e}")
        
        # üßπ √âTAPE 0: PREPROCESSING AVANC√â
        print(f"üßπ [√âTAPE 0] Preprocessing: '{clarified_query[:30]}...'")
        # Initialisation par d√©faut
        filtered_query = clarified_query
        query_combinations = [[word] for word in clarified_query.split()]
        
        try:
            from .smart_stopwords import filter_query_for_meilisearch
            from .query_combinator import generate_query_combinations
            
            # Filtrage stop words
            filtered_query = filter_query_for_meilisearch(clarified_query)
            print(f"üìù Stop words supprim√©s: '{filtered_query}'")
            
            # G√©n√©ration N-grammes
            query_combinations = generate_query_combinations(filtered_query)
            print(f"üî§ N-grammes: {len(query_combinations)} combinaisons")
            
        except Exception as e:
            print(f"‚ùå Erreur preprocessing: {str(e)[:50]}")
            # Les valeurs par d√©faut sont d√©j√† d√©finies
        
        # üéØ √âTAPE 1: RECHERCHE MEILISEARCH (PARALL√àLE GLOBALE - AUCUN EARLY EXIT)
        print(f"üîç [√âTAPE 1] MeiliSearch - Recherche parall√®le globale...")
        meili_success = False
        try:
            # Import de la nouvelle fonction de recherche parall√®le globale
            from database.vector_store_clean_v2 import search_all_indexes_parallel
            
            # Recherche EXHAUSTIVE dans TOUS les index en parall√®le
            meili_results = await search_all_indexes_parallel(
                query=filtered_query,  # IMPORTANT: requ√™te HYDE + stopwords filtr√©s ‚Üí n-grams propres
                company_id=company_id,
                limit=10
            )
            
            if meili_results and len(meili_results.strip()) > 0:
                # Le contexte est d√©j√† format√© par search_all_indexes_parallel
                results['meili_context'] = meili_results
                results['total_documents'] = meili_results.count('DOCUMENT #')
                results['search_methods_used'].append('meili_parallel_global')
                results['search_method'] = 'meilisearch_parallel_global'
                meili_success = True
                print(f"‚úÖ [√âTAPE 1] MeiliSearch Parall√®le Globale OK: {results['total_documents']} docs")
            else:
                print(f"‚ùå [√âTAPE 1] MeiliSearch: Aucun r√©sultat ‚Üí Fallback Supabase")
                
        except Exception as e:
            print(f"‚ùå MeiliSearch erreur: {str(e)[:50]} ‚Üí Fallback")
        
        # üîÑ √âTAPE FALLBACK: RECHERCHE SUPABASE (si MeiliSearch √©choue)
        if not meili_success:
            print(f"üîÑ [√âTAPE 2] Supabase fallback...")
            try:
                # ‚úÖ PHASE 1: Utiliser singleton pr√©-charg√© (pas nouvelle instance!)
                from .supabase_optimized_384 import get_supabase_optimized_384
                supabase = get_supabase_optimized_384(use_float16=True)
                
                # Recherche s√©mantique avec query originale
                supabase_docs = await supabase.search_documents(
                    query=query,
                    company_id=company_id,
                    limit=10  # Augment√© pour avoir plus de docs √† rescorer
                )
                
                if supabase_docs:
                    # ‚úÖ RESCORING + FILTRAGE + EXTRACTION PR√âCISE (PARALL√âLIS√â)
                    try:
                        import asyncio
                        from core.smart_metadata_extractor import rescore_documents, filter_by_dynamic_threshold, detect_query_intentions, get_company_boosters
                        from core.context_extractor import extract_relevant_context
                        from core.conversation_notepad import get_conversation_notepad
                        
                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        # PARALL√âLISATION: Notepad + Intentions + Boosters
                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        async def get_notepad_data():
                            if user_id:
                                notepad = get_conversation_notepad()
                                notepad_data = notepad.get_notepad(user_id, company_id)
                                return {
                                    "produit": notepad_data.get("product", {}).get("name", ""),
                                    "zone": notepad_data.get("delivery", {}).get("zone", "")
                                }
                            return {}
                        
                        async def get_intentions():
                            return detect_query_intentions(query)
                        
                        async def get_boosters():
                            return get_company_boosters(company_id) if company_id else None
                        
                        # Ex√©cution parall√®le
                        print("‚ö° [PARALLEL] Lancement notepad + intentions + boosters...")
                        user_context, intentions, boosters = await asyncio.gather(
                            get_notepad_data(),
                            get_intentions(),
                            get_boosters()
                        )
                        print(f"‚ö° [PARALLEL] Termin√© - Intentions: {list(intentions.get('categories', []))}")
                        
                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        # ‚úÖ AM√âLIORATION 4: PIPELINE OPTIMIS√â (Rescoring + Extraction parall√®les)
                        # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                        
                        async def rescore_and_filter():
                            """Rescoring + filtrage"""
                            docs = rescore_documents(supabase_docs, query, user_context, company_id)
                            return filter_by_dynamic_threshold(docs)
                        
                        async def extract_context():
                            """Extraction contexte (peut d√©marrer en parall√®le)"""
                            # Attend rescoring termin√©
                            filtered = await rescore_and_filter()
                            return extract_relevant_context(filtered, intentions, query, user_context)
                        
                        # Ex√©cution optimis√©e
                        supabase_docs = await extract_context()
                        print(f"‚ö° [OPTIMIZED] Rescoring + Filtrage + Extraction termin√©s")
                        print(f"üîç [FILTRAGE] {len(supabase_docs)} docs retenus apr√®s pipeline optimis√©")
                        
                    except Exception as e:
                        print(f"‚ö†Ô∏è [RESCORING] Erreur: {e} - Utilisation docs bruts")
                        import traceback
                        traceback.print_exc()
                    
                    # ‚úÖ AM√âLIORATION 5: D√©tection ambigu√Øt√©
                    try:
                        from core.ambiguity_detector import detect_ambiguity, format_ambiguity_message
                        
                        is_ambiguous, ambiguity_type, options = detect_ambiguity(query, supabase_docs)
                        if is_ambiguous:
                            ambiguity_msg = format_ambiguity_message(ambiguity_type, options)
                            print(f"‚ö†Ô∏è [AMBIGU√èT√â] D√©tect√©e: {ambiguity_type} - {len(options)} options")
                            # Injecter dans le contexte
                            results['ambiguity_detected'] = True
                            results['ambiguity_message'] = ambiguity_msg
                        else:
                            results['ambiguity_detected'] = False
                    except Exception as e:
                        print(f"‚ö†Ô∏è [AMBIGU√èT√â] Erreur d√©tection: {e}")
                        results['ambiguity_detected'] = False
                    
                    # Assemblage contexte Supabase avec titres fixes
                    supabase_context = self._format_supabase_context(supabase_docs)
                    
                    # Ajouter message ambigu√Øt√© si d√©tect√©
                    if results.get('ambiguity_detected'):
                        supabase_context = results['ambiguity_message'] + "\n\n" + supabase_context
                    
                    results['supabase_context'] = supabase_context
                    results['total_documents'] = len(supabase_docs)
                    results['search_methods_used'].append('supabase_semantic')
                    results['search_method'] = 'supabase_fallback'
                    print(f"‚úÖ Supabase OK: {len(supabase_docs)} docs, {len(supabase_context)} chars")
                else:
                    # M√™me si 0 r√©sultats, on marque que Supabase a √©t√© utilis√©
                    results['search_method'] = 'supabase_fallback'
                    results['search_methods_used'].append('supabase_semantic')
                    print("‚ùå Supabase: 0 r√©sultats")
                    
            except Exception as e:
                # M√™me en cas d'erreur, on marque la tentative Supabase
                results['search_method'] = 'supabase_fallback'
                results['search_methods_used'].append('supabase_error')
                print(f"‚ùå Supabase erreur: {str(e)[:50]}")
        
        # ========== FIN TRACKING SEARCH ==========
        if tracker:
            tracker.end_step(
                documents_found=results['total_documents'],
                search_method=results['search_method']
            )
        
        return results
    
    async def generate_response(self, query: str, search_results: Dict[str, Any], company_id: str, company_name: str = "notre entreprise", user_id: str = None, images=None) -> str:
        """ü§ñ G√©n√®re une r√©ponse avec prompt dynamique Supabase et m√©moire conversationnelle"""
        
        # ========== R√âCUP√âRATION PROMPT DYNAMIQUE (AVANT TRAITEMENT IMAGE) ==========
        try:
            dynamic_prompt = await self._get_dynamic_prompt(company_id, company_name)
            print("‚úÖ Prompt dynamique r√©cup√©r√©")
            
            # Extraire configuration entreprise du prompt
            prompt_config = self._extract_config_from_prompt(dynamic_prompt)
            print(f"üìã [CONFIG EXTRAITE] Wave: {prompt_config['wave_phone']}, Acompte: {prompt_config['required_amount']} FCFA")
        except Exception as e:
            print(f"‚ö†Ô∏è Prompt par d√©faut: {str(e)[:30]}")
            dynamic_prompt = f"Tu es un assistant client professionnel pour {company_name}."
            prompt_config = {'wave_phone': None, 'required_amount': None}
        
        # ========== ANALYSE IMAGE SI PR√âSENTE ==========
        image_context = ""
        if images and len(images) > 0:
            print("üì∏ [IMAGE] D√©tect√©e, analyse via Botlive...")
            try:
                from core.image_analyzer import analyze_image_for_rag
                import requests
                import base64
                
                # T√©l√©charger l'image depuis l'URL
                image_url = images[0]
                print(f"üì• [IMAGE] T√©l√©chargement: {image_url[:80]}...")
                
                response = requests.get(image_url, timeout=15)
                response.raise_for_status()
                
                # Convertir en base64
                image_base64 = base64.b64encode(response.content).decode('utf-8')
                print(f"‚úÖ [IMAGE] T√©l√©charg√©e et convertie en base64 ({len(image_base64)} chars)")
                
                # Analyser l'image avec config entreprise
                image_analysis = await analyze_image_for_rag(
                    image_data=image_base64,
                    user_id=user_id or "unknown",
                    context=query,
                    company_phone=prompt_config.get('wave_phone'),
                    required_amount=prompt_config.get('required_amount')
                )
                
                print(f"‚úÖ [IMAGE] Type: {image_analysis['type']}, Confiance: {image_analysis.get('confidence', 0):.2f}")
                
                # ========== FALLBACK AUTOMATIQUE VERS BOTLIVE POUR PAIEMENTS ==========
                if image_analysis["type"] == "payment":
                    print("üí≥ [FALLBACK] Image de paiement d√©tect√©e ‚Üí Redirection vers Botlive")
                    try:
                        from core.botlive_rag_hybrid import botlive_hybrid
                        
                        # Construire contexte pour Botlive
                        payment_data = image_analysis["data"]
                        
                        # Convertir amount en int (Botlive attend un int, pas une string)
                        amount = payment_data.get('amount', 0)
                        if isinstance(amount, str):
                            try:
                                amount = int(amount)
                            except (ValueError, TypeError):
                                amount = 0
                        
                        # Utiliser config extraite du prompt (avec fallback)
                        wave_phone = prompt_config.get('wave_phone') or '225 07 87 36 07 57'
                        required_amount = prompt_config.get('required_amount') or 2000
                        
                        context = {
                            'detected_objects': [],
                            'filtered_transactions': [{
                                'amount': amount,
                                'currency': 'FCFA',
                                'phone': payment_data.get('phone', ''),
                                'verified': payment_data.get('verified', False)
                            }],
                            'expected_deposit': required_amount,
                            'company_phone': wave_phone
                        }
                        
                        # Appeler Botlive
                        print(f"üîÑ [FALLBACK] Appel Botlive avec context: {context}")
                        botlive_result = await botlive_hybrid.process_request(
                            user_id=user_id or "unknown",
                            message=query,
                            context=context,
                            conversation_history="",
                            company_id=company_id
                        )
                        
                        print(f"‚úÖ [FALLBACK] R√©ponse Botlive re√ßue, retour au RAG")
                        return botlive_result.get('response', 'Erreur traitement paiement')
                        
                    except Exception as fallback_error:
                        print(f"‚ùå [FALLBACK] Erreur Botlive: {fallback_error}")
                        # Continuer avec RAG normal en cas d'erreur
                
                # Construire contexte selon type
                if image_analysis["type"] == "payment":
                    payment_data = image_analysis["data"]
                    image_context = f"""

üì∏ PREUVE PAIEMENT RE√áUE:
- Montant: {payment_data.get('amount', 0)} FCFA
- Num√©ro: {payment_data.get('phone', 'Non d√©tect√©')}
- Transaction: {payment_data.get('transaction_id', 'Non d√©tect√©')}
- Statut: {'‚úÖ VALID√â (‚â•2000 FCFA)' if payment_data.get('verified') else '‚ùå INSUFFISANT (<2000 FCFA)'}
- V√©rifi√© le: {payment_data.get('verified_at', 'N/A')}

INSTRUCTION: Confirme ou refuse la commande selon le statut. Sois naturel et professionnel.
"""
                
                elif image_analysis["type"] == "product":
                    product_data = image_analysis["data"]
                    image_context = f"""

üì∏ IMAGE PRODUIT RE√áUE:
- Produit: {product_data.get('product_name', 'Non identifi√©')}
- Type: {product_data.get('product_type', 'Non d√©tect√©')}
- Taille: {product_data.get('size', 'Non d√©tect√©e')}
- Quantit√© visible: {product_data.get('quantity_visible', 'Non d√©tect√©e')}
- Marque: {product_data.get('brand', 'Non d√©tect√©e')}
- √âtat: {product_data.get('condition', 'Non d√©tect√©')}

INSTRUCTION: Tu PEUX maintenant parler de ces d√©tails pr√©cis sur le produit. 
R√©ponds naturellement √† la question du client.
"""
                
                else:
                    image_context = f"""

üì∏ IMAGE RE√áUE (type non identifi√©):
Analyse: {image_analysis.get('data', {}).get('raw_text', 'Aucune analyse disponible')}

INSTRUCTION: Demande poliment au client de pr√©ciser ce qu'il souhaite savoir sur cette image.
"""
                
                print(f"üìù [IMAGE_CONTEXT] Ajout√© au prompt ({len(image_context)} chars)")
                
            except Exception as e:
                print(f"‚ùå [IMAGE_ERROR] Erreur analyse: {e}")
                image_context = "\n\n‚ö†Ô∏è Image re√ßue mais analyse √©chou√©e. Demande au client de la renvoyer."
        
        # ========== NOTEPAD CONVERSATIONNEL: EXTRACTION AUTOMATIQUE ==========
        try:
            from .conversation_notepad import (
                get_conversation_notepad,
                extract_product_info,
                extract_delivery_zone,
                extract_phone_number,
                extract_price_from_response
            )
            
            notepad = get_conversation_notepad()
            
            # Extraction automatique des infos de la requ√™te utilisateur
            product_info = extract_product_info(query)
            if product_info:
                # Extraire le prix de la r√©ponse LLM (on le fera apr√®s g√©n√©ration)
                logger.info(f"üìù Produit d√©tect√©: {product_info}")
            
            delivery_zone = extract_delivery_zone(query)
            if delivery_zone:
                logger.info(f"üöö Zone d√©tect√©e: {delivery_zone}")
            
            phone = extract_phone_number(query)
            if phone:
                notepad.update_phone(user_id, company_id, phone)
                logger.info(f"üìû T√©l√©phone enregistr√©: {phone}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur extraction notepad: {e}")
        
        # ========== EXTRACTION INTELLIGENTE ZONE LIVRAISON (REGEX + NORMALISATION) ==========
        delivery_context = ""
        delivery_zone_found = False  # ‚úÖ Flag pour filtrage docs
        
        try:
            from core.delivery_zone_extractor import (
                get_delivery_cost_smart,
                format_delivery_info
            )
            
            # Extraire zone de la requ√™te avec normalisation
            delivery_info = get_delivery_cost_smart(query)
            
            if delivery_info.get("cost"):
                # Zone trouv√©e! Injecter dans notepad
                from core.conversation_notepad import get_conversation_notepad
                notepad = get_conversation_notepad()
                notepad.update_delivery(
                    user_id, 
                    company_id, 
                    delivery_info["name"], 
                    delivery_info["cost"]
                )
                logger.info(f"üöö Zone extraite: {delivery_info['name']} = {delivery_info['cost']} FCFA (source: {delivery_info['source']})")
                
                # ‚úÖ MARQUER ZONE TROUV√âE (pour filtrage docs MeiliSearch)
                delivery_zone_found = True
                
                # Formater pour injection dans prompt
                delivery_context = format_delivery_info(delivery_info)
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Extraction zone livraison √©chou√©e: {e}")
            delivery_context = ""
            delivery_zone_found = False
        
        # ‚úÖ TOUJOURS AJOUTER L'HEURE CI (en d√©but ou fin de delivery_context)
        try:
            from core.timezone_helper import get_delivery_context_with_time
            time_context = get_delivery_context_with_time()
            if time_context:
                # Si une zone est d√©tect√©e, l'heure est d√©j√† dans delivery_context via format_delivery_info()
                # Sinon, cr√©er un delivery_context avec juste l'heure
                if not delivery_context:
                    delivery_context = time_context
                    logger.info(f"‚è∞ Heure CI inject√©e (aucune zone d√©tect√©e)")
                else:
                    logger.info(f"‚è∞ Heure CI d√©j√† incluse dans delivery_context (zone d√©tect√©e)")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible d'injecter l'heure CI: {e}")
        
        # ========== MODE BOTLIVE: CONVERSATION GUID√âE AVEC IMAGES ==========
        live_manager = LiveModeManager()
        if live_manager.get_status():
            print("üî¥ [BOTLIVE] Mode Live actif - Traitement conversationnel guid√©")
            
            # Import des modules Live
            from .live_conversation_state import get_live_conversation_state
            from .live_prompts import get_botlive_prompt, get_next_steps_instruction
            
            live_state = get_live_conversation_state()
            
            # Si des images sont fournies (depuis Messenger)
            if images and len(images) > 0:
                print(f"üî¥ [BOTLIVE] {len(images)} image(s) re√ßue(s)")
                
                # Analyser chaque image avec YOLO/OCR
                for i, image_url in enumerate(images):
                    print(f"üî¥ [BOTLIVE] Analyse image {i+1}: {image_url[:50]}...")
                    
                    try:
                        # T√©l√©charger et analyser l'image
                        detection_result, detection_type = await self._analyze_live_image(image_url)
                        
                        # Fallback d'affectation si inconnu: assigner selon l'√©tat manquant
                        if detection_type == "unknown":
                            missing_kind = live_state.get_missing(user_id)
                            if missing_kind in ("product", "payment"):
                                print(f"üî¥ [BOTLIVE] Fallback assign => {missing_kind}")
                                detection_type = missing_kind
                        
                        # Stocker dans l'√©tat conversationnel
                        status = live_state.add_detection(
                            user_id, company_id, image_url, detection_result, detection_type
                        )
                        
                        print(f"üî¥ [BOTLIVE] Image analys√©e: type={detection_type}, status={status}")
                        
                        # Si commande compl√®te
                        if status == "ready":
                            session = live_state.get_session(user_id)
                            final_message = await self._finalize_live_order(session)
                            live_state.clear(user_id)
                            return final_message
                        
                    except Exception as e:
                        print(f"üî¥ [BOTLIVE] Erreur analyse image: {e}")
                        return "‚ùå Erreur lors de l'analyse de votre image. Veuillez r√©essayer."
            
            # Extraire zone livraison √©ventuelle depuis le message et m√©moriser
            try:
                zone = self._extract_delivery_zone_from_text(query)
                if zone:
                    live_state.set_delivery_zone(user_id, zone)
            except Exception:
                pass

            # G√©n√©rer contexte et prompt Botlive (avec frais livraison compacts)
            detection_context = live_state.get_context_for_llm(user_id)
            try:
                from .live_prompts import get_delivery_fees_compact
                detection_context = detection_context + get_delivery_fees_compact()
            except Exception:
                pass
            missing = live_state.get_missing(user_id)
            next_steps = get_next_steps_instruction(missing)
            
            botlive_prompt = get_botlive_prompt(company_name, detection_context, next_steps)
            
            print(f"üî¥ [BOTLIVE] Contexte: {detection_context[:100]}...")
            print(f"üî¥ [BOTLIVE] √âtapes suivantes: {missing}")
            
            # G√©n√©rer r√©ponse avec prompt sp√©cialis√© Botlive
            try:
                response = await self.llm_client.complete(
                    prompt=f"{botlive_prompt}\n\nMessage utilisateur: {query}\n\nR√©ponse:",
                    temperature=0.3,  # Plus d√©terministe pour les commandes
                    max_tokens=512
                )
                
                print(f"üî¥ [BOTLIVE] R√©ponse g√©n√©r√©e: {len(response)} chars")
                return response
                
            except Exception as e:
                print(f"üî¥ [BOTLIVE] Erreur LLM: {e}")
                return "‚ùå Erreur technique. Veuillez r√©essayer."
        
        # --- Mode RAG Normal (existant) ---
        try:
            live_manager = LiveModeManager()
            if live_manager.get_status() and images and isinstance(images, (list, tuple)) and len(images) >= 2:
                # Import paresseux pour √©viter l'import circulaire
                from .botlive_engine import BotliveEngine
                print("üîÄ Redirection vers le moteur Botlive")
                botlive = BotliveEngine()
                return botlive.process_live_order(images[0], images[1])
        except Exception as e:
            print(f"[LIVE MODE] Erreur routage Botlive: {e}")

        # --- Patch robustesse LLM ---
        if self.llm_client is None:
            print("[LLM INIT] LLM client non initialis√©, tentative de r√©initialisation...")
            try:
                await self.initialize()
            except Exception as e:
                print(f"[LLM INIT][ERREUR] Impossible d'initialiser le LLM client: {e}")
                return "[ERREUR LLM] Impossible d'initialiser le mod√®le de langage."
        if self.llm_client is None:
            print("[LLM INIT][ERREUR] LLM client toujours None apr√®s initialisation.")
            return "[ERREUR LLM] LLM non disponible."

        print(f"ü§ñ [√âTAPE 3] G√©n√©ration LLM...")

        # --- M√©moire conversationnelle OPTIMIS√âE ---
        try:
            from core.optimized_conversation_memory import get_optimized_conversation_context, add_user_conversation_message
        except ImportError:
            get_optimized_conversation_context = lambda *a, **kw: ""
            add_user_conversation_message = lambda *a, **kw: None
        
        conversation_context = ""
        if user_id:
            # D'abord r√©cup√©rer le contexte existant
            conversation_context = get_optimized_conversation_context(user_id, company_id)
            print(f"üß† R√©sum√© m√©moire conversationnelle OPTIMIS√âE : {len(conversation_context)} caract√®res")
            
            # Puis ajouter le message utilisateur AVANT g√©n√©ration pour mise √† jour du contexte
            try:
                await add_user_conversation_message(user_id, company_id, query, self.llm_client)
                # R√©cup√©rer le contexte mis √† jour
                conversation_context = get_optimized_conversation_context(user_id, company_id)
                print(f"üß† Contexte mis √† jour apr√®s ajout message : {len(conversation_context)} caract√®res")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur mise √† jour m√©moire: {e}")
                # Continuer avec l'ancien contexte
        
        # ========== FILTRER DOCS LIVRAISON SI REGEX A TROUV√â ==========
        # Si regex a trouv√© la zone, supprimer les docs delivery de MeiliSearch (doublon)
        meili_context_filtered = search_results['meili_context']
        
        if delivery_zone_found:
            logger.info("‚úÖ [DOCS] Zone trouv√©e par regex ‚Üí Supprimer docs delivery de MeiliSearch (doublon)")
            
            # Filtrer les lignes contenant "LIVRAISON" ou "delivery_" dans l'index
            lines = meili_context_filtered.split('\n')
            filtered_lines = []
            skip_until_next_doc = False
            
            for line in lines:
                # D√©tecter d√©but d'un document delivery
                if 'delivery_' in line or 'LIVRAISON -' in line:
                    skip_until_next_doc = True
                    continue
                
                # D√©tecter d√©but d'un nouveau document (r√©initialiser le skip)
                if line.startswith('DOCUMENT #'):
                    skip_until_next_doc = False
                
                # Garder la ligne si on ne skip pas
                if not skip_until_next_doc:
                    filtered_lines.append(line)
            
            meili_context_filtered = '\n'.join(filtered_lines)
            logger.info(f"üì¶ [DOCS] Docs delivery filtr√©s: {len(lines)} ‚Üí {len(filtered_lines)} lignes")
        
        # Construction du contexte structur√©
        context_parts = []
        if conversation_context:
            context_parts.append(conversation_context)
        if meili_context_filtered:  # ‚úÖ Utiliser contexte filtr√©
            context_parts.append(meili_context_filtered)
        if search_results['supabase_context']:
            context_parts.append(search_results['supabase_context'])

        structured_context = "\n".join(context_parts) if context_parts else "Aucun document trouv√©"
        print(f"üìÑ Contexte: {len(structured_context)} caract√®res")
        # Log contexte r√©duit
        context_preview = structured_context[:100] + ('...' if len(structured_context) > 100 else '')
        print(f"üìÑ [CONTEXTE] {context_preview}")
        
        # ========== AFFICHAGE VIOLET : CONTEXTE FINAL COMPLET ENVOY√â AU LLM ==========
        if structured_context and structured_context != "Aucun document trouv√©":
            print(f"\033[95m" + "="*80 + "\033[0m")
            print(f"\033[95müü£ CONTEXTE FINAL COMPLET ENVOY√â AU LLM\033[0m")
            print(f"\033[95m" + "="*80 + "\033[0m")
            print(f"\033[95m{structured_context}\033[0m")
            print(f"\033[95m" + "="*80 + "\033[0m")

        # --- ENRICHISSEMENT CONTEXTE PAR EXTRACTION REGEX --- D√âSACTIV√â
        # ‚ö†Ô∏è SYST√àME D√âSACTIV√â : Le LLM avec <thinking> g√®re lui-m√™me l'extraction
        # Ce syst√®me filtrait les documents et ne gardait que certaines infos
        # D√©sormais : Documents complets envoy√©s au LLM qui d√©cide lui-m√™me
        print("\nüìÑ [CONTEXTE] Envoi des documents COMPLETS au LLM (regex extraction d√©sactiv√©e)")
        
        # try:
        #     print("\nüîé [REGEX] D√©but extraction regex sur les documents pertinents...")
        #     from core.rag_regex_extractor import extract_regex_entities_from_docs
        #     # Simulation : reconstituer la liste des docs pertinents (Meili ou Supabase)
        #     docs = []
        #     if hasattr(search_results, 'meili_docs') and search_results['meili_docs']:
        #         docs = search_results['meili_docs']
        #         print(f"[REGEX] {len(docs)} docs MeiliSearch √† analyser")
        #     elif hasattr(search_results, 'supabase_docs') and search_results['supabase_docs']:
        #         docs = search_results['supabase_docs']
        #         print(f"[REGEX] {len(docs)} docs Supabase √† analyser")
        #     # Sinon, fallback : tenter d'extraire les docs depuis le contexte brut
        #     if not docs and structured_context:
        #         docs = [{"content": structured_context}]
        #         print("[REGEX] Aucun doc structur√©, fallback sur contexte brut")
        #     print("[REGEX] Chargement des patterns m√©tier...")
        #     regex_entities = extract_regex_entities_from_docs(docs)
        #     print(f"[REGEX] Extraction termin√©e. Patterns trouv√©s : {sum(len(v) for v in regex_entities.values())}")
        #     regex_summary = []
        #     for label, values in regex_entities.items():
        #         if values:
        #             regex_summary.append(f"[REGEX {label}] " + ", ".join(set(values)))
        #             print(f"[REGEX] {label}: {values}")
        #     if regex_summary:
        #         structured_context += "\n\n" + "\n".join(regex_summary)
        #         print(f"[REGEX] Extraits ajout√©s au contexte : {len(regex_summary)} patterns structur√©s")
        #         print(f"[REGEX] [D√âTAIL EXTRAITS] Entit√©s ajout√©es:\n" + "\n".join(regex_summary))
        #     else:
        #         print("[REGEX] Aucune entit√© extraite des documents")
        #     print("[REGEX] Auto-apprentissage : v√©rification de nouveaux patterns potentiels (voir logs de core/rag_regex_extractor.py)")
        #     print(f"[REGEX] [CONTEXTE FINAL] Taille apr√®s enrichissement: {len(structured_context)} caract√®res")
        # except Exception as e:
        #     print(f"[REGEX] Erreur enrichissement contexte : {e}")
        
        # üìã PROMPT D√âJ√Ä R√âCUP√âR√â AU D√âBUT (avant traitement image)
        print(f"üìã [D√âTAIL PROMPT] Contenu:\n{dynamic_prompt[:300]}{'...' if len(dynamic_prompt) > 300 else ''}")
        
        # Enrichissement intelligent du prompt avec d√©tection de remises
        pricing_enhancement = self._detect_pricing_context(query, structured_context)
        
        # üöÄ CONSTRUCTION PROMPT CLASSIQUE (Enhanced d√©sactiv√©)
        # Le syst√®me enhanced_prompt_engine g√©n√©rait des templates buggy
        # D√©sactiv√© pour utiliser directement le prompt simple
        print("‚úÖ Utilisation du prompt classique (enhanced d√©sactiv√©)")
        
        # Construction dynamique du prompt : remplacement des placeholders par les vraies valeurs
        conversation_history = search_results.get('conversation_history', '')
        system_prompt = dynamic_prompt
        system_prompt = system_prompt.replace("{context}", structured_context)
        system_prompt = system_prompt.replace("{history}", conversation_history)
        system_prompt = system_prompt.replace("{question}", query)
        if pricing_enhancement:
            system_prompt = system_prompt.replace("{pricing_enhancement}", pricing_enhancement)
        else:
            system_prompt = system_prompt.replace("{pricing_enhancement}", "")
        
        # ========== INJECTION CONTEXTE IMAGE ==========
        if image_context:
            system_prompt += image_context
            print("‚úÖ [PROMPT] Contexte image inject√©")
        
        # ========== INJECTION CONTEXTE LIVRAISON (REGEX EXTRACTION) ==========
        if delivery_context:
            # Injecter UNE SEULE FOIS avant <context> (remplacer seulement la 1√®re occurrence)
            system_prompt = system_prompt.replace(
                "<context>",
                f"{delivery_context}\n\n<context>",
                1  # Remplacer UNIQUEMENT la 1√®re occurrence
            )
            print("‚úÖ [PROMPT] Contexte livraison inject√© (frais exacts)")
        
        # ========== ENRICHISSEMENT CONTEXTE BOTLIVE (STATE + NOTEPAD) ==========
        try:
            from core.rag_tools_integration import enrich_prompt_with_context
            
            # Enrichir avec √©tat commande et notepad si user_id disponible
            if user_id:
                system_prompt = enrich_prompt_with_context(
                    base_prompt=system_prompt,
                    user_id=user_id,
                    company_id=company_id,  # ‚úÖ Passer company_id pour notepad
                    include_state=True,  # Inclure √©tat commande
                    include_notepad=True  # Inclure notes utilisateur
                )
                print(f"üé® Prompt enrichi avec contexte utilisateur (state + notepad)")
        except Exception as e:
            print(f"‚ö†Ô∏è Enrichissement contexte √©chou√©: {e}")
        
        # ========== INJECTION ENHANCED MEMORY DANS PROMPT (PRIORITAIRE) ==========
        try:
            from .enhanced_memory import get_enhanced_memory
            
            if user_id:
                enhanced_memory = get_enhanced_memory()
                memory_context = enhanced_memory.get_context_for_llm(user_id, company_id)
                
                if memory_context:
                    system_prompt += f"\n\n{memory_context}"
                    print(f"üß† [ENHANCED_MEMORY] Contexte inject√© ({len(memory_context)} chars)")
        except Exception as e:
            print(f"‚ö†Ô∏è [ENHANCED_MEMORY] Injection √©chou√©e: {e}")
        
        # ========== INJECTION CONTEXTE NOTEPAD (SIMPLIFI√â) ==========
        try:
            from .conversation_notepad import ConversationNotepad
            
            if user_id:
                notepad = ConversationNotepad.get_instance()
                notepad_data = notepad.get_all(user_id, company_id)
                
                if notepad_data:
                    # Construire contexte format√©
                    notepad_lines = ["\nüìã CONTEXTE COLLECT√â (NE PAS REDEMANDER):"]
                    if notepad_data.get('produit'):
                        notepad_lines.append(f"   ‚úÖ Produit: {notepad_data['produit']}")
                    if notepad_data.get('zone'):
                        notepad_lines.append(f"   ‚úÖ Zone: {notepad_data['zone']}")
                    if notepad_data.get('telephone'):
                        notepad_lines.append(f"   ‚úÖ T√©l√©phone: {notepad_data['telephone']}")
                    if notepad_data.get('paiement'):
                        notepad_lines.append(f"   ‚úÖ Paiement: {notepad_data['paiement']}")
                    
                    notepad_context = "\n".join(notepad_lines)
                    system_prompt += f"\n{notepad_context}\n"
                    logger.info(f"üìã Contexte notepad inject√©: {len(notepad_context)} chars")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Injection notepad √©chou√©e: {e}")
        
        # Ajout fallback si bloc question/contexte absent
        if "{question}" in system_prompt:
            system_prompt += f"\nQUESTION: {query}"
        if "{context}" in system_prompt:
            system_prompt += f"\nCONTEXTE DISPONIBLE:\n{structured_context}"
        system_prompt += "\n\nR√âPONSE:"
        
        # --- LOG ENRICHISSANT AVANT ENVOI LLM ---
        def highlight(val, color_code):
            return f"\033[{color_code}m{val}\033[0m"

        prompt_log = system_prompt
        injects = {
            company_name: '93',  # Jaune
            "Assistant": '92',   # Vert (ai_name)
            "commerce": '96',    # Cyan (secteur_activite)
            query: '91',          # Rouge (question)
        }
        for val, code in injects.items():
            if val and isinstance(val, str) and len(val) > 0:
                prompt_log = prompt_log.replace(val, highlight(val, code))
        # pricing_enhancement en magenta si pr√©sent
        if pricing_enhancement.strip():
            prompt_log = prompt_log.replace(pricing_enhancement.strip(), highlight(pricing_enhancement.strip(), '95'))
        # Context en bleu
        if structured_context.strip():
            prompt_log = prompt_log.replace(structured_context.strip(), highlight(structured_context.strip(), '94'))

        # Affichage COMPLET du prompt pour debug
        print(f"\n{'='*80}")
        print(f"üß† PROMPT COMPLET ENVOY√â AU LLM ({len(system_prompt)} chars)")
        print(f"{'='*80}")
        print(system_prompt)
        print(f"{'='*80}")
        
        try:
            # ========== TRACKING: APPEL LLM ==========
            try:
                from core.rag_performance_tracker import get_tracker
                tracker = get_tracker(getattr(self, '_request_id', 'unknown'))
                tracker.start_step("llm_generation", prompt_length=len(system_prompt))
            except:
                tracker = None
            
            llm_result = await self.llm_client.complete(
                prompt=system_prompt,
                temperature=0.7,
                max_tokens=1024  # Augment√© de 400 √† 1024 pour r√©ponses compl√®tes
            )
            
            # Extraire r√©ponse et usage
            if isinstance(llm_result, dict):
                response = llm_result.get("response", llm_result)
                usage = llm_result.get("usage", {})
                model_used = llm_result.get("model", "unknown")
                
                # Enregistrer usage tokens
                if tracker and usage:
                    tracker.record_llm_usage(
                        model=model_used,
                        prompt_tokens=usage.get("prompt_tokens", 0),
                        completion_tokens=usage.get("completion_tokens", 0)
                    )
                    tracker.end_step(response_length=len(str(response)))
            else:
                response = llm_result
                if tracker:
                    tracker.end_step(response_length=len(str(response)))
            
            print(f"‚úÖ LLM r√©ponse: {len(str(response))} caract√®res")
            
            # ========== AFFICHAGE COLOR√â DE LA R√âPONSE LLM ==========
            print("\n\033[1m===== [R√âPONSE LLM COMPL√àTE] =====\033[0m")
            print(f"\033[93m{response}\033[0m")  # JAUNE pour la r√©ponse compl√®te
            print("\033[1m===== [FIN R√âPONSE LLM] =====\033[0m\n")
            
            # ========== SAUVEGARDE R√âPONSE COMPL√àTE (pour tests) ==========
            full_llm_response = response  # Sauvegarder AVANT extraction
            
            # ========== EXTRACTION BALISE <response> + OUTILS BOTLIVE ==========
            # Utiliser le syst√®me d'extraction avanc√© de Botlive
            thinking = ""  # Initialiser AVANT le try
            try:
                from core.rag_tools_integration import process_llm_response
                
                # Traitement complet avec outils
                processed = process_llm_response(
                    llm_output=response,
                    user_id=user_id,
                    enable_tools=True  # Activer calculator, notepad, state tracker
                )
                
                response = processed["response"]
                # ‚úÖ CRITIQUE: Ne pas √©craser thinking s'il est d√©j√† rempli
                extracted_thinking = processed.get("thinking", "")
                if extracted_thinking:  # Seulement si non vide
                    thinking = extracted_thinking
                tools_executed = processed.get("tools_executed", 0)
                
                # Stocker pour acc√®s externe (tests)
                self._last_full_response = full_llm_response
                self._last_thinking = thinking
                
                print(f"‚úÖ Extraction <response>: {len(response)} chars")
                if thinking:
                    print(f"üß† Thinking extrait: {len(thinking)} chars")
                if tools_executed > 0:
                    print(f"üîß Outils ex√©cut√©s: {tools_executed}")
                    
            except Exception as e:
                # Fallback: extraction simple
                print(f"‚ö†Ô∏è Fallback extraction simple: {e}")
                response_match = re.search(r'<response>(.*?)</response>', response, re.DOTALL | re.IGNORECASE)
                if response_match:
                    response = response_match.group(1).strip()
                else:
                    response = re.sub(r'<thinking>.*?</thinking>', '', response, flags=re.DOTALL | re.IGNORECASE).strip()
            
            # ========== POST-TRAITEMENT: ENHANCED MEMORY (PRIORITAIRE) ==========
            try:
                from .enhanced_memory import get_enhanced_memory
                
                if user_id:
                    enhanced_memory = get_enhanced_memory()
                    enhanced_memory.save_interaction(
                        user_id=user_id,
                        company_id=company_id,
                        user_message=query,
                        llm_response=response
                    )
                    logger.info(f"üß† Enhanced memory mise √† jour")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Enhanced memory √©chou√©: {e}")
            
            # ========== POST-TRAITEMENT: MISE √Ä JOUR NOTEPAD AVEC R√âPONSE LLM ==========
            try:
                from .conversation_notepad import (
                    get_conversation_notepad,
                    extract_price_from_response
                )
                
                if user_id and product_info:
                    notepad = get_conversation_notepad()
                    
                    # Extraire le prix de la r√©ponse LLM
                    price = extract_price_from_response(response)
                    
                    if price:
                        notepad.update_product(
                            user_id, company_id,
                            product_name=product_info['product_type'],
                            quantity=product_info['quantity'],
                            price=price,
                            variant=product_info.get('variant')
                        )
                        logger.info(f"üí∞ Prix extrait et enregistr√©: {price} FCFA")
                
                # Mise √† jour zone de livraison si d√©tect√©e
                if user_id and delivery_zone:
                    # Extraire le co√ªt de livraison de la r√©ponse
                    delivery_cost_match = re.search(
                        r'livraison[^0-9]*(\d+[\s\u202f]?\d{3})\s*(?:FCFA|F)',
                        response
                    )
                    if delivery_cost_match:
                        cost_str = delivery_cost_match.group(1).replace('\u202f', '').replace(' ', '')
                        delivery_cost = float(cost_str)
                        
                        notepad = get_conversation_notepad()
                        notepad.update_delivery(user_id, company_id, delivery_zone, delivery_cost)
                        logger.info(f"üöö Livraison enregistr√©e: {delivery_zone} - {delivery_cost} FCFA")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Post-traitement notepad √©chou√©: {e}")
            
            # ========== CALCUL ET AFFICHAGE COLOR√â DES TOKENS ==========
            try:
                import tiktoken
                enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
                prompt_tokens = len(enc.encode(system_prompt))
                response_tokens = len(enc.encode(response))
            except Exception:
                # fallback simple : split sur les espaces
                prompt_tokens = len(system_prompt.split())
                response_tokens = len(response.split())
            
            total_tokens = prompt_tokens + response_tokens
            
            # ROUGE pour les infos de tokens
            print(f"\033[91m[LLM] Tokens totaux prompt+r√©ponse : {total_tokens} {'(TROP √âLEV√â)' if total_tokens > 4000 else '(OK)'}\033[0m")
            print(f"\033[91m[LLM] D√©tail : prompt={prompt_tokens} | r√©ponse={response_tokens} (estimation co√ªt)\033[0m")
            # M√©moire d√©j√† mise √† jour AVANT g√©n√©ration - pas besoin de re-traitement

            # --- R√âCAPITULATIF AUTOMATIQUE D√âSACTIV√â (g√©n√®re prix fant√¥mes) ---
            # Cause: Template bugu√© avec "500 FCFA" inexistants
            # √Ä r√©activer apr√®s correction du template
            # from core.recap_template import generate_order_summary
            #
            # user_query_lower = query.lower()
            # add_recap = any(word in user_query_lower for word in ['r√©cap', 'r√©capitulatif', 'r√©sum√©', 'confirmation', 'commande'])
            #
            # if generate_order_summary and add_recap:
            #     conversation_history = []
            #     if user_id:
            #         try:
            #             from core.conversation_memory import get_full_conversation_history
            #             conversation_history = get_full_conversation_history(user_id, company_id)
            #         except:
            #             conversation_history = [{"message": query, "user_id": user_id}]
            #     
            #     customer_info = self._extract_customer_from_context(conversation_history, user_id)
            #     products = self._extract_products_from_context(query, structured_context, conversation_history)
            #     delivery_info = self._extract_delivery_from_context(query, structured_context, customer_info)
            #     price_info = self._calculate_dynamic_pricing(products, delivery_info, structured_context)
            #     
            #     recap = generate_order_summary(customer_info, products, delivery_info, price_info, company_id)
            #     response += f"\n\nüìã R√âCAPITULATIF STRUCTUR√â :\n{recap}"

            # ========== EX√âCUTION DES ACTIONS LLM (BLOC-NOTE, CALCULATRICE) ==========
            try:
                from core.botlive_tools import execute_tools_in_response
                response = execute_tools_in_response(
                    response, 
                    user_id or "unknown", 
                    company_id
                )
                print("‚úÖ [TOOLS] Actions LLM ex√©cut√©es (bloc-note, calculatrice)")
            except Exception as e:
                print(f"‚ö†Ô∏è [TOOLS] Erreur ex√©cution actions: {e}")

            return response
        except Exception as e:
            print(f"‚ùå LLM erreur: {str(e)[:50]}")
            return f"Je rencontre une difficult√© technique. Pouvez-vous reformuler votre question ?"
    
    async def _analyze_live_image(self, image_url: str) -> tuple:
        """
        Analyse une image avec YOLO/OCR pour d√©terminer son type et contenu.
        
        Returns:
            tuple: (detection_result: dict, detection_type: str)
        """
        import tempfile
        import requests
        import os
        
        # T√©l√©charger l'image
        try:
            r = requests.get(image_url, timeout=15)
            r.raise_for_status()
            
            # Cr√©er fichier temporaire
            fd, temp_path = tempfile.mkstemp(suffix=".jpg")
            with os.fdopen(fd, "wb") as f:
                f.write(r.content)
            
            # Analyser avec BotliveEngine
            from .botlive_engine import BotliveEngine
            engine = BotliveEngine()
            
            # Tenter d√©tection produit
            product_result = engine.detect_product(temp_path)
            
            # Tenter OCR paiement
            payment_result = engine.verify_payment(temp_path)
            
            # D√©terminer le type d'image (heuristique renforc√©e)
            import re
            raw = (payment_result.get("raw_text", "") or "").lower()
            has_currency = bool(re.search(r"\b(fcfa|xof|cfa)\b", raw))
            has_amount = bool(re.search(r"\b\d{3,6}\b", raw))  # 3 √† 6 chiffres cons√©cutifs
            has_payment_kw = any(k in raw for k in ["wave", "orange", "mtn", "moov", "momo", "paiement", "recu", "re√ßu"]) 

            if payment_result.get("amount") or (has_currency and has_amount) or (has_payment_kw and has_amount):
                return payment_result, "payment"
            elif product_result.get("confidence", 0) >= 0.25:
                return product_result, "product"
            else:
                # Image non reconnue - demander √† l'utilisateur
                return {
                    "name": "image_non_reconnue",
                    "confidence": 0.0,
                    "raw_text": payment_result.get("raw_text", "")
                }, "unknown"
        
        except Exception as e:
            print(f"[BOTLIVE] Erreur analyse image: {e}")
            return {"error": str(e)}, "error"
        
        finally:
            # Nettoyer le fichier temporaire
            try:
                if 'temp_path' in locals():
                    os.remove(temp_path)
            except Exception:
                pass
    
    async def _finalize_live_order(self, session: dict) -> str:
        """
        Finalise une commande Live avec les 2 images collect√©es.
        
        Args:
            session: Session contenant product_img, payment_img et d√©tections
        
        Returns:
            Message de confirmation de commande
        """
        try:
            # Extraire les informations des d√©tections
            product_detection = session.get("product_detection", {})
            payment_detection = session.get("payment_detection", {})
            
            # Construire le message de confirmation
            product_name = product_detection.get("name", "produit")
            product_confidence = product_detection.get("confidence", 0) * 100
            
            payment_amount = payment_detection.get("amount", "")
            payment_currency = payment_detection.get("currency", "FCFA")
            payment_ref = payment_detection.get("reference", "")
            
            # Message de base
            confirmation_parts = [
                "üéâ **COMMANDE VALID√âE !**",
                "",
                f"üì¶ **Produit:** {product_name} (d√©tection: {product_confidence:.1f}%)"
            ]
            
            # Informations de paiement
            if payment_amount:
                confirmation_parts.append(f"üí≥ **Paiement:** {payment_amount} {payment_currency}")
            else:
                confirmation_parts.append("üí≥ **Paiement:** Montant d√©tect√© mais non lisible")
            
            if payment_ref:
                confirmation_parts.append(f"üìã **R√©f√©rence:** {payment_ref}")
            
            confirmation_parts.extend([
                "",
                "‚úÖ Votre commande est en cours de traitement.",
                "üìû Nous vous contacterons pour la livraison.",
                "",
                "Merci de votre confiance ! üôè"
            ])
            
            # TODO: Sauvegarder en base de donn√©es
            # await self._save_live_order_to_db(session)
            
            return "\n".join(confirmation_parts)
            
        except Exception as e:
            print(f"[BOTLIVE] Erreur finalisation: {e}")
            return "‚úÖ Commande re√ßue ! Nous la traitons et vous recontactons rapidement."
    
    def _extract_config_from_prompt(self, prompt: str) -> dict:
        """Extrait WAVE_ENTREPRISE et ACOMPTE_REQUIS du prompt"""
        import re
        
        config = {
            'wave_phone': None,
            'required_amount': None
        }
        
        # Extraire WAVE_ENTREPRISE: +225 0787360757
        wave_match = re.search(r'WAVE_ENTREPRISE:\s*\+?(\d+\s*\d+\s*\d+)', prompt)
        if wave_match:
            config['wave_phone'] = wave_match.group(1).replace(' ', '')
        
        # Extraire ACOMPTE_REQUIS: 2000 FCFA
        acompte_match = re.search(r'ACOMPTE_REQUIS:\s*(\d+)\s*FCFA', prompt)
        if acompte_match:
            config['required_amount'] = int(acompte_match.group(1))
        
        return config
    
    async def _get_dynamic_prompt(self, company_id: str, company_name: str) -> str:
        """üìã R√©cup√®re le prompt dynamique via company_booster (nouveau syst√®me universel)"""
        try:
            # ‚úÖ NOUVEAU: Utiliser le g√©n√©rateur de prompt dynamique bas√© sur company_booster
            from core.dynamic_prompt_generator import get_prompt_for_company
            
            try:
                # R√©cup√©rer le prompt depuis company_booster (avec cache automatique)
                prompt = get_prompt_for_company(
                    company_id=company_id,
                    supabase_client=self.supabase
                )
                logger.info(f"‚úÖ [DYNAMIC PROMPT] Prompt g√©n√©r√© depuis company_booster pour {company_id[:8]}...")
                return prompt
            
            except Exception as e_booster:
                logger.warning(f"‚ö†Ô∏è [DYNAMIC PROMPT] Fallback ancien syst√®me: {e_booster}")
                
                # FALLBACK: Ancien syst√®me (get_company_system_prompt)
                from database.supabase_client import get_company_system_prompt
                
                prompt = await get_company_system_prompt(company_id)
                if prompt and len(prompt.strip()) > 0:
                    # Rendre les variables du template robustement avec des valeurs par d√©faut
                    try:
                        personalized_prompt = prompt.format(
                            company_name=company_name,
                            ai_name="Assistant",
                            secteur_activite="commerce",
                            mission_principale="",
                            objectif_final=""
                        )
                    except KeyError as e:
                        # Si des variables inconnues sont pr√©sentes, utiliser le template tel quel
                        print(f"‚ö†Ô∏è Variable manquante dans template (cache): {e}")
                        personalized_prompt = prompt
                    
                    logger.info(f"üìã Prompt (ancien syst√®me) r√©cup√©r√© pour {company_id[:8]}...")
                    return personalized_prompt
                
                # Fallback ultime si pas de prompt personnalis√©
                logger.info("üìã Utilisation prompt par d√©faut (aucun syst√®me disponible)")
                return f"""Tu es un assistant client professionnel pour {company_name}.

R√àGLES STRICTES:
- Utilise UNIQUEMENT les informations du contexte fourni
- Si l'information n'est pas dans le contexte, dis "Je n'ai pas cette information"
- Mentionne tes sources: "Selon nos informations..."
- Sois professionnel et empathique
- Ne jamais inventer de donn√©es"""
        
        except Exception as e:
            logger.error(f"‚ùå Erreur r√©cup√©ration prompt dynamique: {e}")
            return f"Tu es un assistant client professionnel pour {company_name}."
    
    def _detect_pricing_context(self, query: str, context: str) -> str:
        """D√©tecte intelligemment les questions de prix et remises pour enrichir le prompt"""
        
        query_lower = query.lower()
        context_lower = context.lower()
        
        # D√©tection de questions sur les remises/quantit√©s
        quantity_keywords = ["paquets", "quantit√©", "remise", "r√©duction", "prix", "tarif", "combien", "co√ªte"]
        pricing_keywords = ["fcfa", "prix", "tarif", "co√ªt", "montant"]
        
        is_quantity_question = any(word in query_lower for word in quantity_keywords)
        has_pricing_context = any(word in context_lower for word in pricing_keywords)
        
        if is_quantity_question and has_pricing_context:
            # Extraction dynamique des tarifs d√©gressifs depuis le contexte
            
            # Patterns pour d√©tecter les tarifs d√©gressifs
            degressive_patterns = [
                r"(\d+)\s*paquets?\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa",
                r"(\d+)\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa.*?([0-9,\.]+)\s*f/paquet",
                r"(\d+)\s*paquets?\s*\([^)]*\)\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa"
            ]
            
            pricing_info = []
            for pattern in degressive_patterns:
                matches = re.findall(pattern, context_lower)
                for match in matches:
                    try:
                        quantity = int(match[0])
                        price = match[1].replace(",", "").replace(".", "")
                        if quantity > 1 and len(price) >= 3:  # Prix r√©aliste
                            pricing_info.append(f"{quantity} paquets = {match[1]} FCFA")
                    except:
                        continue
            
            if pricing_info:
                return f"""
INSTRUCTION SP√âCIALE TARIFICATION:
- V√©rifiez TOUJOURS les tarifs d√©gressifs dans le contexte
- Tarifs d√©tect√©s: {' | '.join(pricing_info[:5])}
- Si le client demande une quantit√© sp√©cifique, cherchez le tarif exact dans le contexte
- Calculez les √©conomies r√©alis√©es par rapport au prix unitaire
- Mentionnez explicitement les remises disponibles"""
        
        # D√©tection de questions sur l'acompte
        if any(word in query_lower for word in ["acompte", "payer", "total", "confirmer", "commande"]):
            # Recherche directe de l'acompte dans le contexte
            acompte_found = None
            acompte_patterns = [
                r"un acompte de (\d+) fcfa",
                r"acompte de (\d+) fcfa", 
                r"condition de commande.*?(\d+) fcfa",
                r"acompte.*?(\d+).*?fcfa"
            ]
            
            for pattern in acompte_patterns:
                match = re.search(pattern, context_lower)
                if match:
                    try:
                        amount = int(match.group(1))
                        if 500 <= amount <= 50000:
                            acompte_found = amount
                            break
                    except:
                        continue
            
            if acompte_found:
                return f"""
INSTRUCTION SP√âCIALE PAIEMENT:
- ACOMPTE D√âTECT√â: {acompte_found} FCFA dans le contexte
- Mentionnez explicitement: "Un acompte de {acompte_found} FCFA est requis"
- Ne jamais indiquer 0 FCFA car l'acompte est de {acompte_found} FCFA
- Utilisez cette information pour tous les calculs de paiement"""
            else:
                return """
INSTRUCTION SP√âCIALE PAIEMENT:
- Recherchez dans le contexte les informations sur l'acompte requis
- Patterns √† chercher: "acompte de X FCFA", "condition de commande", "avant que la commande"
- Extrayez le montant exact de l'acompte depuis le contexte
- Ne jamais indiquer 0 FCFA si un acompte est mentionn√© dans le contexte"""
        
        return ""
    
    def _extract_customer_from_context(self, conversation_history: List[Dict], user_id: str) -> Dict[str, Any]:
        """Extrait intelligemment les informations client depuis l'historique"""
        customer_info = {
            "name": user_id,
            "phone": "",
            "address": ""
        }
        
        for message in conversation_history:
            text = message.get("message", "").lower()
            
            # Extraction nom
            name_patterns = [
                r"mon nom (?:c'est|est) ([a-zA-Z√Ä-√ø\s]+)",
                r"je (?:m'appelle|suis) ([a-zA-Z√Ä-√ø\s]+)",
                r"nom[:\s]+([a-zA-Z√Ä-√ø\s]+)"
            ]
            
            for pattern in name_patterns:
                match = re.search(pattern, text)
                if match:
                    name = match.group(1).strip().title()
                    if len(name) > 2 and len(name) < 50:
                        customer_info["name"] = name
                        break
            
            # Extraction t√©l√©phone
            phone_patterns = [
                r"(\+225\s?\d{2}\s?\d{2}\s?\d{2}\s?\d{2}\s?\d{2})",
                r"(0\d{2}\s?\d{2}\s?\d{2}\s?\d{2}\s?\d{2})",
                r"(\d{10})"
            ]
            
            for pattern in phone_patterns:
                match = re.search(pattern, text)
                if match:
                    phone = re.sub(r'\s+', '', match.group(1))
                    if len(phone) >= 10:
                        customer_info["phone"] = phone
                        break
            
            # Extraction adresse/zone
            location_keywords = ["cocody", "yopougon", "plateau", "adjam√©", "abobo", "marcory", "koumassi", "treichville"]
            for keyword in location_keywords:
                if keyword in text:
                    customer_info["address"] = keyword.title()
                    break
        
        return customer_info
    
    def _extract_products_from_context(self, query: str, context: str, conversation_history: List[Dict]) -> List[Dict[str, Any]]:
        """Extrait intelligemment les produits depuis le contexte et la conversation"""
        products = []
        
        # Analyser la conversation pour les produits mentionn√©s
        all_text = query + " " + " ".join([msg.get("message", "") for msg in conversation_history])
        text_lower = all_text.lower()
        
        # Extraction quantit√©
        quantity_patterns = [
            r"(\d+)\s*paquets?",
            r"quantit√©[:\s]*(\d+)",
            r"je (?:veux|souhaite|prends)\s*(\d+)"
        ]
        
        quantity = 1
        for pattern in quantity_patterns:
            match = re.search(pattern, text_lower)
            if match:
                try:
                    quantity = int(match.group(1))
                    if 1 <= quantity <= 100:  # Validation r√©aliste
                        break
                except:
                    continue
        
        # Extraction type de produit
        product_type = "Non sp√©cifi√©"
        if "culottes" in text_lower:
            product_type = "Couches culottes"
        elif "pression" in text_lower:
            product_type = "Couches √† pression"
        elif "adultes" in text_lower:
            product_type = "Couches adultes"
        
        # Extraction taille
        size = "Non sp√©cifi√©e"
        size_patterns = [
            r"taille\s*(\d+)",
            r"size\s*(\d+)",
            r"t(\d+)"
        ]
        
        for pattern in size_patterns:
            match = re.search(pattern, text_lower)
            if match:
                size = f"Taille {match.group(1)}"
                break
        
        # Extraction prix depuis le contexte (dynamique)
        unit_price = 0
        total_price = 0
        
        # Recherche prix dans le contexte pour la quantit√© exacte
        price_patterns = [
            rf"{quantity}\s*paquets?\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa",
            r"(\d+)\s*paquets?\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa"
        ]
        
        context_lower = context.lower()
        for pattern in price_patterns:
            matches = re.findall(pattern, context_lower)
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        qty, price_str = match[0], match[1] if len(match) > 1 else match[0]
                        if int(qty) == quantity:
                            total_price = int(price_str.replace(",", "").replace(".", ""))
                            unit_price = total_price // quantity
                            break
                    else:
                        total_price = int(match.replace(",", "").replace(".", ""))
                        unit_price = total_price // quantity
                        break
                except:
                    continue
        
        # Si pas de prix exact trouv√©, chercher prix unitaire
        if unit_price == 0:
            unit_patterns = [
                r"1\s*paquet\s*[-‚Äì‚Äî]\s*([0-9,\.]+)\s*f?\s*cfa",
                r"prix\s*unitaire[:\s]*([0-9,\.]+)"
            ]
            
            for pattern in unit_patterns:
                match = re.search(pattern, context_lower)
                if match:
                    try:
                        unit_price = int(match.group(1).replace(",", "").replace(".", ""))
                        total_price = unit_price * quantity
                        break
                    except:
                        continue
        
        products.append({
            "description": f"{product_type} {size}",
            "type": product_type.split()[-1].lower() if product_type != "Non sp√©cifi√©" else "produit",
            "size": size,
            "quantity": quantity,
            "unit_price": unit_price,
            "total_price": total_price
        })
        
        return products
    
    def _extract_delivery_from_context(self, query: str, context: str, customer_info: Dict) -> Dict[str, Any]:
        """Extrait intelligemment les informations de livraison"""
        delivery_info = {
            "zone": customer_info.get("address", "Non sp√©cifi√©e"),
            "delivery_cost": 0,
            "delivery_time": "√Ä confirmer",
            "address": customer_info.get("address", "Non fournie")
        }
        
        # Extraction co√ªt de livraison depuis le contexte
        zone_lower = delivery_info["zone"].lower()
        context_lower = context.lower()
        
        # Patterns pour d√©tecter les co√ªts de livraison par zone
        delivery_patterns = [
            rf"{zone_lower}[^0-9]*(\d+)\s*fcfa",
            r"zones?\s*centrales?[^0-9]*(\d+)\s*fcfa",
            r"abidjan[^0-9]*(\d+)\s*fcfa",
            r"tarif[:\s]*(\d+)\s*fcfa"
        ]
        
        for pattern in delivery_patterns:
            match = re.search(pattern, context_lower)
            if match:
                try:
                    cost = int(match.group(1))
                    if 500 <= cost <= 10000:  # Validation r√©aliste
                        delivery_info["delivery_cost"] = cost
                        break
                except:
                    continue
        
        # Extraction d√©lai de livraison
        if "jour m√™me" in context_lower or "avant 11h" in context_lower:
            delivery_info["delivery_time"] = "Jour m√™me"
        elif "lendemain" in context_lower:
            delivery_info["delivery_time"] = "Lendemain"
        elif "24h" in context_lower:
            delivery_info["delivery_time"] = "24h"
        elif "48h" in context_lower or "2 jours" in context_lower:
            delivery_info["delivery_time"] = "48h"
        
        return delivery_info
    
    def _calculate_dynamic_pricing(self, products: List[Dict], delivery_info: Dict, context: str) -> Dict[str, Any]:
        """Calcule dynamiquement les prix totaux"""
        subtotal = sum(product.get("total_price", 0) for product in products)
        delivery_cost = delivery_info.get("delivery_cost", 0)
        total = subtotal + delivery_cost
        
        return {
            "subtotal": subtotal,
            "delivery_cost": delivery_cost,
            "total": total,
            "products": products
        }
    
    async def process_query(self, query: str, company_id: str, user_id: str, company_name: str = None, skip_faq_cache: bool = False, request_id: str = None, images: List[str] = None) -> UniversalRAGResult:
        """
        üåç TRAITEMENT UNIVERSEL AVANC√â D'UNE REQU√äTE
        
        Pipeline complet :
        1. üá´üá∑ Analyse NLP fran√ßaise (intentions, entit√©s, normalisation)
        2. üîç Recherche hybride intelligente
        3. üîÄ Fusion multi-sources
        4. üéØ Prompt enrichi
        5. ü§ñ G√©n√©ration LLM
        6. ‚úÖ V√©rification QA
        """
        # Stocker request_id pour le tracker
        self._request_id = request_id or 'unknown'
        # --- FAQ CACHE (avant tout traitement lourd) ---
        # ‚úÖ FAQ CACHE R√âACTIV√â POUR OPTIMISATION PERFORMANCE
        if not skip_faq_cache:
            try:
                from core.faq_answer_cache import faq_answer_cache
                cache_key_context = company_id  # Pour √©viter collisions multi-entreprise
                cached = faq_answer_cache.get(query, cache_key_context)
                if cached:
                    print("‚ö° [FAQ CACHE] R√©ponse instantan√©e (cache hit)")
                    return UniversalRAGResult(
                        response=cached,
                        confidence=1.0,
                        documents_found=True,
                        processing_time_ms=1,
                        search_method="faq_cache",
                        context_used="FAQ_CACHE"
                    )
            except Exception as e:
                print(f"[FAQ CACHE] Erreur cache: {e}")

        """
        üåç TRAITEMENT UNIVERSEL D'UNE REQU√äTE
        
        Principe : Simple, robuste, adaptatif
        """
        start_time = time.time()
        
        print(f"\nüåç [D√âBUT] RAG Universel: '{query[:40]}...'")
        print(f"üè¢ Company: {company_id[:12]}... | User: {user_id[:12]}...")
        
        try:
            # 1. Initialisation si n√©cessaire
            if not self.llm_client:
                print("üîß Initialisation LLM client...")
                await self.initialize()
            
            # 2. Recherche s√©quentielle MeiliSearch ‚Üí Supabase
            search_results = await self.search_sequential_sources(query, company_id, user_id=user_id)
            
            # 3. G√©n√©ration de r√©ponse avec prompt dynamique
            company_display_name = company_name or f"l'entreprise {company_id[:8]}"
            response = await self.generate_response(query, search_results, company_id, company_display_name, user_id, images=images)

            # 4. Traitement des outils dans la r√©ponse LLM
            # üß† SMART CONTEXT MANAGER - Passer company_id et documents sources
            try:
                from core.rag_tools_integration import process_llm_response
                processed_result = process_llm_response(response, user_id, company_id, enable_tools=True)

                response = processed_result['response']
                thinking = processed_result['thinking']

                print(f"üîß [TOOLS] {processed_result['tools_executed']} outils ex√©cut√©s")
                if processed_result['state_updated']:
                    print(f"üíæ [TOOLS] √âtat mis √† jour")

            except Exception as e:
                print(f"‚ö†Ô∏è [TOOLS] Erreur ex√©cution outils: {e}")
                # Continuer avec la r√©ponse originale

            # --- Stockage dans le cache FAQ (apr√®s g√©n√©ration) ---
            # ‚úÖ FAQ CACHE R√âACTIV√â POUR OPTIMISATION PERFORMANCE
            if not skip_faq_cache:
                try:
                    from core.faq_answer_cache import faq_answer_cache
                    cache_key_context = company_id
                    faq_answer_cache.set(query, cache_key_context, response)
                    print("[FAQ CACHE] R√©ponse stock√©e dans le cache FAQ")
                except Exception as e:
                    print(f"[FAQ CACHE] Erreur stockage: {e}")
            # 4. Calcul de la confiance
            confidence = self.calculate_confidence(search_results, response)
            print(f"üìä Confiance calcul√©e: {confidence:.2f}")
            
            # 5. Construction du r√©sultat
            processing_time = (time.time() - start_time) * 1000
            
            # ‚úÖ CRITIQUE: Utiliser self._last_thinking qui est sauvegard√© ligne 1118
            # La variable locale 'thinking' peut √™tre √©cras√©e par des appels ult√©rieurs
            thinking_text = getattr(self, '_last_thinking', thinking)
            print(f"üîç [THINKING_FINAL] Thinking √† retourner: {len(thinking_text)} chars")
            print(f"üîç [THINKING_FINAL] Contenu: {thinking_text[:200] if thinking_text else 'VIDE'}")
            print(f"üîç [THINKING_FINAL] Source: {'self._last_thinking' if hasattr(self, '_last_thinking') else 'variable locale'}")
            
            result = UniversalRAGResult(
                response=response,
                confidence=confidence,
                documents_found=search_results['total_documents'] > 0,
                processing_time_ms=processing_time,
                search_method=search_results['search_method'],
                context_used=search_results['supabase_context'] or search_results['meili_context'] or "Aucun",
                thinking=thinking_text,
                validation=None  # Sera ajout√© par botlive si n√©cessaire
            )
            print(f"‚úÖ [FIN] RAG termin√©: {processing_time:.0f}ms | M√©thode: {search_results['search_method']}")
            
            # üß† AUTO-LEARNING: Track ex√©cution en background
            try:
                from core.auto_learning_wrapper import track_rag_execution
                
                # Extraire thinking_data depuis r√©ponse
                thinking_data = {}
                if hasattr(thinking_result, 'thinking') and thinking_result.thinking:
                    thinking_data = thinking_result.thinking
                
                # Pr√©parer documents utilis√©s
                documents_used = []
                if search_results.get('meili_context'):
                    # Extraire documents depuis contexte MeiliSearch
                    for doc in search_results.get('meili_results', []):
                        documents_used.append({
                            'id': doc.get('id', 'unknown'),
                            'content': doc.get('content', ''),
                            'source': 'meilisearch'
                        })
                if search_results.get('supabase_results'):
                    # Documents Supabase
                    for doc in search_results.get('supabase_results', []):
                        documents_used.append({
                            'id': doc.get('id', 'unknown'),
                            'content': doc.get('content', ''),
                            'source': 'supabase'
                        })
                
                # Track en background (non-bloquant)
                asyncio.create_task(track_rag_execution(
                    company_id=company_id,
                    user_id=user_id,
                    query=query,
                    thinking_data=thinking_data,
                    documents_used=documents_used,
                    response_time_ms=int(processing_time),
                    llm_model=self.llm_model,
                    conversation_id=request_id
                ))
                
            except Exception as learning_error:
                # Silent fail - ne pas bloquer RAG
                print(f"‚ö†Ô∏è [AUTO-LEARNING] Erreur tracking: {learning_error}")
            
            return result
            
        except Exception as e:
            processing_time = (time.time() - start_time) * 1000
            print(f"‚ùå [ERREUR] RAG √©chec: {str(e)[:50]} | {processing_time:.0f}ms")
            
            # Fallback d'urgence
            return UniversalRAGResult(
                response="Je rencontre une difficult√© technique. Pouvez-vous r√©essayer ?",
                confidence=0.1,
                documents_found=False,
                processing_time_ms=processing_time,
                search_method="fallback",
                context_used="Aucun"
            )
    
    def calculate_confidence(self, search_results: Dict[str, Any], response: str) -> float:
        """Calcule la confiance de mani√®re simple"""
        base_confidence = 0.5
        
        # Bonus si documents trouv√©s
        if search_results['total_documents'] > 0:
            base_confidence += 0.3
        
        # Bonus si plusieurs sources
        if len(search_results['search_methods_used']) > 1:
            base_confidence += 0.1
        
        # Bonus si r√©ponse substantielle
        if len(response) > 100:
            base_confidence += 0.1
        
        return min(base_confidence, 1.0)
    
    def _format_meili_context(self, meili_results: str, query_combinations: List[List[str]]) -> str:
        """Formate le contexte MeiliSearch avec titres fixes par index"""
        if not meili_results:
            return ""
        
        print("üé® [ASSEMBLAGE] Formatage contexte MeiliSearch...")
        
        # Simulation du formatage par index (√† adapter selon la vraie structure)
        formatted_context = "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n"
        formatted_context += "‚ïë  üì¶ INFORMATIONS TROUV√âES DANS MEILISEARCH                           ‚ïë\n"
        formatted_context += "‚ïë  Recherche par mots-cl√©s optimis√©e                                   ‚ïë\n"
        formatted_context += "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\n"
        
        # üî• √âTAPE 1: Calculer les scores pour TOUS les documents
        documents = meili_results.split('\n\n')
        doc_scores = []
        
        for doc in documents:
            if doc.strip():
                score = self._calculate_keyword_score(doc, query_combinations)
                keywords = self._extract_found_keywords(doc, query_combinations)
                doc_scores.append({
                    'content': doc,
                    'score': score,
                    'keywords': keywords
                })
        
        # üî• √âTAPE 2: TRIER par score D√âCROISSANT (meilleurs d'abord)
        doc_scores.sort(key=lambda x: x['score'], reverse=True)
        
        # üî• √âTAPE 3: FILTRER - Garder seulement les documents avec score >= 4/10
        filtered_docs = [d for d in doc_scores if d['score'] >= 4]
        
        # Si pas assez de docs avec score √©lev√©, prendre au moins les 3 meilleurs
        if len(filtered_docs) < 3:
            filtered_docs = doc_scores[:3]
        
        # Limiter √† 5 docs maximum
        filtered_docs = filtered_docs[:5]
        
        print(f"üìä [FILTRAGE] {len(documents)} docs ‚Üí {len(filtered_docs)} retenus (score >= 4/10)")
        
        # üî• √âTAPE 4: Afficher dans l'ordre de pertinence
        docs_processed = 0
        for i, doc_info in enumerate(filtered_docs, 1):
            score = doc_info['score']
            stars = self._get_star_rating(score)
            keywords = doc_info['keywords']
            content = doc_info['content']
            
            formatted_context += f"{stars} INFORMATION MEILISEARCH #{i} (Score: {score}/10)\n"
            formatted_context += f"üìä Mots-cl√©s trouv√©s: {keywords}\n"
            formatted_context += f"üìù Contenu: {content[:200]}...\n\n"
            docs_processed += 1
        
        print(f"‚úÖ Assemblage MeiliSearch: {docs_processed} docs format√©s (tri√©s par pertinence)")
        return formatted_context
    
    def _format_supabase_context(self, supabase_docs: List[Dict]) -> str:
        """Formate le contexte Supabase avec titres fixes"""
        if not supabase_docs:
            return ""
        
        print("üé® [ASSEMBLAGE] Formatage contexte Supabase...")
        
        formatted_context = "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n"
        formatted_context += "‚ïë  üìä INFORMATIONS TROUV√âES DANS SUPABASE (S√âMANTIQUE)                ‚ïë\n"
        formatted_context += "‚ïë  Recherche par similarit√© vectorielle                               ‚ïë\n"
        formatted_context += "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n\n"
        
        for i, doc in enumerate(supabase_docs, 1):
            content = doc.get('content', '')  # Document COMPLET sans troncature
            score = doc.get('similarity_score', 0)
            stars = self._get_star_rating(int(score * 10))
            
            formatted_context += f"{stars} DOCUMENT S√âMANTIQUE #{i} (Score: {score:.3f})\n"
            formatted_context += f"üìä Similarit√© cosinus: {score*100:.1f}%\n"
            formatted_context += f"üìù Contenu: {content}\n\n"
        
        print(f"‚úÖ Assemblage Supabase: {len(supabase_docs)} docs format√©s")
        return formatted_context
    
    def _calculate_keyword_score(self, document: str, query_combinations: List[List[str]]) -> int:
        """
        Calcule le score de pertinence bas√© sur les mots-cl√©s trouv√©s ET la puissance des n-grams
        
        Logique de scoring am√©lior√©e :
        - N-gram de 3 mots trouv√© : +5 points (tr√®s pertinent)
        - N-gram de 2 mots trouv√© : +3 points (pertinent)
        - Mot seul trouv√© : +1 point (peu pertinent)
        
        Maximum : 10 points
        """
        doc_lower = document.lower()
        score = 0
        max_score = 10
        
        # D√©tecter les n-grams dans l'ordre d√©croissant (plus long = plus pertinent)
        for combo in query_combinations:
            combo_text = " ".join(combo).lower()
            
            # N-gram de 3+ mots (tr√®s pertinent)
            if len(combo) >= 3 and combo_text in doc_lower:
                score += 5
                continue  # Ne pas compter les mots individuels
            
            # N-gram de 2 mots (pertinent)
            if len(combo) == 2 and combo_text in doc_lower:
                score += 3
                continue
            
            # Mots individuels (peu pertinent mais utile)
            if len(combo) == 1:
                for word in combo:
                    if word.lower() in doc_lower:
                        score += 1
        
        # Normaliser le score sur 10
        return max(1, min(max_score, score))
    
    def _extract_found_keywords(self, document: str, query_combinations: List[List[str]]) -> str:
        """
        Extrait les mots-cl√©s trouv√©s dans le document
        Priorise l'affichage des n-grams trouv√©s (plus pertinent)
        """
        doc_lower = document.lower()
        found_ngrams = []
        found_words = []
        
        # D'abord chercher les n-grams (plus pertinents)
        for combo in query_combinations:
            combo_text = " ".join(combo)
            
            # N-gram de 2+ mots
            if len(combo) >= 2 and combo_text.lower() in doc_lower:
                found_ngrams.append(f'"{combo_text}"')  # Guillemets pour indiquer un n-gram
            # Mots individuels
            elif len(combo) == 1:
                for word in combo:
                    if word.lower() in doc_lower and word not in found_words:
                        found_words.append(word)
        
        # Afficher d'abord les n-grams, puis les mots seuls
        all_found = found_ngrams + found_words
        
        if all_found:
            result = ", ".join(all_found[:6])
            if found_ngrams:
                result += f" (n-gram: {len(found_ngrams)})"
            return result
        
        return "aucun sp√©cifique"
    
    def _get_star_rating(self, score: int) -> str:
        """Convertit un score en notation √©toiles"""
        if score >= 8:
            return "üåüüåüüåüüåüüåü"
        elif score >= 6:
            return "üåüüåüüåüüåü‚≠ê"
        elif score >= 4:
            return "üåüüåüüåü‚≠ê‚≠ê"
        elif score >= 2:
            return "üåüüåü‚≠ê‚≠ê‚≠ê"
        else:
            return "üåü‚≠ê‚≠ê‚≠ê‚≠ê"

# Instance globale (lazy init)
universal_rag = None

def get_universal_rag_engine():
    """Lazy initialization of the global RAG engine"""
    global universal_rag
    if universal_rag is None:
        universal_rag = UniversalRAGEngine()
    return universal_rag

# Fonction d'interface simple
async def get_universal_rag_response(
    message: str, 
    company_id: str, 
    user_id: str, 
    images: List[str] = None,
    conversation_history: str = "",
    skip_faq_cache: bool = False,
    request_id: str = None,
    company_name: str = None
) -> Dict[str, Any]:
    """
    üåç INTERFACE SIMPLE POUR RAG UNIVERSEL
    
    Usage:
    result = await get_universal_rag_response("Que vendez-vous?", "company123", "user456", [], "USER: Bonjour...")
    print(result['response'])
    """
    
    # üîç LOGS M√âMOIRE CONVERSATIONNELLE - POINT D'ENTR√âE RAG
    print(f"üîç [RAG_ENTRY] R√âCEPTION REQU√äTE:")
    print(f"üîç [RAG_ENTRY] Message: '{message}'")
    print(f"üîç [RAG_ENTRY] Company: {company_id}")
    print(f"üîç [RAG_ENTRY] User: {user_id}")
    print(f"üîç [RAG_ENTRY] Images: {len(images) if images else 0}")
    print(f"üîç [RAG_ENTRY] conversation_history: '{conversation_history}'")
    print(f"üîç [RAG_ENTRY] Taille historique: {len(conversation_history)} chars")
    print(f"üîç [RAG_ENTRY] Contient Cocody: {'Cocody' in conversation_history}")
    print()
    
    engine = get_universal_rag_engine()
    result = await engine.process_query(message, company_id, user_id, company_name, skip_faq_cache, request_id, images=images)
    
    return {
        'response': result.response,
        'confidence': result.confidence,
        'documents_found': result.documents_found,
        'processing_time_ms': result.processing_time_ms,
        'search_method': result.search_method,
        'context_used': result.context_used,
        'thinking': getattr(result, 'thinking', ''),  # ‚Üê AJOUT thinking
        'validation': getattr(result, 'validation', None),  # ‚Üê AJOUT validation
        'success': True
    }

if __name__ == "__main__":
    import sys
    
    # Test dynamique avec param√®tres de ligne de commande
    async def test():
        # Param√®tres dynamiques
        company_id = sys.argv[1] if len(sys.argv) > 1 else "MpfnlSbqwaZ6F4HvxQLRL9du0yG3"
        user_id = sys.argv[2] if len(sys.argv) > 2 else "testuser129"
        company_name = sys.argv[3] if len(sys.argv) > 3 else "Rue du Gros"
        message = sys.argv[4] if len(sys.argv) > 4 else "Que vendez-vous?"
        
        print(f"üß™ Test RAG Engine avec:")
        print(f"üè¢ Company: {company_id}")
        print(f"üë§ User: {user_id}")
        print(f"üè™ Name: {company_name}")
        print(f"üí¨ Message: {message}")
        print("-" * 50)
        
        if system_prompt:
            result = await run_llm_with_tools(message, company_id, user_id, system_prompt, conversation_history)
        else:
            result = await universal_rag.process_query(message, company_id, user_id, company_name)
        
        print(f"ü§ñ R√©ponse: {result['response']}")
        print(f"üìä Confiance: {result['confidence']:.2f}")
        print(f"üîç M√©thode: {result['search_method']}")
    
    asyncio.run(test())
    
    def _generate_progression_directive(self, notepad_context: str, user_id: str, company_id: str) -> str:
        """
        G√©n√®re une directive dynamique de progression bas√©e sur l'√©tat actuel
        
        Args:
            notepad_context: Contexte du notepad
            user_id: ID utilisateur
            company_id: ID entreprise
        
        Returns:
            str: Directive format√©e pour le LLM
        """
        try:
            # Analyser le contexte notepad pour d√©terminer ce qui est collect√©
            has_product = False
            has_delivery = False
            has_phone = False
            has_payment = False
            
            if notepad_context:
                has_product = "Produits command√©s:" in notepad_context or "produit" in notepad_context.lower()
                has_delivery = "Zone de livraison:" in notepad_context or "livraison" in notepad_context.lower()
                has_phone = "T√©l√©phone:" in notepad_context or "t√©l√©phone" in notepad_context.lower()
                has_payment = "M√©thode de paiement:" in notepad_context or "paiement" in notepad_context.lower()
            
            # Calculer le taux de compl√©tion
            collected_count = sum([has_product, has_delivery, has_phone, has_payment])
            total_required = 4
            completion_rate = (collected_count / total_required) * 100
            
            # Construire la directive
            directive = "üéØ DIRECTIVE PROGRESSION COMMANDE (RECALCUL√âE EN TEMPS R√âEL):\n"
            directive += f"üìä Taux de compl√©tion: {completion_rate:.0f}% ({collected_count}/{total_required} infos collect√©es)\n\n"
            
            # Statut de chaque champ
            if has_product:
                directive += "‚úÖ PRODUIT collect√©\n"
            else:
                directive += "‚ùå PRODUIT manquant ‚Üí PRIORIT√â ABSOLUE: Identifier le produit (lot 150 ou 300)\n"
            
            if has_delivery:
                directive += "‚úÖ LIVRAISON collect√©e\n"
            else:
                directive += "‚ùå LIVRAISON manquante ‚Üí PROCHAINE √âTAPE: Demander la zone/commune\n"
            
            if has_phone:
                directive += "‚úÖ T√âL√âPHONE collect√©\n"
            else:
                directive += "‚ùå T√âL√âPHONE manquant ‚Üí PROCHAINE √âTAPE: Demander le num√©ro de contact\n"
            
            if has_payment:
                directive += "‚úÖ PAIEMENT collect√©\n"
            else:
                directive += "‚ùå PAIEMENT manquant ‚Üí PROCHAINE √âTAPE: Proposer Wave + acompte 2000 FCFA\n"
            
            # Directive finale selon l'√©tat
            directive += "\n"
            if collected_count == 0:
                directive += "üéØ ACTION IMM√âDIATE: Commencer par identifier le PRODUIT souhait√©\n"
            elif collected_count < total_required:
                # Identifier la prochaine info manquante prioritaire
                if not has_product:
                    next_step = "PRODUIT"
                elif not has_delivery:
                    next_step = "ZONE DE LIVRAISON"
                elif not has_phone:
                    next_step = "T√âL√âPHONE"
                else:
                    next_step = "PAIEMENT"
                
                directive += f"üéØ PROCHAINE INFO √Ä COLLECTER: {next_step}\n"
                directive += "‚ö†Ô∏è IMPORTANT: Adapter la question selon le contexte de l'√©change actuel\n"
            else:
                directive += "üéâ TOUTES LES INFOS COLLECT√âES!\n"
                directive += "üéØ ACTION FINALE: Fournir un R√âCAPITULATIF complet et demander CONFIRMATION\n"
            
            return directive
            
        except Exception as e:
            logger.error(f"‚ùå Erreur g√©n√©ration directive: {e}")
            return ""
