#!/usr/bin/env python3
"""
ğŸš€ RAG ENGINE AMÃ‰LIORÃ‰ AVEC CACHE SÃ‰MANTIQUE RÃ‰VOLUTIONNAIRE
============================================================
IntÃ©gration du SemanticIntentCache avec le systÃ¨me RAG existant

WORKFLOW RÃ‰VOLUTIONNAIRE:
1. DÃ©tection d'intention (systÃ¨me actuel)
2. Recherche cache sÃ©mantique (NOUVEAU)
3. Si cache HIT â†’ RÃ©ponse instantanÃ©e
5. Performance: 10x plus rapide pour les requÃªtes similaires
"""

import asyncio
import time
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime

from core.semantic_intent_cache import (
    get_semantic_intent_cache, 
    create_intent_signature_from_detection,
    IntentSignature
)
from core.universal_rag_engine import UniversalRAGEngine
from utils import log3

class EnhancedRAGWithSemanticCache:
    """
    ğŸ§  RAG Engine amÃ©liorÃ© avec cache sÃ©mantique intention-aware
    
    AMÃ‰LIORATIONS:
    - Cache multi-granulaire basÃ© sur les intentions
    - Recherche sÃ©mantique avec embeddings vectoriels
    - Two-Stage Retrieval pour performance optimale
    - IntÃ©gration transparente avec le systÃ¨me existant
    """
    
    def __init__(self, base_rag_engine: UniversalRAGEngine):
        self.base_rag = base_rag_engine
        self.semantic_cache = get_semantic_intent_cache()
        
        # Configuration du cache
        self.cache_enabled = True
        self.cache_confidence_threshold = 0.4
        self.store_responses_in_cache = True
        
        # Statistiques de performance
        self.performance_stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_cache_response_time": 0.0,
            "avg_rag_response_time": 0.0,
            "total_time_saved": 0.0
        }
        
        log3("[ENHANCED_RAG]", "ğŸš€ RAG Engine avec Cache SÃ©mantique initialisÃ©")
    
    async def process_query(self, 
                          query: str, 
                          company_id: str, 
                          user_id: str = None,
                          conversation_history: str = "",
                          force_rag: bool = False) -> Dict[str, Any]:
        """
        ğŸ¯ Traite une requÃªte avec cache sÃ©mantique intelligent
        
        Args:
            query: Question de l'utilisateur
            company_id: ID de l'entreprise
            user_id: ID de l'utilisateur (optionnel)
            conversation_history: Historique de conversation
            force_rag: Force l'utilisation du RAG (bypass cache)
        
        Returns:
            Dict avec response, source, confidence, timing, etc.
        """
        start_time = time.time()
        self.performance_stats["total_queries"] += 1
        
        # ğŸ” LOGS MÃ‰MOIRE CONVERSATIONNELLE - RÃ‰CEPTION
        print(f"ğŸ” [ENHANCED_RAG] RÃ‰CEPTION CONVERSATION:")
        print(f"ğŸ” [ENHANCED_RAG] Query: '{query}'")
        print(f"ğŸ” [ENHANCED_RAG] conversation_history reÃ§u: '{conversation_history}'")
        print(f"ğŸ” [ENHANCED_RAG] Taille conversation: {len(conversation_history)} chars")
        print(f"ğŸ” [ENHANCED_RAG] Contient Cocody: {'Cocody' in conversation_history}")
        print()
        
        result = {
            "response": "",
            "source": "unknown",
            "confidence": 0.0,
            "cache_hit": False,
            "processing_time": 0.0,
            "intent_detected": None,
            "entities_extracted": {},
            "debug_info": {}
        }
        
        try:
            # Ã‰TAPE 1: DÃ©tection d'intention (utilise ton systÃ¨me actuel)
            intent_detection_start = time.time()
            
            # ğŸ” LOGS MÃ‰MOIRE - TRANSMISSION Ã€ DÃ‰TECTION INTENTION
            print(f"ğŸ” [ENHANCED_RAG] TRANSMISSION Ã€ DÃ‰TECTION INTENTION:")
            print(f"ğŸ” [ENHANCED_RAG] conversation_history transmis: '{conversation_history}'")
            print()
            
            intent_result = await self._detect_intent_with_entities(query, company_id, conversation_history)
            intent_detection_time = time.time() - intent_detection_start
            
            result["intent_detected"] = intent_result.get("primary_intent")
            result["entities_extracted"] = intent_result.get("entities", {})
            result["debug_info"]["intent_detection_time"] = intent_detection_time
            
            # CrÃ©er la signature d'intention pour le cache
            intent_signature = create_intent_signature_from_detection(intent_result)
            
            # Ã‰TAPE 2: Recherche dans le cache sÃ©mantique (si activÃ© et pas forcÃ©)
            if self.cache_enabled and not force_rag:
                cache_start = time.time()
                
                cached_result = await self.semantic_cache.get_cached_response(
                    query=query,
                    intent_signature=intent_signature,
                    conversation_history=conversation_history
                )
                
                cache_time = time.time() - cache_start
                result["debug_info"]["cache_search_time"] = cache_time
                
                if cached_result:
                    cached_response, cache_confidence = cached_result
                    
                    if cache_confidence >= self.cache_confidence_threshold:
                        # CACHE HIT - RÃ©ponse instantanÃ©e !
                        result.update({
                            "response": cached_response,
                            "source": "semantic_cache",
                            "confidence": cache_confidence,
                            "cache_hit": True,
                            "processing_time": time.time() - start_time
                        })
                        
                        # Statistiques
                        self.performance_stats["cache_hits"] += 1
                        self.performance_stats["avg_cache_response_time"] = (
                            (self.performance_stats["avg_cache_response_time"] * (self.performance_stats["cache_hits"] - 1) + 
                             result["processing_time"]) / self.performance_stats["cache_hits"]
                        )
                        
                        # Estimation du temps Ã©conomisÃ© (vs RAG complet)
                        estimated_rag_time = 8.0  # Estimation basÃ©e sur tes mÃ©triques
                        time_saved = max(0, estimated_rag_time - result["processing_time"])
                        self.performance_stats["total_time_saved"] += time_saved
                        result["debug_info"]["time_saved"] = time_saved
                        
                        log3("[ENHANCED_RAG]", f"âœ… CACHE HIT (conf: {cache_confidence:.3f}, {result['processing_time']:.2f}s): {query[:50]}...")
                        return result
            
            # Ã‰TAPE 3: Cache MISS - Utiliser le RAG classique
            rag_start = time.time()
            self.performance_stats["cache_misses"] += 1
            
            log3("[ENHANCED_RAG]", f"ğŸ” Cache MISS - Utilisation RAG classique: {query[:50]}...")
            
            # Utiliser ton systÃ¨me RAG existant
            search_results = await self.base_rag.search_sequential_sources(query, company_id)
            rag_response = await self.base_rag.generate_response(
                query=query,
                search_results=search_results,
                company_id=company_id,
                user_id=user_id
            )
            
            rag_time = time.time() - rag_start
            result["debug_info"]["rag_processing_time"] = rag_time
            
            # Statistiques RAG
            self.performance_stats["avg_rag_response_time"] = (
                (self.performance_stats["avg_rag_response_time"] * (self.performance_stats["cache_misses"] - 1) + 
                 rag_time) / self.performance_stats["cache_misses"]
            )
            
            # Ã‰TAPE 4: Stocker la rÃ©ponse dans le cache sÃ©mantique
            if self.store_responses_in_cache and rag_response and len(rag_response.strip()) > 10:
                store_start = time.time()
                
                await self.semantic_cache.store_response(
                    query=query,
                    response=rag_response,
                    intent_signature=intent_signature,
                    conversation_history=conversation_history,
                    ttl_seconds=3600  # 1 heure par dÃ©faut
                )
                
                store_time = time.time() - store_start
                result["debug_info"]["cache_store_time"] = store_time
                
                log3("[ENHANCED_RAG]", f"ğŸ’¾ RÃ©ponse stockÃ©e dans cache: {intent_signature.primary_intent}")
            
            # Finaliser le rÃ©sultat
            result.update({
                "response": rag_response,
                "source": "rag_engine",
                "confidence": search_results.get("confidence", 0.8),
                "cache_hit": False,
                "processing_time": time.time() - start_time,
                "search_results": search_results,
                "sources": self._extract_sources_from_search_results(search_results)
            })
            
            log3("[ENHANCED_RAG]", f"âœ… RAG rÃ©ponse gÃ©nÃ©rÃ©e ({result['processing_time']:.2f}s): {len(rag_response)} chars")
            return result
            
        except Exception as e:
            log3("[ENHANCED_RAG]", f"âŒ Erreur traitement requÃªte: {e}")
            result.update({
                "response": "DÃ©solÃ©, une erreur s'est produite lors du traitement de votre demande.",
                "source": "error",
                "confidence": 0.0,
                "cache_hit": False,
                "processing_time": time.time() - start_time,
                "error": str(e)
            })
            return result
    
    async def _detect_intent_with_entities(self, query: str, company_id: str, conversation_history: str) -> Dict[str, Any]:
        """
        ğŸ¯ DÃ©tection d'intention avec extraction d'entitÃ©s
        
        Cette fonction utilise ton systÃ¨me de dÃ©tection d'intention existant.
        Tu peux l'adapter selon ton implÃ©mentation actuelle.
        """
        # ğŸ” LOGS MÃ‰MOIRE - DANS DÃ‰TECTION INTENTION
        print(f"ğŸ” [INTENT_DETECTION] RÃ‰CEPTION DANS DÃ‰TECTION:")
        print(f"ğŸ” [INTENT_DETECTION] Query: '{query}'")
        print(f"ğŸ” [INTENT_DETECTION] conversation_history: '{conversation_history}'")
        print(f"ğŸ” [INTENT_DETECTION] Taille: {len(conversation_history)} chars")
        print()
        
        try:
            # PLACEHOLDER: Remplace par ton systÃ¨me de dÃ©tection d'intention actuel
            # Exemple d'intÃ©gration avec ton systÃ¨me existant:
            
            # Option 1: Si tu as un module de dÃ©tection d'intention
            # from core.advanced_intent_classifier import detect_intent
            # return await detect_intent(query, company_id, conversation_history)
            
            # Option 2: Utiliser les patterns de ton systÃ¨me actuel
            intent_result = await self._basic_intent_detection(query, conversation_history)
            
            # ğŸ” LOGS MÃ‰MOIRE - RÃ‰SULTAT DÃ‰TECTION INTENTION
            print(f"ğŸ” [INTENT_DETECTION] RÃ‰SULTAT DÃ‰TECTION:")
            print(f"ğŸ” [INTENT_DETECTION] Intent dÃ©tectÃ©: {intent_result.get('primary_intent')}")
            print(f"ğŸ” [INTENT_DETECTION] Contexte utilisÃ©: '{intent_result.get('context', '')}'")
            print()
            
            return intent_result
            
        except Exception as e:
            log3("[ENHANCED_RAG]", f"âš ï¸ Erreur dÃ©tection intention: {e}")
            # Fallback: intention gÃ©nÃ©rique
            return {
                "primary_intent": "GENERAL_QUERY",
                "secondary_intents": [],
                "entities": self._extract_basic_entities(query),
                "confidence": 0.5,
                "context": conversation_history
            }
    
    async def _basic_intent_detection(self, query: str, conversation_history: str) -> Dict[str, Any]:
        """
        ğŸ” DÃ©tection d'intention basique (Ã  remplacer par ton systÃ¨me)
        """
        # ğŸ” LOGS MÃ‰MOIRE - DANS BASIC INTENT DETECTION
        print(f"ğŸ” [BASIC_INTENT] ANALYSE INTENTION:")
        print(f"ğŸ” [BASIC_INTENT] Query: '{query}'")
        print(f"ğŸ” [BASIC_INTENT] conversation_history reÃ§u: '{conversation_history}'")
        print(f"ğŸ” [BASIC_INTENT] Utilise le contexte: {len(conversation_history) > 0}")
        print()
        
        query_lower = query.lower()
        
        # DÃ©tection basique par mots-clÃ©s (Ã  amÃ©liorer avec ton systÃ¨me)
        if any(word in query_lower for word in ["prix", "coÃ»t", "combien", "tarif"]):
            primary_intent = "PRIX_INFORMATION"
        elif any(word in query_lower for word in ["livraison", "livrer", "envoyer"]):
            primary_intent = "LIVRAISON_INFORMATION"
        elif any(word in query_lower for word in ["produit", "article", "item"]):
            primary_intent = "PRODUIT_INFORMATION"
        elif any(word in query_lower for word in ["paiement", "payer", "wave"]):
            primary_intent = "PAIEMENT_INFORMATION"
        elif any(word in query_lower for word in ["contact", "tÃ©lÃ©phone", "whatsapp"]):
            primary_intent = "CONTACT_INFORMATION"
        else:
            primary_intent = "GENERAL_QUERY"
        
        # Extraction d'entitÃ©s basiques
        entities = self._extract_basic_entities(query)
        
        return {
            "primary_intent": primary_intent,
            "secondary_intents": [],
            "entities": entities,
            "confidence": 0.8,
            "context": conversation_history
        }
    
    def _extract_basic_entities(self, query: str) -> Dict[str, str]:
        """
        ğŸ·ï¸ Extraction d'entitÃ©s basiques (Ã  amÃ©liorer avec ton systÃ¨me)
        """
        entities = {}
        query_lower = query.lower()
        
        # Zones gÃ©ographiques
        zones = ["cocody", "yopougon", "plateau", "adjamÃ©", "abobo", "marcory", "koumassi", "treichville"]
        for zone in zones:
            if zone in query_lower:
                entities["zone"] = zone.title()
                break
        
        # Produits
        produits = ["casque", "couche", "pampers", "smartphone", "ordinateur"]
        for produit in produits:
            if produit in query_lower:
                entities["produit"] = produit
                break
        
        # Couleurs
        couleurs = ["rouge", "bleu", "noir", "blanc", "vert", "jaune"]
        for couleur in couleurs:
            if couleur in query_lower:
                entities["couleur"] = couleur
                break
        
        return entities
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ğŸ“Š Retourne les statistiques de performance"""
        cache_stats = self.semantic_cache.get_stats()
        
        total_queries = max(1, self.performance_stats["total_queries"])
        cache_hit_rate = (self.performance_stats["cache_hits"] / total_queries) * 100
        
        return {
            "enhanced_rag_stats": {
                **self.performance_stats,
                "cache_hit_rate_percent": round(cache_hit_rate, 2),
                "avg_time_saved_per_query": (
                    self.performance_stats["total_time_saved"] / total_queries
                )
            },
            "semantic_cache_stats": cache_stats,
            "performance_improvement": {
                "estimated_speedup": f"{cache_hit_rate:.1f}% des requÃªtes 10x plus rapides",
                "total_time_saved_minutes": round(self.performance_stats["total_time_saved"] / 60, 2)
            }
        }
    
    def _extract_sources_from_search_results(self, search_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extrait les sources des rÃ©sultats de recherche"""
        sources = []
        
        # Extraire de Supabase
        supabase_context = search_results.get("supabase_context", "")
        if supabase_context:
            sources.append({
                "content": supabase_context[:500],
                "score": search_results.get("confidence", 0.8),
                "index": "supabase_vector"
            })
        
        # Extraire de MeiliSearch
        meili_context = search_results.get("meili_context", "")
        if meili_context:
            sources.append({
                "content": meili_context[:500],
                "score": 0.9,  # MeiliSearch gÃ©nÃ©ralement plus prÃ©cis
                "index": "meilisearch"
            })
        
        return sources
    
    async def clear_cache(self, company_id: Optional[str] = None):
        """ğŸ—‘ï¸ Vide le cache sÃ©mantique"""
        await self.semantic_cache.clear_cache(company_id)
        log3("[ENHANCED_RAG]", f"ğŸ—‘ï¸ Cache vidÃ© pour: {company_id or 'toutes les entreprises'}")
    
    def enable_cache(self, enabled: bool = True):
        """âš™ï¸ Active/dÃ©sactive le cache sÃ©mantique"""
        self.cache_enabled = enabled
        log3("[ENHANCED_RAG]", f"âš™ï¸ Cache sÃ©mantique: {'activÃ©' if enabled else 'dÃ©sactivÃ©'}")
    
    def set_cache_confidence_threshold(self, threshold: float):
        """ğŸ¯ DÃ©finit le seuil de confiance pour utiliser le cache"""
        self.cache_confidence_threshold = max(0.0, min(1.0, threshold))
        log3("[ENHANCED_RAG]", f"ğŸ¯ Seuil confiance cache: {self.cache_confidence_threshold}")

# Factory function pour crÃ©er l'instance
def create_enhanced_rag_with_cache(base_rag_engine: UniversalRAGEngine) -> EnhancedRAGWithSemanticCache:
    """ğŸ­ Factory pour crÃ©er un RAG Engine amÃ©liorÃ© avec cache sÃ©mantique"""
    return EnhancedRAGWithSemanticCache(base_rag_engine)

if __name__ == "__main__":
    # Test d'intÃ©gration
    async def test_enhanced_rag():
        print("ğŸš€ TEST RAG ENGINE AVEC CACHE SÃ‰MANTIQUE")
        print("=" * 60)
        
        # CrÃ©er une instance de test (tu devras adapter avec ton RAG rÃ©el)
        base_rag = UniversalRAGEngine()
        await base_rag.initialize()
        
        enhanced_rag = create_enhanced_rag_with_cache(base_rag)
        
        # Test 1: PremiÃ¨re requÃªte (cache miss)
        print("\nğŸ“ Test 1: PremiÃ¨re requÃªte")
        result1 = await enhanced_rag.process_query(
            query="Combien coÃ»te la livraison Ã  Cocody ?",
            company_id="test_company",
            user_id="test_user",
            conversation_history="L'utilisateur demande des informations."
        )
        
        print(f"RÃ©sultat 1: {result1['source']} - {result1['processing_time']:.2f}s")
        print(f"Cache hit: {result1['cache_hit']}")
        
        # Test 2: RequÃªte similaire (cache hit attendu)
        print("\nğŸ“ Test 2: RequÃªte similaire")
        result2 = await enhanced_rag.process_query(
            query="Quel est le tarif pour envoyer Ã  Cocody ?",
            company_id="test_company",
            user_id="test_user",
            conversation_history="L'utilisateur s'intÃ©resse aux coÃ»ts."
        )
        
        print(f"RÃ©sultat 2: {result2['source']} - {result2['processing_time']:.2f}s")
        print(f"Cache hit: {result2['cache_hit']}")
        
        # Afficher les statistiques
        stats = enhanced_rag.get_performance_stats()
        print(f"\nğŸ“Š Statistiques de performance:")
        print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    import json
    asyncio.run(test_enhanced_rag())
