"""
üöÄ RAG ENGINE SIMPLIFI√â - VERSION COMPL√àTEMENT FONCTIONNELLE
Architecture √©pur√©e, performante et maintenable SANS d√©pendances manquantes
"""
import asyncio
import logging
import traceback
from typing import Dict, List, Optional, Any
from core.preprocessing import post_hyde_filter
from core.smart_stopwords import filter_query_for_meilisearch
from core.cache_manager import cache_manager
from core.quick_context_lookup import QuickContextLookup
from core.context_sources import ContextSources
from core.dynamic_offtopic_detector import DynamicOffTopicDetector
from core.intelligent_hallucination_guard import IntelligentHallucinationGuard
from core.intention_router import intention_router
from core.llm_client import GroqLLMClient
from core.business_config_manager import BusinessConfigManager
from core.prompt_manager import PromptManager
from database.supabase_client import get_supabase_client
from database.vector_store import search_meili_keywords  # Utilise la version optimis√©e MeiliSearch
from core.price_calculator import calculate_order_price, extract_delivery_zone_from_conversation
from core.recap_template import generate_order_summary, extract_customer_info_from_conversation
from core.progressive_memory_system import process_conversation_message, get_memory_system
from utils import log3
from datetime import datetime
import os
import json
import uuid
import hashlib
import time
import logging
from core.query_combinator import generate_query_combinations
# from core.meilisearch_orchestrator import   # D√©sactiv√©, non critique pour la recherche principale
from core.dynamic_catalog_loader import _extract_delivery_cost, _extract_delivery_time, _extract_zones_from_text
from core.adaptive_meili_optimizer import get_meili_optimizer
from core.hyde_search_optimizer import HydeSearchOptimizer

logger = logging.getLogger(__name__)
# from .dual_search_engine import DualSearchEngine  # Module non utilis√©
from .conversation_memory import conversation_memory, update_conversation_context
from .smart_context_fusion import fuse_all_contexts, enhance_context_with_suggestions
from .conversation import save_message, get_history
from .dynamic_offtopic_detector import analyze_offtopic_query
from core.supabase_vector_search import SupabaseVectorSearch
from database.vector_store import search_meili_keywords  # Utilise la version optimis√©e MeiliSearch
from core.adaptive_meili_optimizer import get_meili_optimizer
from core.hyde_search_optimizer import HydeSearchOptimizer

# üöÄ NOUVEAUX SYST√àMES ANTI-HALLUCINATION 2024
from .advanced_intent_classifier import classify_intent_advanced, IntentType
from .context_aware_hallucination_guard import validate_response_contextual, ValidationLevel
from .intelligent_fallback_system import generate_intelligent_fallback, FallbackType
from .confidence_scoring_system import calculate_confidence_score, ConfidenceLevel

# üìä SYST√àME DE LOGGING D√âTAILL√â
from .detailed_logging_system import (
    start_detailed_logging, log_intent_classification, log_document_search,
    log_llm_generation, log_validation, log_confidence_scoring, log_fallback,
    log_response_sent, log_error, get_structured_logs, save_logs_to_file
)

# === MODE DEBUG ANTI-HALLUCINATION ===
DEBUG_HALLUCINATION = True  # Activ√© par d√©faut pour traquer les hallucinations

def debug_log_hallucination(label: str, content: str, force_full: bool = True):
    """Log sp√©cialis√© pour traquer les hallucinations - affiche le contenu complet"""
    if DEBUG_HALLUCINATION:
        display_content = content
        print(f"\nüîç [ANTI-HALLUCINATION DEBUG] {label}")
        print("=" * 80)
        print(display_content)
        print("=" * 80 + "\n")

class SimplifiedRAGEngine:
    """
    üéØ MOTEUR RAG SIMPLIFI√â
    - Architecture claire et modulaire
    - Dual search: Supabase (s√©mantique) + Meilisearch (full-text)
    - Performance optimis√©e avec async parall√®le
    - Logs structur√©s et debugging facile
    """

    def __init__(self):
        self.llm_client = GroqLLMClient()
        self.config_manager = BusinessConfigManager()
        self.supabase_client = get_supabase_client()
        self.prompt_manager = PromptManager(self.supabase_client)
        self.hallucination_guard = IntelligentHallucinationGuard()

    async def dual_search(self, query: str, company_id: str) -> tuple[List[Dict], str, str]:
        """
        üîç RECHERCHE DUALE OPTIMIS√âE AVEC ROUTAGE D'INTENTIONS
        Supabase s√©mantique + Meilisearch cibl√© par intention + D√©tection hors-sujet
        """
        log3("[DUAL_SEARCH]", f"üöÄ D√©but recherche: '{query}'")
        
        # 0. D√©tection hors-sujet avec fallback intelligent
        try:
            offtopic_analysis = await analyze_offtopic_query(query, company_id, context_available=False)
            # G√©rer le cas o√π offtopic_analysis est un objet QueryAnalysis
            if hasattr(offtopic_analysis, 'is_offtopic'):
                is_offtopic = offtopic_analysis.is_offtopic
            elif isinstance(offtopic_analysis, dict):
                is_offtopic = offtopic_analysis.get("is_offtopic", False)
            else:
                is_offtopic = False
        except Exception as e:
            log3("[DUAL_SEARCH]", f"‚ö†Ô∏è Erreur d√©tection hors-sujet: {e}")
            is_offtopic = False
        
        if is_offtopic:
            log3("[DUAL_SEARCH]", "üö´ Requ√™te hors-sujet d√©tect√©e")
            return [], "", ""
        
        # 1. G√©n√©ration d'hypoth√®se HYDE avanc√©e
        try:
            # D√©tection d'intention pour HYDE
            if any(word in query.lower() for word in ['vendez', 'produits', 'services', 'catalogue', 'offre']):
                intent_type = "product_inquiry"
            elif any(word in query.lower() for word in ['prix', 'co√ªt', 'tarif', 'combien']):
                intent_type = "product_inquiry"
            elif any(word in query.lower() for word in ['livraison', 'd√©lai', 'transport', 'exp√©dition']):
                intent_type = "delivery_info"
            elif any(word in query.lower() for word in ['contact', 't√©l√©phone', 'email', 'aide', 'support']):
                intent_type = "support_contact"
            elif any(word in query.lower() for word in ['entreprise', 'soci√©t√©', 'qui', '√† propos']):
                intent_type = "company_info"
            else:
                intent_type = "general"
            
            # G√©n√©ration d'hypoth√®se selon l'intention
            if intent_type == "product_inquiry":
                optimized_query = f"""Voici un catalogue d√©taill√© de nos produits et services :

PRODUITS PRINCIPAUX :
- Produit A : Description d√©taill√©e, caract√©ristiques, avantages
- Produit B : Sp√©cifications techniques, cas d'usage  
- Produit C : Gamme compl√®te, options personnalis√©es

SERVICES ASSOCI√âS :
- Service de conseil et d'accompagnement
- Support technique et maintenance
- Formation et documentation

NOTRE EXPERTISE :
- X ann√©es d'exp√©rience dans le domaine
- Certifications et qualifications
- R√©f√©rences clients et t√©moignages

Pour plus d'informations sur nos produits, contactez-nous ou consultez notre catalogue d√©taill√©."""
            elif intent_type == "pricing_info":
                optimized_query = f"""Voici nos tarifs et conditions commerciales :

TARIFS DE BASE :
- Produit A : Prix de base, options et variantes
- Produit B : Tarifs d√©gressifs selon quantit√©
- Service C : Forfaits et tarifs horaires

CONDITIONS SP√âCIALES :
- Remises pour commandes importantes
- Tarifs pr√©f√©rentiels pour partenaires
- Offres promotionnelles saisonni√®res

MODALIT√âS DE PAIEMENT :
- Paiement comptant : Remise de X%
- Paiement diff√©r√© : Conditions et d√©lais
- Financement : Solutions de paiement √©chelonn√©

Pour un devis personnalis√©, contactez notre service commercial."""
            elif intent_type == "delivery_info":
                optimized_query = f"""Nos options de livraison et d√©lais :

LIVRAISON STANDARD :
- D√©lai : X jours ouvr√©s
- Zone de livraison : France m√©tropolitaine
- Suivi de commande en temps r√©el

LIVRAISON EXPRESS :
- D√©lai : 24-48h
- Zones prioritaires
- Frais suppl√©mentaires

LIVRAISON INTERNATIONALE :
- D√©lais selon destination
- Formalit√©s douani√®res
- Assurance transport

INSTALLATION ET MISE EN SERVICE :
- Service d'installation professionnel
- Formation utilisateur
- Garantie et SAV

Consultez nos conditions g√©n√©rales pour plus de d√©tails."""
            else:
                optimized_query = f"""Document hypoth√©tique pour: {query}

Ce document contient des informations d√©taill√©es sur le sujet demand√©, incluant :
- Contexte et explications
- D√©tails techniques et pratiques
- Exemples et cas d'usage
- Ressources et r√©f√©rences

Pour toute question compl√©mentaire, n'h√©sitez pas √† nous contacter."""
            
            hyde_metadata = {
                "original_query": query,
                "hypothesis": optimized_query,
                "intent_type": intent_type,
                "confidence": 0.8,
                "generation_method": "advanced_template"
            }
            
            log3("[HYDE_ADVANCED]", {
                "original_query": query,
                "intent_type": intent_type,
                "hypothesis_length": len(optimized_query),
                "hypothesis_preview": optimized_query,
                "confidence": 0.8
            })
        except Exception as e:
            log3("[HYDE_ADVANCED]", f"‚ö†Ô∏è Erreur HYDE avanc√©: {e}")
            optimized_query = query
            hyde_metadata = {}
        
        # Utiliser la requ√™te optimis√©e pour les deux recherches
        hyde_hypothesis = optimized_query
        
        # 2. Recherche Supabase avec HyDE sp√©cialis√©
        supabase_start = time.time()
        
        async def supabase_search_async():
            try:
                log3("[DEBUG_SUPABASE]", "üîç D√©but supabase_search_async")
                # Utilisation de la m√©thode qui fonctionne (comme dans le test)
                supabase_engine = SupabaseVectorSearch()
                await supabase_engine.initialize()
                
                # G√©n√©ration embedding avec la m√™me m√©thode que le test
                embedding = await supabase_engine.generate_embedding(hyde_hypothesis)
                
                results = await supabase_engine.search_vectors(
                    query_embedding=embedding,
                    company_id=company_id,
                    top_k=5,
                    min_score=0.3
                )
                
                # Formatage des r√©sultats pour compatibilit√©
                formatted_results = []
                context_parts = []
                
                for result in results:
                    formatted_results.append({
                        "content": result.content,
                        "score": result.score,
                        "title": result.metadata.get("title", "Document"),
                        "chunk_id": result.id,
                        "metadata": result.metadata
                    })
                    context_parts.append(result.content)
                
                context = "\n\n".join(context_parts)
                return formatted_results, context
            except Exception as e:
                log3("[SUPABASE_ERROR]", f"Erreur recherche Supabase: {e}")
                return [], ""
        
        supabase_task = asyncio.create_task(supabase_search_async())
        
        # 3. Optimisation MeiliSearch avec apprentissage adaptatif
        async def meili_search_async():
            try:
                # Utiliser la requ√™te HyDE optimis√©e pour MeiliSearch aussi
                meili_query = hyde_hypothesis
                
                # Optimisation MeiliSearch adaptative
                try:
                    from core.adaptive_meili_optimizer import optimize_meili_query
                    optimized_meili_query, optimization_metadata = await optimize_meili_query(
                        meili_query, 
                        company_id, 
                        hyde_metadata
                    )
                    # Log supprim√© pour clarifier diagnostic Supabase
                except Exception as e:
                    # Log supprim√© pour clarifier diagnostic Supabase
                    optimized_meili_query = meili_query
                
                # FILTRAGE STOP WORDS: Appliquer le syst√®me de filtrage avant MeiliSearch
                filtered_query = filter_query_for_meilisearch(optimized_meili_query)
                logger.info(f"[STOP_WORDS_FILTER] üîç FILTRAGE STOP WORDS APPLIQU√â (OPTIMIZER):")
                logger.info(f"[STOP_WORDS_FILTER] - Query originale: '{optimized_meili_query}'")
                logger.info(f"[STOP_WORDS_FILTER] - Query filtr√©e: '{filtered_query}'")
                logger.info(f"[STOP_WORDS_FILTER] - R√©duction: {len(optimized_meili_query)} ‚Üí {len(filtered_query)} caract√®res")
                
                # Recherche MeiliSearch
                return await search_meili_keywords(
                    query=filtered_query,  # Query filtr√©e avec stop words
                    company_id=company_id,
                    target_indexes=None
                )
            except Exception as e:
                # Log supprim√© pour clarifier diagnostic Supabase
                return []
        
        meili_start = time.time()
        meili_task = asyncio.create_task(meili_search_async())
        
        # Attente des r√©sultats en parall√®le
        try:
            log3("[DEBUG_GATHER]", "üîç AVANT asyncio.gather() - T√¢ches cr√©√©es")
            (supabase_results, supabase_context), meili_results = await asyncio.gather(
                supabase_task,
                meili_task,
                return_exceptions=True
            )
            
            # Calcul des temps de performance
            supabase_time = (time.time() - supabase_start) * 1000
            meili_time = (time.time() - meili_start) * 1000
            
            log3("[PERFORMANCE_SEARCH]", {
                "supabase_time_ms": f"{supabase_time:.2f}",
                "meili_time_ms": f"{meili_time:.2f}",
                "total_search_time_ms": f"{max(supabase_time, meili_time):.2f}"
            })
            
            # Gestion des erreurs avec traceback d√©taill√©
            if isinstance(supabase_results, Exception):
                log3("[ERROR_SUPABASE]", {
                    "error_type": type(supabase_results).__name__,
                    "error_message": str(supabase_results),
                    "traceback_preview": traceback.format_exc(),
                    "query_preview": optimized_query,
                    "company_id": company_id
                })
                supabase_results, supabase_context = [], ""
            
            if isinstance(meili_results, Exception):
                # Log supprim√© pour clarifier diagnostic Supabase
                meili_results = []
            
            # Mise √† jour des performances de l'optimiseur MeiliSearch
            if meili_results and not isinstance(meili_results, Exception):
                try:
                    optimizer = get_meili_optimizer(company_id)
                    await optimizer.update_pattern_performance(
                        query, 
                        len(meili_results), 
                        100  # Temps approximatif - sera am√©lior√© avec monitoring
                    )
                except Exception as e:
                    # Log supprim√© pour clarifier diagnostic Supabase
                    pass
        
            # LOGS DE TRANSFORMATION DE DONN√âES AVANT FUSION
            log3("[DATA_TRANSFORMATION]", {
                "supabase_raw_type": type(supabase_results).__name__,
                "supabase_raw_length": len(supabase_results) if hasattr(supabase_results, '__len__') else "N/A",
                "supabase_preview": str(supabase_results),
                "meili_raw_type": type(meili_results).__name__,
                "meili_raw_length": len(meili_results) if hasattr(meili_results, '__len__') else "N/A",
                "meili_preview": str(meili_results)
            })
            
            # LOGS D√âTAILL√âS COMPLETS - TOUS LES DOCUMENTS TROUV√âS
            # S√©curisation des types pour √©viter 'str' object has no attribute 'get'
            transformation_start = time.time()
            
            safe_supabase_results = []
            if isinstance(supabase_results, list):
                for doc in supabase_results:
                    if isinstance(doc, dict):
                        safe_supabase_results.append(doc)
                    else:
                        safe_supabase_results.append({"content": str(doc), "score": "N/A", "title": "Raw Result"})
            
            safe_meili_results = []
            if not isinstance(meili_results, Exception) and meili_results:
                # CORRECTION: MeiliSearch retourne un dict avec 'results' key
                if isinstance(meili_results, dict) and 'results' in meili_results:
                    # Extraire les r√©sultats du dict MeiliSearch
                    actual_results = meili_results['results']
                    # Log supprim√© pour clarifier diagnostic Supabase
                    for doc in actual_results:
                        if isinstance(doc, dict):
                            safe_meili_results.append(doc)
                        else:
                            safe_meili_results.append({"content": str(doc), "score": "N/A", "title": "Raw Result"})
                elif isinstance(meili_results, list):
                    for doc in meili_results:
                        if isinstance(doc, dict):
                            safe_meili_results.append(doc)
                        else:
                            safe_meili_results.append({"content": str(doc), "score": "N/A", "title": "Raw Result"})
                elif isinstance(meili_results, str):
                    safe_meili_results.append({"content": meili_results, "score": "N/A", "title": "String Result"})
            
            transformation_time = (time.time() - transformation_start) * 1000
            log3("[PERFORMANCE_TRANSFORMATION]", f"Temps transformation donn√©es: {transformation_time:.2f}ms")
            
            # === 2. DOCUMENTS SUPABASE COMPLETS ===
            if safe_supabase_results:
                supabase_debug = f"TOTAL: {len(safe_supabase_results)} documents\n\n"
                for i, doc in enumerate(safe_supabase_results[:2]): # Limiter l'aper√ßu √† 2 documents
                    supabase_debug += f"--- DOCUMENT {i+1} ---\n"
                    supabase_debug += f"Score: {doc.get('score', 'N/A')}\n"
                    supabase_debug += f"Titre: {doc.get('title', 'N/A')}\n"
                    supabase_debug += f"Contenu aper√ßu: {doc.get('content', '')}\n\n"
                debug_log_hallucination("DOCUMENTS SUPABASE TROUV√âS", supabase_debug)
            else:
                debug_log_hallucination("DOCUMENTS SUPABASE TROUV√âS", "Aucun document trouv√©.")
            
            # === 3. DOCUMENTS MEILISEARCH COMPLETS ===
            if safe_meili_results:
                meili_debug = f"TOTAL: {len(safe_meili_results)} documents\n\n"
                for i, doc in enumerate(safe_meili_results[:2]): # Limiter l'aper√ßu √† 2 documents
                    meili_debug += f"--- DOCUMENT {i+1} ---\n"
                    meili_debug += f"Score: {doc.get('score', 'N/A')}\n"
                    meili_debug += f"Titre: {doc.get('title', 'N/A')}\n"
                    meili_debug += f"Contenu aper√ßu: {doc.get('content', '')[:100]}...\n\n"
                debug_log_hallucination("DOCUMENTS MEILISEARCH TROUV√âS", meili_debug)
            else:
                debug_log_hallucination("DOCUMENTS MEILISEARCH TROUV√âS", "Aucun document trouv√©.")

            # Log d√©taill√© des documents trouv√©s
            log3("[DOCUMENTS_TROUVES_SUPABASE]", {
                "nombre_documents": len(supabase_results),
                "documents": [
                    {
                        "index": i+1,
                        "score": doc.get("score", "N/A"),
                        "title": doc.get("title", "Sans titre"),
                        "content_preview": doc.get("content", ""),
                        "metadata": doc.get("metadata", {})
                    }
                    for i, doc in enumerate(supabase_results[:3])  # Limite √† 3 pour lisibilit√©
                ]
            })
            
            log3("[DOCUMENTS_TROUVES_MEILI]", {
                "nombre_documents": len(safe_meili_results),
                "documents": [
                    {
                        "index": i+1,
                        "id": doc.get("id", "N/A"),
                        "type": doc.get("type", "N/A"),
                        "content_preview": doc.get("content", "")
                    }
                    for i, doc in enumerate(safe_meili_results[:3])
                ]
            })

            log3("[DUAL_SEARCH]", {
                "supabase_results": len(supabase_results),
                "meili_results": len(safe_meili_results),
                "context_length": len(supabase_context),
                "hyde_enhanced": True
            })
            
            # Fusion des contextes MeiliSearch + Supabase
            merged_context = self.merge_contexts(supabase_context, safe_meili_results)
            
            return supabase_results, safe_meili_results, merged_context
            
        except Exception as e:
            log3("[DUAL_SEARCH]", f"üí• Erreur g√©n√©rale: {str(e)}")
            return [], [], ""

    def merge_contexts(self, supabase_context: str, meili_results) -> str:
        """
        üîó FUSION INTELLIGENTE DES CONTEXTES
        Combine les r√©sultats Supabase et MeiliSearch de mani√®re optimale
        """
        contexts = []
        
        if supabase_context and supabase_context.strip():
            contexts.append(f"=== CONTEXTE S√âMANTIQUE ===\n{supabase_context}")
        
        if meili_results and isinstance(meili_results, list):
            meili_formatted = []
            for i, doc in enumerate(meili_results, 1):
                if isinstance(doc, dict):
                    doc_text = f"=== PRODUIT {i} ===\n"
                    doc_text += f"ID: {doc.get('id', 'N/A')}\n"
                    doc_text += f"Type: {doc.get('type', 'N/A')}\n"
                    if doc.get('content'):
                        doc_text += f"Contenu: {doc.get('content', '')}\n"
                    if doc.get('metadata'):
                        doc_text += f"M√©tadonn√©es: {doc.get('metadata', {})}\n"
                    meili_formatted.append(doc_text)
            
            if meili_formatted:
                contexts.append(f"=== RECHERCHE TEXTUELLE ===\n" + "\n".join(meili_formatted))
        
        return "\n\n".join(contexts) if contexts else ""

    async def generate_response(self, query: str, context: str, company_id: str, conversation_history: str = "") -> str:
        """
        G√©n√®re une r√©ponse en utilisant le LLM avec le contexte fourni et l'historique.
        """
        start_time = datetime.now()
        
        # R√©cup√©rer le prompt syst√®me personnalis√©
        prompt_data = await self.prompt_manager.get_active_prompt(company_id)
        system_prompt = "Vous √™tes un assistant IA sp√©cialis√© dans l'analyse de documents d'entreprise."
        
        if prompt_data:
            system_prompt = prompt_data.get('prompt_template', system_prompt)
            log3("[RAG SIMPLIFIED] Prompt personnalis√©", f"Version: {prompt_data.get('version', 'N/A')}")
        
        # Construction du prompt complet avec historique
        history_section = ""
        if conversation_history and conversation_history.strip():
            history_section = f"\n\nHistorique de la conversation:\n{conversation_history[-1000:]}\n"  # Limiter √† 1000 chars
        
        full_prompt = f"""{system_prompt}

Contexte:
{context}{history_section}

Question: {query}

R√©ponse:"""
        
        # Logger le prompt complet pour debugging
        await self.log_prompt_to_supabase(company_id, query, full_prompt, context, conversation_history)
        
        log3("[RAG SIMPLIFIED] Prompt construit", f"Taille: {len(full_prompt)} caract√®res")
        
        try:
            # Configuration du mod√®le LLM principal (70B avec fallback automatique)
            main_model = os.getenv("GROQ_LLM_MODEL", "llama-3.3-70b-versatile")
            
            response = await self.llm_client.complete(
                prompt=full_prompt,
                model_name=main_model,  # Utilise le LLM principal 70B
                temperature=0.2,
                max_tokens=500
            )
            
            generation_time = (datetime.now() - start_time).total_seconds()
            # === 5. R√âPONSE FINALE DU LLM ===
            debug_log_hallucination("R√âPONSE FINALE DU LLM", response)
            log3("[RAG SIMPLIFIED] R√©ponse g√©n√©r√©e", f"Temps: {generation_time:.2f}s | Taille: {len(response)} chars")
            
            return response
            
        except Exception as e:
            log3("[RAG SIMPLIFIED] Erreur g√©n√©ration", f"{type(e).__name__}: {str(e)}")
            return "D√©sol√©, une erreur s'est produite lors de la g√©n√©ration de la r√©ponse."

    async def log_prompt_to_supabase(self, company_id: str, user_query: str, full_prompt: str, context: str, history: str) -> None:
        """
        Enregistre le prompt complet dans Supabase pour debugging et analyse.
        """
        try:
            from config import SUPABASE_URL, SUPABASE_KEY
            import requests
            
            log_entry = {
                "id": str(uuid.uuid4()),
                "company_id": company_id,
                "user_query": user_query,
                "full_prompt": full_prompt,
                "context_used": context,
                "conversation_history": history,
                "timestamp": datetime.utcnow().isoformat(),
                "prompt_length": len(full_prompt),
                "context_length": len(context),
                "history_length": len(history)
            }
            
            url = f"{SUPABASE_URL}/rest/v1/prompt_logs"
            headers = {
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}",
                "Content-Type": "application/json"
            }
            
            # Utiliser requests au lieu de httpx
            response = requests.post(url, json=log_entry, headers=headers, timeout=10)
            if response.status_code == 201:
                log3("[PROMPT LOG] Enregistr√©", f"ID: {log_entry['id'][:8]}...")
            else:
                log3("[PROMPT LOG] Erreur", f"HTTP {response.status_code}")
                    
        except Exception as e:
            log3("[PROMPT LOG] Exception", f"{type(e).__name__}: {str(e)}")

    def get_cache_key(self, message: str, company_id: str) -> str:
        """G√©n√®re une cl√© de cache unique"""
        key_data = f"{company_id}:{message}"
        return f"rag_simple:{hashlib.md5(key_data.encode()).hexdigest()}"

    async def process_message(self, message: str, company_id: str, user_id: str) -> str:
        """
        Traite un message utilisateur avec le RAG engine simplifi√©.
        Retourne la r√©ponse g√©n√©r√©e par le LLM.
        """
        start_time = datetime.now()
        
        # Validation des entr√©es
        if not message or not message.strip():
            return "Je n'ai pas re√ßu de message. Pouvez-vous reformuler votre question ?"
        
        if not company_id:
            return "Erreur: company_id manquant."
        
        # === 1. QUESTION INITIALE ===
        debug_log_hallucination("QUESTION INITIALE", f"Utilisateur: {message}")
        
        # Sauvegarder le message utilisateur
        await save_message(company_id, user_id, "user", message)
        
        # R√©cup√©rer l'historique des conversations
        conversation_history = await get_history(company_id, user_id)
        log3("[RAG SIMPLIFIED] Historique", f"Charg√©: {len(conversation_history)} caract√®res")
        
        # D√©sactiver le cache pour les tests
        cache_disabled = True
        if cache_disabled:
            log3("[RAG SIMPLIFIED] Cache d√©sactiv√©", "Mode test actif")
        
        try:
            # üéØ D√âTECTION D'INTENTIONS POUR ROUTAGE INTELLIGENT
            intentions = intention_router.detect_intentions(message)
            log3("[INTENTION_ROUTER]", {
                "primary_intention": intentions.primary,
                "all_intentions": list(intentions.intentions.keys()) if intentions.intentions else [],
                "confidence": intentions.confidence_score,
                "target_indexes": intention_router.get_target_indexes(company_id, intentions)
            })
            
            # üöÄ OPTIMISATION: √âviter la recherche si pas n√©cessaire
            skip_search = self.hallucination_guard.should_skip_search(message, company_id)
            
            if skip_search:
                log3("[RAG_OPTIMIZATION]", "üöÄ Recherche √©vit√©e - Requ√™te sociale/conversationnelle")
                supabase_results, supabase_context = [], ""
                meili_context = ""
            else:
                # 1. G√©n√©ration d'hypoth√®se HYDE avanc√©e
                try:
                    logger.info(f"[RAG_ADVANCED] üß† G√©n√©ration d'hypoth√®se HYDE")
                    
                    # D√©tection d'intention pour HYDE
                    if any(word in message.lower() for word in ['vendez', 'produits', 'services', 'catalogue', 'offre']):
                        intent_type = "product_inquiry"
                    elif any(word in message.lower() for word in ['prix', 'co√ªt', 'tarif', 'combien']):
                        intent_type = "pricing_info"
                    elif any(word in message.lower() for word in ['livraison', 'd√©lai', 'transport', 'exp√©dition']):
                        intent_type = "delivery_info"
                    elif any(word in message.lower() for word in ['contact', 't√©l√©phone', 'email', 'aide', 'support']):
                        intent_type = "support_contact"
                    elif any(word in message.lower() for word in ['entreprise', 'soci√©t√©', 'qui', '√† propos']):
                        intent_type = "company_info"
                    else:
                        intent_type = "general"
                    
                    # G√©n√©ration d'hypoth√®se selon l'intention
                    if intent_type == "product_inquiry":
                        hyde_hypothesis = f"""Informations sur nos produits et services :

PRODUITS ET SERVICES :
- Nous proposons une gamme de produits et services
- Catalogue de produits disponibles
- Services associ√©s et accompagnement
- Offres commerciales et promotions

INFORMATIONS COMMERCIALES :
- Modes de paiement disponibles
- Conditions de vente et livraison
- Support client et assistance
- Contact commercial et informations

POUR PLUS D'INFORMATIONS :
- Consultez notre catalogue
- Contactez notre √©quipe commerciale
- D√©couvrez nos offres sp√©ciales
- Informations sur nos produits"""
                    else:
                        hyde_hypothesis = f"""Document hypoth√©tique pour: {message}

Ce document contient des informations d√©taill√©es sur le sujet demand√©, incluant :
- Contexte et explications
- D√©tails techniques et pratiques
- Exemples et cas d'usage
- Ressources et r√©f√©rences

Pour toute question compl√©mentaire, n'h√©sitez pas √† nous contacter."""
                    
                    logger.info(f"[RAG_ADVANCED] üß† Hypoth√®se HYDE g√©n√©r√©e: {intent_type} ({len(hyde_hypothesis)} caract√®res)")
                    
                    # Recherche Supabase avec la m√©thode qui fonctionne
                    supabase_engine = SupabaseVectorSearch()
                    await supabase_engine.initialize()
                    
                    # G√©n√©ration embedding avec la m√™me m√©thode que le test
                    embedding = await supabase_engine.generate_embedding(hyde_hypothesis)
                    
                    supabase_results = await supabase_engine.search_vectors(
                        query_embedding=embedding,
                        company_id=company_id,
                        top_k=5,
                        min_score=0.3
                    )
                    
                    # Extraire le contexte des r√©sultats
                    context_parts = []
                    for result in supabase_results:
                        context_parts.append(result.content)
                    supabase_context = "\n\n".join(context_parts)
                    
                    logger.info(f"[RAG_ADVANCED] Contexte Supabase trouv√©: {len(supabase_context)} caract√®res")
                except Exception as e:
                    logger.warning(f"[RAG_ADVANCED] Erreur HYDE/Supabase: {e}")
                    supabase_context = ""
                
                # 2. Recherche MeiliSearch avec routage intelligent par intentions
                # FILTRAGE STOP WORDS: Appliquer le syst√®me de filtrage avant MeiliSearch
                filtered_query = filter_query_for_meilisearch(message)
                logger.info(f"[STOP_WORDS_FILTER] üîç FILTRAGE STOP WORDS APPLIQU√â (RAG_ADVANCED):")
                logger.info(f"[STOP_WORDS_FILTER] - Query originale: '{message}'")
                logger.info(f"[STOP_WORDS_FILTER] - Query filtr√©e: '{filtered_query}'")
                logger.info(f"[STOP_WORDS_FILTER] - R√©duction: {len(message)} ‚Üí {len(filtered_query)} caract√®res")
                
                meili_results_docs = await search_meili_keywords(filtered_query, company_id)
                print('DEBUG TYPE MEILI:', type(meili_results_docs), meili_results_docs)
                if not isinstance(meili_results_docs, list):
                    print('BUG: MeiliSearch returned non-list!', type(meili_results_docs), meili_results_docs)
                    meili_results_docs = []
                # Construction du meili_context √† partir des documents s√©lectionn√©s
                meili_context_parts = []
                for i, doc in enumerate(meili_results_docs):
                    content = doc.get('searchable_text') or doc.get('content_fr') or doc.get('content')
                    matched_combo = doc.get('matched_query_combo', ["N/A"])
                    index_source = doc.get('index_source', "N/A")
                    doc_id = doc.get('id', f"doc_{i+1}")

                    formatted_doc_content = content # Contenu par d√©faut
                    content_type_label = "INFORMATIONS G√âN√âRALES"

                    # D√©terminer le type de contenu et appliquer un formatage sp√©cifique si n√©cessaire
                    if "products" in index_source.lower():
                        content_type_label = "INFORMATIONS PRODUITS"
                    elif "delivery" in index_source.lower():
                        content_type_label = "INFORMATIONS DE LIVRAISON"
                        if content:
                            extracted_cost = _extract_delivery_cost(content)
                            extracted_time = _extract_delivery_time(content)
                            extracted_zones = _extract_zones_from_text(content)
                            formatted_doc_content = (
                                f"Zones concern√©es: {', '.join(extracted_zones) if extracted_zones else 'N/A'}\n"
                                f"Co√ªt de livraison: {extracted_cost} FCFA\n"
                                f"D√©lais de livraison: {extracted_time}\n\n"
                                f"{content}" # Inclure le contenu original aussi
                            )
                    elif "support_paiement" in index_source.lower():
                        content_type_label = "INFORMATIONS SUPPORT PAIEMENT"
                    elif "localisation" in index_source.lower():
                        content_type_label = "INFORMATIONS DE LOCALISATION"
                    elif "company_docs" in index_source.lower():
                        content_type_label = "INFORMATIONS D'ENTREPRISE"


                    if formatted_doc_content:
                        formatted_doc = (
                            f"=== DOCUMENT {i+1} (ID: {doc_id}) ===\n"
                            f"TYPE DE CONTENU : {content_type_label}\n"
                            f"COMBINAISON(S) DE RECHERCHE : {matched_combo}\n"
                            f"INDEX SOURCE : {index_source}\n\n"
                            f"CONTENU DU DOCUMENT :\n{formatted_doc_content}\n\n"
                            f"--- FIN DOCUMENT {i+1} ---"
                        )
                        meili_context_parts.append(formatted_doc)
                meili_context = "\n\n".join(meili_context_parts)
                
                # Logs supprim√©s pour clarifier diagnostic Supabase

                # Les meili_results sont maintenant une liste de documents complets, plus besoin de post-traitement complexe ici
                meili_results = meili_results_docs # Pour compatibilit√© avec les logs existants si n√©cessaire
            
            # Convertir en format r√©sultats standardis√©
            safe_meili_results = [{"content": meili_context, "type": "meilisearch"}] if meili_context else []
            
            # Le contexte Supabase est d√©j√† fourni par la fonction
            
            log3("[RAG SIMPLIFIED] R√©sultats extraits", {
                "supabase_results": len(supabase_results) if supabase_results else 0,
                "meili_results_count": len(safe_meili_results),
                "supabase_context_length": len(supabase_context) if supabase_context else 0
            })
            
            # 3. üß† FUSION INTELLIGENTE AVEC M√âMOIRE CONVERSATIONNELLE
            log3("[FUSION_INTELLIGENTE]", "D√©but fusion contextes avec m√©moire conversationnelle")
            
            # Fusionner tous les contextes intelligemment
            intelligent_context = fuse_all_contexts(
                supabase_results=supabase_results,
                meilisearch_results=safe_meili_results,
                user_message=message,
                user_id=user_id,
                company_id=company_id
            )
            
            # Enrichir avec suggestions intelligentes
            enhanced_context = enhance_context_with_suggestions(
                context=intelligent_context,
                user_message=message,
                user_id=user_id,
                company_id=company_id
            )
            
            # === 4. FUSION DES CONTEXTES ===
            debug_log_hallucination("CONTEXTE FUSIONN√â POUR LE LLM", enhanced_context)
            
            # 4. G√©n√©ration de la r√©ponse avec historique
            response = await self.generate_response(message, enhanced_context, company_id, conversation_history)
            
            # üõ°Ô∏è VALIDATION ANTI-HALLUCINATION INTELLIGENTE
            hallucination_check = await self.hallucination_guard.evaluate_response(
                user_query=message,
                ai_response=response,
                company_id=company_id,
                supabase_results=supabase_results,
                meili_results=safe_meili_results,
                supabase_context=supabase_context,
                meili_context=meili_context
            )
            
            log3("[HALLUCINATION_GUARD]", {
                "is_safe": hallucination_check.is_safe,
                "confidence": hallucination_check.confidence_score,
                "intention": hallucination_check.intention_detected,
                "search_required": hallucination_check.search_required,
                "documents_found": hallucination_check.documents_found,
                "judge_decision": hallucination_check.judge_decision,
                "processing_time_ms": hallucination_check.processing_time_ms
            })
            
            # Si la r√©ponse est rejet√©e, utiliser la r√©ponse sugg√©r√©e
            if not hallucination_check.is_safe and hallucination_check.suggested_response:
                log3("[HALLUCINATION_GUARD]", f"üö´ R√©ponse rejet√©e: {hallucination_check.reason}")
                response = hallucination_check.suggested_response
            
            # üß† Mettre √† jour la m√©moire conversationnelle
            update_conversation_context(
                user_message=message,
                assistant_response=response,
                user_id=user_id,
                company_id=company_id
            )
            
            # Sauvegarder la r√©ponse dans l'historique
            await save_message(
                company_id=company_id,
                user_id=user_id,
                role="assistant",
                content=response
            )
            
            # M√©triques finales
            total_time = (datetime.now() - start_time).total_seconds()
            log3("[RAG SIMPLIFIED] Traitement termin√©", f"Temps total: {total_time:.2f}s")
            
            return response
            
        except Exception as e:
            log3("[RAG SIMPLIFIED] Erreur traitement", f"{type(e).__name__}: {str(e)}")
            return "D√©sol√©, une erreur s'est produite. Veuillez r√©essayer."


# Instance globale
rag_engine = SimplifiedRAGEngine()

async def get_rag_response_advanced(
    message: str,
    user_id: str,
    company_id: str,
    conversation_id: str = None,
    use_hyde: bool = True,
    debug: bool = False
) -> Dict[str, Any]:
    """
    üöÄ RAG ENGINE AVANC√â 2024 - VERSION SANS ANTI-HALLUCINATION
    Syst√®me simplifi√© sans validation anti-hallucination pour un fonctionnement direct
    """
    start_time = time.time()
    
    # üìä D√âMARRAGE DU LOGGING D√âTAILL√â
    request_data = {
        "user_id": user_id,
        "company_id": company_id,
        "message": message,
        "conversation_id": conversation_id,
        "use_hyde": use_hyde,
        "debug": debug
    }
    request_id = start_detailed_logging(request_data)
    
    # Log de d√©marrage
    logger.info(f"[DETAILED_LOGGING] üöÄ Logging d√©taill√© activ√© - Request ID: {request_id}")
    
    try:
        # üö´ D√âSACTIVATION COMPL√àTE DU SYST√àME ANTI-HALLUCINATION
        # Plus de classification d'intention, validation, ou scoring de confiance
        
        # 1. Recherche de documents (TOUJOURS activ√©e)
        logger.info(f"[RAG_ADVANCED] üîç √âtape 1: Recherche de documents")
        
        # Logging d√©taill√© de la recherche
        log_document_search("SUPABASE_SEMANTIC", {"query": message, "company_id": company_id}, [], "")
        
        supabase_results = []
        meili_results = []
        supabase_context = ""
        meili_context = ""
        
        # PRIORIT√â 1: Recherche MeiliSearch multi-index (PRIORIT√â ABSOLUE)
        try:
            logger.info(f"[RAG_ADVANCED] üîç Recherche MeiliSearch multi-index (PRIORIT√â)")
            
            # 1. Filtrage des stop words (Module 1)
            filtered_query = filter_query_for_meilisearch(message)
            logger.info(f"[STOP_WORDS_FILTER] Query filtr√©e: \'{filtered_query}\'")

            # 2. G√©n√©ration des combinaisons de mots (Module 2)
            query_combinations = generate_query_combinations(filtered_query)
            logger.info(f"[QUERY_COMBINATOR] Combinations g√©n√©r√©es: {query_combinations}")

            # 3. Orchestration de la recherche MeiliSearch (Module 3)
            meili_results_docs = await search_meili_keywords(filtered_query, company_id)
            print('DEBUG TYPE MEILI:', type(meili_results_docs), meili_results_docs)
            if not isinstance(meili_results_docs, list):
                print('BUG: MeiliSearch returned non-list!', type(meili_results_docs), meili_results_docs)
                meili_results_docs = []
            # Construction du meili_context √† partir des documents s√©lectionn√©s
            meili_context_parts = []
            for i, doc in enumerate(meili_results_docs):
                content = doc.get('searchable_text') or doc.get('content_fr') or doc.get('content')
                matched_combo = doc.get('matched_query_combo', ["N/A"])
                index_source = doc.get('index_source', "N/A")
                doc_id = doc.get('id', f"doc_{i+1}")

                formatted_doc_content = content # Contenu par d√©faut
                content_type_label = "INFORMATIONS G√âN√âRALES"

                # D√©terminer le type de contenu et appliquer un formatage sp√©cifique si n√©cessaire
                if "products" in index_source.lower():
                    content_type_label = "INFORMATIONS PRODUITS"
                elif "delivery" in index_source.lower():
                    content_type_label = "INFORMATIONS DE LIVRAISON"
                    if content:
                        extracted_cost = _extract_delivery_cost(content)
                        extracted_time = _extract_delivery_time(content)
                        extracted_zones = _extract_zones_from_text(content)
                        formatted_doc_content = (
                            f"Zones concern√©es: {', '.join(extracted_zones) if extracted_zones else 'N/A'}\n"
                            f"Co√ªt de livraison: {extracted_cost} FCFA\n"
                            f"D√©lais de livraison: {extracted_time}\n\n"
                            f"{content}" # Inclure le contenu original aussi
                        )
                elif "support_paiement" in index_source.lower():
                    content_type_label = "INFORMATIONS SUPPORT PAIEMENT"
                elif "localisation" in index_source.lower():
                    content_type_label = "INFORMATIONS DE LOCALISATION"
                elif "company_docs" in index_source.lower():
                    content_type_label = "INFORMATIONS D'ENTREPRISE"


                if formatted_doc_content:
                    formatted_doc = (
                        f"=== DOCUMENT {i+1} (ID: {doc_id}) ===\n"
                        f"TYPE DE CONTENU : {content_type_label}\n"
                        f"COMBINAISON(S) DE RECHERCHE : {matched_combo}\n"
                        f"INDEX SOURCE : {index_source}\n\n"
                        f"CONTENU DU DOCUMENT :\n{formatted_doc_content}\n\n"
                        f"--- FIN DOCUMENT {i+1} ---"
                    )
                    meili_context_parts.append(formatted_doc)
            meili_context = "\n\n".join(meili_context_parts)
            
            logger.info(f"[MEILISEARCH_CONTEXT] Contexte MeiliSearch final: {len(meili_context)} caract√®res")
            logger.info(f"[MEILISEARCH_CONTEXT] Nombre de documents dans le contexte: {len(meili_results_docs)}")

            # Les meili_results sont maintenant une liste de documents complets, plus besoin de post-traitement complexe ici
            meili_results = meili_results_docs # Pour compatibilit√© avec les logs existants si n√©cessaire

        except Exception as e:
            logger.error(f"[RAG_ADVANCED] Erreur MeiliSearch (Orchestrateur): {e}")
            meili_context = ""
            meili_results = [] # S'assurer que c'est vide en cas d'erreur
        
        # 2. G√©n√©ration de r√©ponse LLM (SANS VALIDATION)
        logger.info(f"[RAG_ADVANCED] ü§ñ √âtape 2: G√©n√©ration LLM")
        
        try:
            from .llm_client import GroqLLMClient
            llm_client = GroqLLMClient()
            
            # Construction du prompt contextuel avec fusion intelligente
            context_parts = []
            
            # Contexte Supabase (s√©mantique)
            if supabase_context and len(supabase_context.strip()) > 0:
                context_parts.append(f"=== INFORMATIONS S√âMANTIQUES ===\n{supabase_context}")
                logger.info(f"[CONTEXT_FUSION] - Contexte Supabase ajout√©: {len(supabase_context)} caract√®res")
            else:
                logger.warning(f"[CONTEXT_FUSION] - Contexte Supabase vide ou manquant")
            
            # Contexte MeiliSearch (textuel)
            if meili_context and len(meili_context.strip()) > 0:
                context_parts.append(f"=== INFORMATIONS TEXTUELLES ===\n{meili_context}")
                logger.info(f"[CONTEXT_FUSION] - Contexte MeiliSearch ajout√©: {len(meili_context)} caract√®res")
            else:
                logger.warning(f"[CONTEXT_FUSION] - Contexte MeiliSearch vide ou manquant")
            
            # Contexte final
            context = "\n\n".join(context_parts) if context_parts else "Aucun contexte disponible"
            
            # Logs de la fusion
            logger.info(f"[CONTEXT_FUSION] üìã Fusion des contextes:")
            logger.info(f"[CONTEXT_FUSION] - Nombre de sources: {len(context_parts)}")
            logger.info(f"[CONTEXT_FUSION] - Longueur totale: {len(context)} caract√®res")
            logger.info(f"[CONTEXT_FUSION] - Sources utilis√©es:")
            if supabase_context and len(supabase_context.strip()) > 0:
                logger.info(f"[CONTEXT_FUSION]   ‚úì Supabase: {len(supabase_context)} caract√®res")
            else:
                logger.info(f"[CONTEXT_FUSION]   ‚úó Supabase: vide")
            if meili_context and len(meili_context.strip()) > 0:
                logger.info(f"[CONTEXT_FUSION]   ‚úì MeiliSearch: {len(meili_context)} caract√®res")
            else:
                logger.info(f"[CONTEXT_FUSION]   ‚úó MeiliSearch: vide")
            
            # Logs d√©taill√©s du contexte final - VERSION TRONQU√âE
            logger.info(f"[FINAL_CONTEXT] üìã CONTEXTE FINAL ASSEMBL√â:")
            logger.info(f"[FINAL_CONTEXT] ==========================================")
            logger.info(f"[FINAL_CONTEXT] - Contexte Supabase: {len(supabase_context)} caract√®res")
            logger.info(f"[FINAL_CONTEXT] - Contexte MeiliSearch: {len(meili_context)} caract√®res")
            logger.info(f"[FINAL_CONTEXT] - Contexte total: {len(context)} caract√®res")
            logger.info(f"[FINAL_CONTEXT] - Aper√ßu du contenu complet: {context}")
            logger.info(f"[FINAL_CONTEXT] ==========================================")
            
            # Logging d√©taill√© de la g√©n√©ration LLM
            log_llm_generation(context, "", "llama-3.3-70b-versatile", 0.7, 300)
            
            # Prompt simple et direct
            # system_prompt = f"""Tu es Gamma, l'assistant client de Rue du Gros.
            # R√©ponds de mani√®re pr√©cise et professionnelle en te basant sur le contexte fourni.
            # 
            # Contexte: {context}
            # Question: {message}
            # 
            # R√®gles importantes:
            # - R√©ponds de mani√®re naturelle et professionnelle
            # - Utilise les informations du contexte si disponibles
            # - Si tu n'as pas d'informations, dis-le clairement
            # - Sois pr√©cis et factuel"""
            
            # R√©cup√©rer le prompt syst√®me personnalis√© dynamiquement
            from core.prompt_manager import PromptManager
            prompt_manager = PromptManager(get_supabase_client())
            prompt_data = await prompt_manager.get_active_prompt(company_id)
            dynamic_system_prompt = "Tu es Gamma, l'assistant client de Rue du Gros. R√©ponds de mani√®re pr√©cise et professionnelle en te basant sur le contexte fourni. Sois concis."
            
            if prompt_data:
                dynamic_system_prompt = prompt_data.get('prompt_template', dynamic_system_prompt)
                logger.info(f"[RAG_ADVANCED] Prompt personnalis√© charg√© (version: {prompt_data.get('version', 'N/A')})")

            # Construction du prompt complet avec contexte et message
            system_prompt = f"""{dynamic_system_prompt}

            Contexte:
            {context}

            Question: {message}
            """
            
            # Ajouter les informations de m√©moire au prompt principal
            memory_data = None # Initialiser memory_data
            if memory_data and memory_data.get("conversation_summary"):
                summary = memory_data["conversation_summary"]
                completeness = summary.get("completeness_percentage", 0)
                
                memory_context = f"\n\nüß† CONTEXTE CONVERSATIONNEL ACTUEL:\n"
                memory_context += f"üìä Compl√©tude: {completeness:.1f}%\n"
                
                if summary.get("customer_info"):
                    customer = summary["customer_info"]
                    if customer.get("name"):
                        memory_context += f"üë§ Client: {customer['name']}\n"
                    if customer.get("phone"):
                        memory_context += f"üìû T√©l√©phone: {customer['phone']}\n"
                
                if summary.get("delivery_info"):
                    delivery = summary["delivery_info"]
                    if delivery.get("address"):
                        memory_context += f"üìç Adresse: {delivery['address']}\n"
                    if delivery.get("zone"):
                        memory_context += f"üåç Zone: {delivery['zone']}\n"
                
                if summary.get("order_info"):
                    order = summary["order_info"]
                    if order.get("product_type"):
                        memory_context += f"üì¶ Produit: {order['product_type']}\n"
                    if order.get("quantity"):
                        memory_context += f"üî¢ Quantit√©: {order['quantity']}\n"
                
                if summary.get("missing_information"):
                    missing = summary["missing_information"]
                    memory_context += f"‚ùì Informations manquantes: {', '.join(missing)}\n"
                
                system_prompt += memory_context
                logger.info(f"[MEMORY_SYSTEM] ‚úÖ Contexte de m√©moire ajout√© au prompt")
            
            # üßÆ CALCUL DE PRIX AUTOMATIQUE
            price_info = None
            try:
                # D√©tecter si la requ√™te concerne un calcul de prix
                price_keywords = [
                    "prix", "co√ªt", "combien", "tarif", "commander", "acheter", "total", "livraison",
                    "voulais", "prendre", "quantit√©", "montant", "facture", "payer", "panier",
                    "finaliser", "passer commande", "d√©tails commande", "quel est le prix",
                    "facturation", "co√ªter", "avoir", "proposer", "vendre", "souhaite", "d√©sire",
                    "achat", "articles", "produits", "somme", "calculer", "devis"
                ]
                if any(keyword in message.lower() for keyword in price_keywords):
                    logger.info(f"[PRICE_CALCULATOR] üßÆ Calcul de prix d√©tect√©")
                    
                    # Utiliser les informations de m√©moire pour am√©liorer le calcul
                    delivery_zone = None
                    if memory_data and memory_data.get("conversation_summary", {}).get("delivery_info", {}).get("zone"):
                        delivery_zone = memory_data["conversation_summary"]["delivery_info"]["zone"]
                    elif any(zone in message.lower() for zone in ["cocody", "yopougon", "plateau", "adjam√©", "abobo", "marcory", "koumassi", "treichville", "angr√©", "riviera", "bingerville", "port-bou√´t", "att√©coub√©"]):
                        delivery_zone = message
                    
                    # Calculer le prix
                    price_info = calculate_order_price(message, company_id, delivery_zone)
                    
                    if price_info and price_info.get("products"):
                        logger.info(f"[PRICE_CALCULATOR] ‚úÖ Prix calcul√©: {price_info['total']:,} FCFA")
                        
                        # Ajouter les informations de prix au contexte
                        price_context = f"\n\nüßÆ INFORMATIONS DE PRIX CALCUL√âES AUTOMATIQUEMENT:\n{price_info['breakdown']}\n"
                        system_prompt += price_context
                        
                        logger.info(f"[PRICE_CALCULATOR] üìù Contexte de prix ajout√© au prompt")
                    else:
                        logger.info(f"[PRICE_CALCULATOR] ‚ö†Ô∏è Aucun produit identifi√© pour le calcul de prix")
                        
            except Exception as e:
                logger.error(f"[PRICE_CALCULATOR] ‚ùå Erreur calcul prix: {str(e)}")
                price_info = None
            
            ai_response = await llm_client.complete(
                prompt=system_prompt,
                temperature=0.7,
                max_tokens=300
            )
            
            # Logging d√©taill√© de la r√©ponse LLM
            log_llm_generation(system_prompt, ai_response, "llama-3.3-70b-versatile", 0.7, 300)
            
            # Logs d√©taill√©s de la r√©ponse LLM - VERSION TRONQU√âE
            logger.info(f"[LLM_RESPONSE] ü§ñ R√âPONSE LLM G√âN√âR√âE:")
            logger.info(f"[LLM_RESPONSE] ==========================================")
            logger.info(f"[LLM_RESPONSE] - Longueur: {len(ai_response)} caract√®res")
            logger.info(f"[LLM_RESPONSE] - Nombre de lignes: {len(ai_response.splitlines())}")
            logger.info(f"[LLM_RESPONSE] - Nombre de mots: {len(ai_response.split())}")
            logger.info(f"[LLM_RESPONSE] - Aper√ßu du contenu complet: {ai_response}")
            logger.info(f"[LLM_RESPONSE] - Analyse de la r√©ponse: ...") # Laisser l'analyse, elle est concise
            logger.info(f"[LLM_RESPONSE] ==========================================")
            
            # Logs d√©taill√©s du prompt final
            logger.info(f"[LLM_PROMPT] üìù PROMPT COMPLET ENVOY√â AU LLM:")
            logger.info(f"[LLM_PROMPT] ==========================================")
            logger.info(f"[LLM_PROMPT] - Longueur: {len(system_prompt)} caract√®res")
            logger.info(f"[LLM_PROMPT] - Nombre de lignes: {len(system_prompt.splitlines())}")
            logger.info(f"[LLM_PROMPT] - Aper√ßu du contenu complet: {system_prompt}")
            logger.info(f"[LLM_PROMPT] - Param√®tres LLM: ...") # Laisser les param√®tres, ils sont concis
            logger.info(f"[LLM_PROMPT] ==========================================")
            
        except Exception as e:
            logger.error(f"[RAG_ADVANCED] Erreur LLM: {e}")
            ai_response = "Je rencontre une difficult√© technique. Pouvez-vous r√©essayer ?"

        # 3. Sauvegarde de la conversation
        logger.info(f"[RAG_ADVANCED] üíæ √âtape 3: Sauvegarde conversation")
        try:
            await save_message(
                company_id=company_id,
                user_id=user_id,
                role="user",
                content=message
            )
            await save_message(
                company_id=company_id,
                user_id=user_id,
                role="assistant",
                content=ai_response
            )
            logger.info(f"[RAG_ADVANCED] ‚úÖ Conversation sauvegard√©e")
        except Exception as e:
            logger.warning(f"[RAG_ADVANCED] Erreur sauvegarde: {e}")
        
        # 4. Construction de la r√©ponse finale (SANS VALIDATION)
        logger.info(f"[RAG_ADVANCED] üì§ √âtape 4: Construction r√©ponse finale")
        
        # Ajouter les informations de prix √† la r√©ponse si calcul√©es
        final_response = ai_response
        recap_info = None
        
        # üß† GESTION INTELLIGENTE DE LA CONFIRMATION
        if memory_data and memory_data.get("should_confirm"):
            logger.info(f"[MEMORY_SYSTEM] üîî Demande de confirmation d√©tect√©e")
            confirmation_prompt = memory_data.get("confirmation_prompt", "")
            if confirmation_prompt:
                final_response = confirmation_prompt
                logger.info(f"[MEMORY_SYSTEM] ‚úÖ Prompt de confirmation g√©n√©r√©")
        
        elif price_info and price_info.get("products"):
            logger.info(f"[PRICE_CALCULATOR] üìä Ajout des informations de prix √† la r√©ponse")
            
            # D√©tecter si c'est une demande de r√©capitulatif
            recap_keywords = ["r√©capitulatif", "r√©cap", "r√©sum√©", "confirmer", "commande", "total", "prix total"]
            is_recap_request = any(keyword in message.lower() for keyword in recap_keywords)
            
            if is_recap_request:
                logger.info(f"[RECAP_TEMPLATE] üìã G√©n√©ration de r√©capitulatif structur√©")
                
                # Utiliser les informations de m√©moire pour le r√©capitulatif
                customer_info = {
                    "name": "Client",
                    "phone": "Non fourni",
                    "address": "√Ä confirmer"
                }
                
                delivery_info = {
                    "zone": "Zone √† confirmer",
                    "address": "Adresse √† confirmer",
                    "delivery_time": "√Ä confirmer"
                }
                
                # Enrichir avec les donn√©es de m√©moire
                if memory_data and memory_data.get("conversation_summary"):
                    summary = memory_data["conversation_summary"]
                    
                    if summary.get("customer_info"):
                        customer_info.update(summary["customer_info"])
                    
                    if summary.get("delivery_info"):
                        delivery_info.update(summary["delivery_info"])
                
                if price_info.get("delivery_info"):
                    delivery_info.update({
                        "zone": price_info["delivery_info"].zone,
                        "delivery_time": price_info["delivery_info"].delivery_time
                    })
                
                # G√©n√©rer le r√©capitulatif complet
                recap_info = generate_order_summary(
                    customer_info=customer_info,
                    products=price_info["products"],
                    delivery_info=delivery_info,
                    price_info=price_info,
                    company_id=company_id,
                    summary_type="full"
                )
                
                final_response = recap_info
                logger.info(f"[RECAP_TEMPLATE] ‚úÖ R√©capitulatif g√©n√©r√© avec succ√®s")
            else:
                # Ajouter juste le d√©tail des prix
                price_summary = f"\n\n{price_info['breakdown']}"
                final_response += price_summary
        
        response_data = {
            'response': final_response,
            'intent': 'factual_information',  # Toujours factual_information
            'confidence': 1.0,  # Score fixe √† 1.0
            'confidence_level': 'high',  # Niveau fixe √† high
            'validation_safe': True,  # Toujours s√ªr
            'requires_documents': True,  # Toujours True
            'documents_found': bool(supabase_context or meili_context),
            'processing_time_ms': (time.time() - start_time) * 1000,
            'price_calculated': price_info is not None,
            'price_info': price_info if price_info else None,
            'memory_data': memory_data if memory_data else None,
            'conversation_completeness': memory_data.get("conversation_summary", {}).get("completeness_percentage", 0) if memory_data else 0,
            'missing_information': memory_data.get("missing_information", []) if memory_data else [],
            'should_confirm': memory_data.get("should_confirm", False) if memory_data else False,
            'debug_info': {
                'validation_mode': 'disabled',  # D√©sactiv√©
                'validation_result': 'bypassed',  # Contourn√©
                'fallback_used': False,  # Jamais de fallback
                'specific_information_detected': [],  # Vide
                'source_verification': {},  # Vide
                'suggested_action': 'none'  # Aucune action
            }
        }
        
        # üíæ SAUVEGARDE AUTOMATIQUE DES LOGS
        try:
            from .detailed_logging_system import save_logs_to_file
            log_file = save_logs_to_file()
            if log_file:
                logger.info(f"[RAG_ADVANCED] üìÅ Logs sauvegard√©s: {log_file}")
        except Exception as e:
            logger.warning(f"[RAG_ADVANCED] Erreur sauvegarde logs: {e}")
        
        # Logs de r√©sum√© final - VERSION TRONQU√âE
        logger.info(f"[RAG_SUMMARY] üìä R√âSUM√â FINAL DU SYST√àME DE RECHERCHE:")
        logger.info(f"[RAG_SUMMARY] ==========================================")
        logger.info(f"[RAG_SUMMARY] - Requ√™te re√ßue: '{message}'")
        logger.info(f"[RAG_SUMMARY] - Longueur requ√™te: {len(message)} caract√®res")
        logger.info(f"[RAG_SUMMARY] - Documents Supabase trouv√©s: {len(supabase_results)}")
        logger.info(f"[RAG_SUMMARY] - Contexte Supabase: {len(supabase_context)} caract√®res")
        logger.info(f"[RAG_SUMMARY] - Documents MeiliSearch trouv√©s: {len(meili_results_docs) if 'meili_results_docs' in locals() else 'N/A'}")
        logger.info(f"[RAG_SUMMARY] - Contexte MeiliSearch: {len(meili_context)} caract√®res")
        logger.info(f"[RAG_SUMMARY] - Contexte total: {len(context)} caract√®res")
        logger.info(f"[RAG_SUMMARY] - R√©ponse LLM: {len(ai_response)} caract√®res")
        logger.info(f"[RAG_SUMMARY] - Temps de traitement: {(time.time() - start_time) * 1000:.2f} ms")
        logger.info(f"[RAG_SUMMARY] - Statut: Succ√®s")
        logger.info(f"[RAG_SUMMARY] ==========================================")
        
        logger.info(f"[RAG_ADVANCED] ‚úÖ R√©ponse g√©n√©r√©e: 1.000 (high)")
        
        # Logging d√©taill√© de la r√©ponse finale
        log_response_sent(ai_response, {
            "intent": "factual_information",
            "confidence": 1.0,
            "documents_found": len(supabase_results) > 0,
            "processing_time_ms": (time.time() - start_time) * 1000
        })
        
        return response_data
        
    except Exception as e:
        logger.error(f"[RAG_ADVANCED] ‚ùå Erreur critique: {e}")
        log_error("RAG_ENGINE", "CRITICAL_ERROR", e, {
            "message": message,
            "user_id": user_id,
            "company_id": company_id
        })
        
        # Fallback d'urgence
        logger.warning(f"[RAG_ADVANCED] üö® Activation du fallback d'urgence")
        
        emergency_response = {
            'response': "Je rencontre une difficult√© technique. Pouvez-vous r√©essayer ou contacter notre service client ?",
            'intent': 'factual_information',
            'confidence': 1.0,
            'confidence_level': 'high',
            'validation_safe': True,
            'requires_documents': True,
            'documents_found': False,
            'processing_time_ms': (time.time() - start_time) * 1000,
            'debug_info': {
                'error': str(e),
                'emergency_fallback': True
            }
        }
        
        logger.error(f"[RAG_ADVANCED] Fallback d'urgence activ√©: {str(e)}")
        
        return emergency_response

async def get_rag_response(message: str, company_id: str, user_id: str) -> str:
    """
    üéØ FONCTION PRINCIPALE D'EXPORT (COMPATIBILIT√â)
    Interface simple pour l'int√©gration avec app.py
    """
    try:
        # Utilisation du nouveau syst√®me avanc√©
        result = await get_rag_response_advanced(
            message=message,
            user_id=user_id,
            company_id=company_id
        )
        
        # V√©rifier que result est un dictionnaire et contient 'response'
        if isinstance(result, dict) and 'response' in result:
            return result['response']
        elif isinstance(result, str):
            return result
        else:
            logger.error(f"[RAG_RESPONSE] Format de r√©ponse inattendu: {type(result)}")
            return "Erreur de format de r√©ponse"
            
    except Exception as e:
        logger.error(f"[RAG_RESPONSE] Erreur dans get_rag_response: {str(e)}")
        return "Erreur lors de la g√©n√©ration de la r√©ponse"
