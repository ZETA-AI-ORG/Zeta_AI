#!/usr/bin/env python3
"""
üîß FIX CACHE S√âMANTIQUE - AM√âLIORATION ET SIMPLIFICATION
Corrige les probl√®mes du cache s√©mantique pour le rendre utilisable
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

def analyze_cache_issues():
    """Analyse les probl√®mes du cache s√©mantique actuel"""
    
    print("üîç ANALYSE DU CACHE S√âMANTIQUE ACTUEL")
    print("="*60)
    
    try:
        from core.semantic_intent_cache import SemanticIntentCache, IntentSignature
        
        # Test d'initialisation
        print("\n1Ô∏è‚É£ Test d'initialisation...")
        cache = SemanticIntentCache(
            similarity_threshold=0.4  # ‚ö†Ô∏è PROBL√àME: Trop bas
        )
        print(f"   ‚úÖ Cache initialis√©")
        print(f"   ‚ö†Ô∏è Seuil actuel: {cache.similarity_threshold} (TROP BAS)")
        print(f"   üí° Recommand√©: 0.85-0.92")
        
        # Test stats
        print("\n2Ô∏è‚É£ Statistiques cache...")
        stats = cache.get_stats()
        print(f"   üìä Stats: {stats}")
        
        # Test de performance
        print("\n3Ô∏è‚É£ Test de performance...")
        import time
        
        # Simuler une requ√™te
        intent_sig = IntentSignature(
            primary_intent="PRIX",
            secondary_intents=[],
            entities={"produit": "couches_culottes", "quantite": "6"},
            context_hash="test123",
            confidence_score=0.9
        )
        
        # Test stockage
        start = time.time()
        import asyncio
        asyncio.run(cache.store_response(
            query="Combien 6 paquets couches culottes ?",
            response="6 paquets de couches culottes co√ªtent 25.000 FCFA",
            intent_signature=intent_sig,
            conversation_history=""
        ))
        store_time = (time.time() - start) * 1000
        print(f"   ‚è±Ô∏è Temps stockage: {store_time:.0f}ms")
        
        if store_time > 100:
            print(f"   ‚ö†Ô∏è PROBL√àME: Stockage trop lent (>{100}ms)")
        
        # Test r√©cup√©ration
        start = time.time()
        result = asyncio.run(cache.get_cached_response(
            query="Prix 6 paquets couches culottes ?",  # Formulation diff√©rente
            intent_signature=intent_sig,
            conversation_history=""
        ))
        retrieve_time = (time.time() - start) * 1000
        print(f"   ‚è±Ô∏è Temps r√©cup√©ration: {retrieve_time:.0f}ms")
        
        if retrieve_time > 200:
            print(f"   ‚ö†Ô∏è PROBL√àME: R√©cup√©ration trop lente (>{200}ms)")
        
        if result:
            response, confidence = result
            print(f"   ‚úÖ Trouv√©: {response[:50]}...")
            print(f"   üéØ Confiance: {confidence:.2f}")
        else:
            print(f"   ‚ùå Pas trouv√©")
        
        print("\n" + "="*60)
        print("üìä PROBL√àMES IDENTIFI√âS:")
        print("1. ‚ö†Ô∏è Seuil similarit√© trop bas (0.4 ‚Üí 0.88)")
        print("2. ‚ö†Ô∏è Stockage potentiellement lent si >100ms")
        print("3. ‚ö†Ô∏è R√©cup√©ration potentiellement lente si >200ms")
        print("4. ‚ö†Ô∏è Pas de nettoyage auto des entr√©es obsol√®tes")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_optimized_cache():
    """Cr√©e une version optimis√©e du cache s√©mantique"""
    
    print("\nüöÄ CR√âATION CACHE S√âMANTIQUE OPTIMIS√â")
    print("="*60)
    
    code = '''#!/usr/bin/env python3
"""
üöÄ CACHE S√âMANTIQUE OPTIMIS√â V2
Version simplifi√©e et performante du cache s√©mantique
"""

import asyncio
import hashlib
import json
import time
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer, util
import threading

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

@dataclass
class SimpleCacheEntry:
    """Entr√©e de cache simplifi√©e"""
    query: str
    response: str
    query_embedding: List[float]
    timestamp: float
    hit_count: int
    ttl_seconds: int

class OptimizedSemanticCache:
    """
    üéØ Cache S√©mantique Optimis√©
    
    AM√âLIORATIONS:
    - Seuil plus √©lev√© (0.88 au lieu de 0.4)
    - Stockage simplifi√© (pas de two-stage retrieval)
    - Nettoyage automatique des entr√©es obsol√®tes
    - Performance am√©lior√©e (<100ms)
    """
    
    def __init__(self, 
                 similarity_threshold: float = 0.88,  # ‚úÖ AUGMENT√â
                 max_cache_size: int = 1000,          # ‚úÖ R√âDUIT
                 default_ttl: int = 1800):            # ‚úÖ 30min au lieu de 1h
        
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        self.default_ttl = default_ttl
        
        # Stockage en m√©moire seulement (Redis optionnel)
        self.memory_cache: Dict[str, SimpleCacheEntry] = {}
        self.redis_client = None
        
        # Mod√®le d'embeddings l√©ger
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Stats
        self.stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_retrieve_time_ms": 0
        }
        
        self.lock = threading.RLock()
        self._init_redis()
    
    def _init_redis(self):
        """Initialise Redis (optionnel)"""
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(
                    host='localhost',
                    port=6379,
                    db=3,  # DB 3 pour cache optimis√©
                    decode_responses=False
                )
                self.redis_client.ping()
                print("‚úÖ Redis connect√© (DB 3)")
            except:
                self.redis_client = None
    
    def _create_embedding(self, text: str) -> List[float]:
        """Cr√©e un embedding"""
        return self.model.encode(text, convert_to_tensor=False).tolist()
    
    def _compute_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """Calcule la similarit√© cosinus"""
        return util.cos_sim(emb1, emb2).item()
    
    async def get_cached_response(self, query: str) -> Optional[Tuple[str, float]]:
        """
        R√©cup√®re une r√©ponse du cache
        
        Returns:
            (response, confidence) si trouv√©, None sinon
        """
        start_time = time.time()
        
        with self.lock:
            self.stats["total_queries"] += 1
            
            # Cr√©er embedding query
            query_embedding = self._create_embedding(query)
            
            # Chercher dans le cache
            best_match = None
            best_similarity = 0.0
            
            for cache_key, entry in self.memory_cache.items():
                # V√©rifier TTL
                if time.time() - entry.timestamp > entry.ttl_seconds:
                    continue  # Ignorer les entr√©es expir√©es
                
                # Calculer similarit√©
                similarity = self._compute_similarity(query_embedding, entry.query_embedding)
                
                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = entry
            
            # Mettre √† jour stats
            retrieve_time_ms = (time.time() - start_time) * 1000
            self.stats["avg_retrieve_time_ms"] = (
                (self.stats["avg_retrieve_time_ms"] * (self.stats["total_queries"] - 1) + retrieve_time_ms)
                / self.stats["total_queries"]
            )
            
            if best_match:
                self.stats["cache_hits"] += 1
                best_match.hit_count += 1
                
                print(f"‚úÖ Cache HIT (similarity: {best_similarity:.3f}, time: {retrieve_time_ms:.0f}ms)")
                return (best_match.response, best_similarity)
            else:
                self.stats["cache_misses"] += 1
                print(f"‚ùå Cache MISS (time: {retrieve_time_ms:.0f}ms)")
                return None
    
    async def store_response(self, 
                            query: str, 
                            response: str,
                            ttl: Optional[int] = None):
        """Stocke une r√©ponse dans le cache"""
        
        with self.lock:
            # Cr√©er embedding
            query_embedding = self._create_embedding(query)
            
            # Cr√©er cl√© unique
            cache_key = hashlib.md5(query.encode()).hexdigest()
            
            # Cr√©er entr√©e
            entry = SimpleCacheEntry(
                query=query,
                response=response,
                query_embedding=query_embedding,
                timestamp=time.time(),
                hit_count=0,
                ttl_seconds=ttl or self.default_ttl
            )
            
            # Stocker
            self.memory_cache[cache_key] = entry
            
            # Nettoyer si trop d'entr√©es
            if len(self.memory_cache) > self.max_cache_size:
                self._cleanup_old_entries()
            
            print(f"üíæ Cache STORE (total: {len(self.memory_cache)} entr√©es)")
    
    def _cleanup_old_entries(self):
        """Nettoie les entr√©es obsol√®tes"""
        now = time.time()
        
        # Supprimer entr√©es expir√©es
        expired_keys = [
            key for key, entry in self.memory_cache.items()
            if now - entry.timestamp > entry.ttl_seconds
        ]
        
        for key in expired_keys:
            del self.memory_cache[key]
        
        # Si encore trop, supprimer les moins utilis√©es
        if len(self.memory_cache) > self.max_cache_size:
            sorted_entries = sorted(
                self.memory_cache.items(),
                key=lambda x: x[1].hit_count
            )
            
            # Garder seulement les plus utilis√©es
            to_keep = sorted_entries[-self.max_cache_size:]
            self.memory_cache = dict(to_keep)
        
        print(f"üßπ Nettoyage: {len(expired_keys)} expir√©es, {len(self.memory_cache)} conserv√©es")
    
    def clear_cache(self):
        """Vide compl√®tement le cache"""
        with self.lock:
            self.memory_cache.clear()
            print("üóëÔ∏è Cache vid√©")
    
    def get_stats(self) -> Dict:
        """Retourne les statistiques"""
        with self.lock:
            hit_rate = (
                (self.stats["cache_hits"] / self.stats["total_queries"] * 100)
                if self.stats["total_queries"] > 0 else 0
            )
            
            return {
                **self.stats,
                "hit_rate_percent": hit_rate,
                "cache_size": len(self.memory_cache)
            }

# Singleton global
_global_cache = None

def get_optimized_cache() -> OptimizedSemanticCache:
    """Retourne l'instance globale du cache"""
    global _global_cache
    if _global_cache is None:
        _global_cache = OptimizedSemanticCache()
    return _global_cache
'''
    
    # Sauvegarder
    output_path = Path(__file__).parent / "core" / "optimized_semantic_cache.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(code)
    
    print(f"‚úÖ Cache optimis√© cr√©√©: {output_path}")
    print("\nüìã AM√âLIORATIONS:")
    print("   1. ‚úÖ Seuil: 0.4 ‚Üí 0.88 (plus strict)")
    print("   2. ‚úÖ TTL: 3600s ‚Üí 1800s (30min)")
    print("   3. ‚úÖ Max size: 10000 ‚Üí 1000 (plus l√©ger)")
    print("   4. ‚úÖ Nettoyage auto des entr√©es obsol√®tes")
    print("   5. ‚úÖ Pas de two-stage retrieval (plus simple)")
    print("   6. ‚úÖ Performance: <100ms vis√©")

def test_optimized_cache():
    """Teste le cache optimis√©"""
    
    print("\nüß™ TEST DU CACHE OPTIMIS√â")
    print("="*60)
    
    try:
        from core.optimized_semantic_cache import get_optimized_cache
        import asyncio
        
        cache = get_optimized_cache()
        
        # Test 1: Stockage
        print("\n1Ô∏è‚É£ Test stockage...")
        asyncio.run(cache.store_response(
            query="Combien co√ªte 6 paquets de couches culottes ?",
            response="6 paquets de couches culottes co√ªtent 25.000 FCFA"
        ))
        
        # Test 2: R√©cup√©ration exacte
        print("\n2Ô∏è‚É£ Test r√©cup√©ration (formulation identique)...")
        result = asyncio.run(cache.get_cached_response(
            "Combien co√ªte 6 paquets de couches culottes ?"
        ))
        
        if result:
            print(f"   ‚úÖ Trouv√©: {result[0][:50]}...")
            print(f"   üéØ Confiance: {result[1]:.3f}")
        
        # Test 3: R√©cup√©ration similaire
        print("\n3Ô∏è‚É£ Test r√©cup√©ration (formulation diff√©rente)...")
        result = asyncio.run(cache.get_cached_response(
            "Prix de 6 paquets couches culottes ?"
        ))
        
        if result:
            print(f"   ‚úÖ Trouv√©: {result[0][:50]}...")
            print(f"   üéØ Confiance: {result[1]:.3f}")
        else:
            print("   ‚ö†Ô∏è Pas trouv√© (seuil trop √©lev√©)")
        
        # Test 4: Stats
        print("\n4Ô∏è‚É£ Statistiques...")
        stats = cache.get_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        print("\n‚úÖ TESTS TERMIN√âS")
        
    except Exception as e:
        print(f"‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("üîß FIX DU CACHE S√âMANTIQUE")
    print("="*60)
    
    # Analyser les probl√®mes
    analyze_cache_issues()
    
    # Cr√©er version optimis√©e
    create_optimized_cache()
    
    # Tester
    test_optimized_cache()
    
    print("\n‚úÖ FIX TERMIN√â !")
    print("üìå Utiliser: from core.optimized_semantic_cache import get_optimized_cache")
