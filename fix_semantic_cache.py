#!/usr/bin/env python3
"""
ğŸ”§ FIX CACHE SÃ‰MANTIQUE - AMÃ‰LIORATION ET SIMPLIFICATION
Corrige les problÃ¨mes du cache sÃ©mantique pour le rendre utilisable
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

def analyze_cache_issues():
    """Analyse les problÃ¨mes du cache sÃ©mantique actuel"""
    
    print("ğŸ” ANALYSE DU CACHE SÃ‰MANTIQUE ACTUEL")
    print("="*60)
    
    try:
        from core.semantic_intent_cache import SemanticIntentCache, IntentSignature
        
        # Test d'initialisation
        print("\n1ï¸âƒ£ Test d'initialisation...")
        cache = SemanticIntentCache(
            similarity_threshold=0.4  # âš ï¸ PROBLÃˆME: Trop bas
        )
        print(f"   âœ… Cache initialisÃ©")
        print(f"   âš ï¸ Seuil actuel: {cache.similarity_threshold} (TROP BAS)")
        print(f"   ğŸ’¡ RecommandÃ©: 0.85-0.92")
        
        # Test stats
        print("\n2ï¸âƒ£ Statistiques cache...")
        stats = cache.get_stats()
        print(f"   ğŸ“Š Stats: {stats}")
        
        # Test de performance
        print("\n3ï¸âƒ£ Test de performance...")
        import time
        
        # Simuler une requÃªte
        intent_sig = IntentSignature(
            primary_intent="PRIX",
            secondary_intents=[],
            entities={"produit": "couches_culottes", "quantite": "6"},
            context_hash="test123",
            confidence_score=0.9
        )
        
        # Test stockage
        start = time.time()
        import asyncio
        asyncio.run(cache.store_response(
            query="Combien 6 paquets couches culottes ?",
            response="6 paquets de couches culottes coÃ»tent 25.000 FCFA",
            intent_signature=intent_sig,
            conversation_history=""
        ))
        store_time = (time.time() - start) * 1000
        print(f"   â±ï¸ Temps stockage: {store_time:.0f}ms")
        
        if store_time > 100:
            print(f"   âš ï¸ PROBLÃˆME: Stockage trop lent (>{100}ms)")
        
        # Test rÃ©cupÃ©ration
        start = time.time()
        result = asyncio.run(cache.get_cached_response(
            query="Prix 6 paquets couches culottes ?",  # Formulation diffÃ©rente
            intent_signature=intent_sig,
            conversation_history=""
        ))
        retrieve_time = (time.time() - start) * 1000
        print(f"   â±ï¸ Temps rÃ©cupÃ©ration: {retrieve_time:.0f}ms")
        
        if retrieve_time > 200:
            print(f"   âš ï¸ PROBLÃˆME: RÃ©cupÃ©ration trop lente (>{200}ms)")
        
        if result:
            response, confidence = result
            print(f"   âœ… TrouvÃ©: {response[:50]}...")
            print(f"   ğŸ¯ Confiance: {confidence:.2f}")
        else:
            print(f"   âŒ Pas trouvÃ©")
        
        print("\n" + "="*60)
        print("ğŸ“Š PROBLÃˆMES IDENTIFIÃ‰S:")
        print("1. âš ï¸ Seuil similaritÃ© trop bas (0.4 â†’ 0.88)")
        print("2. âš ï¸ Stockage potentiellement lent si >100ms")
        print("3. âš ï¸ RÃ©cupÃ©ration potentiellement lente si >200ms")
        print("4. âš ï¸ Pas de nettoyage auto des entrÃ©es obsolÃ¨tes")
        
        return True
        
    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_optimized_cache():
    """CrÃ©e une version optimisÃ©e du cache sÃ©mantique"""
    
    print("\nğŸš€ CRÃ‰ATION CACHE SÃ‰MANTIQUE OPTIMISÃ‰")
    print("="*60)
    
    code = '''#!/usr/bin/env python3
"""
ğŸš€ CACHE SÃ‰MANTIQUE OPTIMISÃ‰ V2
Version simplifiÃ©e et performante du cache sÃ©mantique
"""

import asyncio
import hashlib
import json
import time
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from sentence_transformers import SentenceTransformer, util
import threading

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

@dataclass
class SimpleCacheEntry:
    """EntrÃ©e de cache simplifiÃ©e"""
    query: str
    response: str
    query_embedding: List[float]
    timestamp: float
    hit_count: int
    ttl_seconds: int

class OptimizedSemanticCache:
    """
    ğŸ¯ Cache SÃ©mantique OptimisÃ©
    
    AMÃ‰LIORATIONS:
    - Seuil plus Ã©levÃ© (0.88 au lieu de 0.4)
    - Stockage simplifiÃ© (pas de two-stage retrieval)
    - Nettoyage automatique des entrÃ©es obsolÃ¨tes
    - Performance amÃ©liorÃ©e (<100ms)
    """
    
    def __init__(self, 
                 similarity_threshold: float = 0.88,  # âœ… AUGMENTÃ‰
                 max_cache_size: int = 1000,          # âœ… RÃ‰DUIT
                 default_ttl: int = 1800):            # âœ… 30min au lieu de 1h
        
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        self.default_ttl = default_ttl
        
        # Stockage en mÃ©moire seulement (Redis optionnel)
        self.memory_cache: Dict[str, SimpleCacheEntry] = {}
        self.redis_client = None
        
        # ModÃ¨le d'embeddings lÃ©ger
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # Stats
        self.stats = {
            "total_queries": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "avg_retrieve_time_ms": 0
        }
        
        self.lock = threading.RLock()
        self._init_redis()
    
    def _init_redis(self):
        """Initialise Redis (optionnel)"""
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(
                    host='localhost',
                    port=6379,
                    db=3,  # DB 3 pour cache optimisÃ©
                    decode_responses=False
                )
                self.redis_client.ping()
                print("âœ… Redis connectÃ© (DB 3)")
            except:
                self.redis_client = None
    
    def _create_embedding(self, text: str) -> List[float]:
        """CrÃ©e un embedding"""
        return self.model.encode(text, convert_to_tensor=False).tolist()
    
    def _compute_similarity(self, emb1: List[float], emb2: List[float]) -> float:
        """Calcule la similaritÃ© cosinus"""
        return util.cos_sim(emb1, emb2).item()
    
    async def get_cached_response(self, query: str) -> Optional[Tuple[str, float]]:
        """
        RÃ©cupÃ¨re une rÃ©ponse du cache
        
        Returns:
            (response, confidence) si trouvÃ©, None sinon
        """
        start_time = time.time()
        
        with self.lock:
            self.stats["total_queries"] += 1
            
            # CrÃ©er embedding query
            query_embedding = self._create_embedding(query)
            
            # Chercher dans le cache
            best_match = None
            best_similarity = 0.0
            
            for cache_key, entry in self.memory_cache.items():
                # VÃ©rifier TTL
                if time.time() - entry.timestamp > entry.ttl_seconds:
                    continue  # Ignorer les entrÃ©es expirÃ©es
                
                # Calculer similaritÃ©
                similarity = self._compute_similarity(query_embedding, entry.query_embedding)
                
                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = entry
            
            # Mettre Ã  jour stats
            retrieve_time_ms = (time.time() - start_time) * 1000
            self.stats["avg_retrieve_time_ms"] = (
                (self.stats["avg_retrieve_time_ms"] * (self.stats["total_queries"] - 1) + retrieve_time_ms)
                / self.stats["total_queries"]
            )
            
            if best_match:
                self.stats["cache_hits"] += 1
                best_match.hit_count += 1
                
                print(f"âœ… Cache HIT (similarity: {best_similarity:.3f}, time: {retrieve_time_ms:.0f}ms)")
                return (best_match.response, best_similarity)
            else:
                self.stats["cache_misses"] += 1
                print(f"âŒ Cache MISS (time: {retrieve_time_ms:.0f}ms)")
                return None
    
    async def store_response(self, 
                            query: str, 
                            response: str,
                            ttl: Optional[int] = None):
        """Stocke une rÃ©ponse dans le cache"""
        
        with self.lock:
            # CrÃ©er embedding
            query_embedding = self._create_embedding(query)
            
            # CrÃ©er clÃ© unique
            cache_key = hashlib.md5(query.encode()).hexdigest()
            
            # CrÃ©er entrÃ©e
            entry = SimpleCacheEntry(
                query=query,
                response=response,
                query_embedding=query_embedding,
                timestamp=time.time(),
                hit_count=0,
                ttl_seconds=ttl or self.default_ttl
            )
            
            # Stocker
            self.memory_cache[cache_key] = entry
            
            # Nettoyer si trop d'entrÃ©es
            if len(self.memory_cache) > self.max_cache_size:
                self._cleanup_old_entries()
            
            print(f"ğŸ’¾ Cache STORE (total: {len(self.memory_cache)} entrÃ©es)")
    
    def _cleanup_old_entries(self):
        """Nettoie les entrÃ©es obsolÃ¨tes"""
        now = time.time()
        
        # Supprimer entrÃ©es expirÃ©es
        expired_keys = [
            key for key, entry in self.memory_cache.items()
            if now - entry.timestamp > entry.ttl_seconds
        ]
        
        for key in expired_keys:
            del self.memory_cache[key]
        
        # Si encore trop, supprimer les moins utilisÃ©es
        if len(self.memory_cache) > self.max_cache_size:
            sorted_entries = sorted(
                self.memory_cache.items(),
                key=lambda x: x[1].hit_count
            )
            
            # Garder seulement les plus utilisÃ©es
            to_keep = sorted_entries[-self.max_cache_size:]
            self.memory_cache = dict(to_keep)
        
        print(f"ğŸ§¹ Nettoyage: {len(expired_keys)} expirÃ©es, {len(self.memory_cache)} conservÃ©es")
    
    def clear_cache(self):
        """Vide complÃ¨tement le cache"""
        with self.lock:
            self.memory_cache.clear()
            print("ğŸ—‘ï¸ Cache vidÃ©")
    
    def get_stats(self) -> Dict:
        """Retourne les statistiques"""
        with self.lock:
            hit_rate = (
                (self.stats["cache_hits"] / self.stats["total_queries"] * 100)
                if self.stats["total_queries"] > 0 else 0
            )
            
            return {
                **self.stats,
                "hit_rate_percent": hit_rate,
                "cache_size": len(self.memory_cache)
            }

# Singleton global
_global_cache = None

def get_optimized_cache() -> OptimizedSemanticCache:
    """Retourne l'instance globale du cache"""
    global _global_cache
    if _global_cache is None:
        _global_cache = OptimizedSemanticCache()
    return _global_cache
'''
    
    # Sauvegarder
    output_path = Path(__file__).parent / "core" / "optimized_semantic_cache.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(code)
    
    print(f"âœ… Cache optimisÃ© crÃ©Ã©: {output_path}")
    print("\nğŸ“‹ AMÃ‰LIORATIONS:")
    print("   1. âœ… Seuil: 0.4 â†’ 0.88 (plus strict)")
    print("   2. âœ… TTL: 3600s â†’ 1800s (30min)")
    print("   3. âœ… Max size: 10000 â†’ 1000 (plus lÃ©ger)")
    print("   4. âœ… Nettoyage auto des entrÃ©es obsolÃ¨tes")
    print("   5. âœ… Pas de two-stage retrieval (plus simple)")
    print("   6. âœ… Performance: <100ms visÃ©")

def test_optimized_cache():
    """Teste le cache optimisÃ©"""
    
    print("\nğŸ§ª TEST DU CACHE OPTIMISÃ‰")
    print("="*60)
    
    try:
        from core.optimized_semantic_cache import get_optimized_cache
        import asyncio
        
        cache = get_optimized_cache()
        
        # Test 1: Stockage
        print("\n1ï¸âƒ£ Test stockage...")
        asyncio.run(cache.store_response(
            query="Combien coÃ»te 6 paquets de couches culottes ?",
            response="6 paquets de couches culottes coÃ»tent 25.000 FCFA"
        ))
        
        # Test 2: RÃ©cupÃ©ration exacte
        print("\n2ï¸âƒ£ Test rÃ©cupÃ©ration (formulation identique)...")
        result = asyncio.run(cache.get_cached_response(
            "Combien coÃ»te 6 paquets de couches culottes ?"
        ))
        
        if result:
            print(f"   âœ… TrouvÃ©: {result[0][:50]}...")
            print(f"   ğŸ¯ Confiance: {result[1]:.3f}")
        
        # Test 3: RÃ©cupÃ©ration similaire
        print("\n3ï¸âƒ£ Test rÃ©cupÃ©ration (formulation diffÃ©rente)...")
        result = asyncio.run(cache.get_cached_response(
            "Prix de 6 paquets couches culottes ?"
        ))
        
        if result:
            print(f"   âœ… TrouvÃ©: {result[0][:50]}...")
            print(f"   ğŸ¯ Confiance: {result[1]:.3f}")
        else:
            print("   âš ï¸ Pas trouvÃ© (seuil trop Ã©levÃ©)")
        
        # Test 4: Stats
        print("\n4ï¸âƒ£ Statistiques...")
        stats = cache.get_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        print("\nâœ… TESTS TERMINÃ‰S")
        
    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("ğŸ”§ FIX DU CACHE SÃ‰MANTIQUE")
    print("="*60)
    
    # Analyser les problÃ¨mes
    analyze_cache_issues()
    
    # CrÃ©er version optimisÃ©e
    create_optimized_cache()
    
    # Tester
    test_optimized_cache()
    
    print("\nâœ… FIX TERMINÃ‰ !")
    print("ğŸ“Œ Utiliser: from core.optimized_semantic_cache import get_optimized_cache")
