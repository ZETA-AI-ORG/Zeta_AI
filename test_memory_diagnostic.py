#!/usr/bin/env python3
"""
üîç TEST DIAGNOSTIC M√âMOIRE CONVERSATIONNELLE
============================================
Analyse pr√©cise de ce qui se passe avec la persistance du contexte
"""

import asyncio
import time
from core.enhanced_rag_with_semantic_cache import EnhancedRAGWithSemanticCache
from core.universal_rag_engine import UniversalRAGEngine

class MemoryDiagnosticTest:
    """üß™ Test diagnostic pour analyser la m√©moire conversationnelle"""
    
    def __init__(self):
        self.company_id = "MpfnlSbqwaZ6F4HvxQLRL9du0yG3"
        self.user_id = "testuser_memory"
        
        # Initialisation du RAG (CACHE D√âSACTIV√â)
        self.base_rag = UniversalRAGEngine()
        self.enhanced_rag = EnhancedRAGWithSemanticCache(self.base_rag)
        self.enhanced_rag.enable_cache(False)
        
        # Historique de conversation pour test
        self.conversation_context = ""
        
    async def run_memory_diagnostic(self):
        """üöÄ Lance le diagnostic complet de m√©moire"""
        print("üîç DIAGNOSTIC M√âMOIRE CONVERSATIONNELLE")
        print("=" * 60)
        print(f"üè¢ Company ID: {self.company_id}")
        print(f"üë§ User ID: {self.user_id}")
        print(f"üö´ Cache: D√âSACTIV√â")
        print()
        
        # Test 1: V√©rifier le contexte initial
        await self._test_1_contexte_initial()
        
        # Test 2: Ajouter information et v√©rifier persistance
        await self._test_2_ajout_information()
        
        # Test 3: Nouvelle requ√™te avec contexte
        await self._test_3_persistance_contexte()
        
        # Test 4: Analyser le prompt final envoy√© au LLM
        await self._test_4_analyse_prompt_llm()
        
    async def _test_1_contexte_initial(self):
        """Test 1: Analyser le contexte initial"""
        print("üß™ TEST 1: CONTEXTE INITIAL")
        print("-" * 40)
        
        query = "Bonjour, je suis √† Cocody"
        
        print(f"üìù Requ√™te: {query}")
        print(f"üß† Contexte avant: '{self.conversation_context}'")
        print(f"üìè Taille contexte: {len(self.conversation_context)} caract√®res")
        print()
        
        # üîç LOGS M√âMOIRE AVANT APPEL RAG
        print(f"üîç [MEMORY_LOG] AVANT APPEL RAG:")
        print(f"üîç [MEMORY_LOG] conversation_history transmis: '{self.conversation_context}'")
        print(f"üîç [MEMORY_LOG] Taille: {len(self.conversation_context)} chars")
        print()
        
        # Appel RAG
        response_data = await self.enhanced_rag.process_query(
            query=query,
            company_id=self.company_id,
            user_id=self.user_id,
            conversation_history=self.conversation_context
        )
        
        # Mise √† jour du contexte (simulation)
        self._update_conversation_context(query, response_data.get("response", ""))
        
        print(f"ü§ñ R√©ponse: {response_data.get('response', 'VIDE')[:100]}...")
        print(f"üß† Contexte apr√®s: '{self.conversation_context}'")
        print(f"üìè Nouvelle taille: {len(self.conversation_context)} caract√®res")
        print()
        
    async def _test_2_ajout_information(self):
        """Test 2: Ajouter information et v√©rifier"""
        print("üß™ TEST 2: AJOUT INFORMATION")
        print("-" * 40)
        
        query = "Je veux des couches culottes 3 paquets"
        
        print(f"üìù Requ√™te: {query}")
        print(f"üß† Contexte avant: '{self.conversation_context}'")
        print(f"üîç Contient 'Cocody': {'Cocody' in self.conversation_context}")
        print()
        
        # üîç LOGS M√âMOIRE AVANT APPEL RAG #2
        print(f"üîç [MEMORY_LOG] AVANT APPEL RAG #2:")
        print(f"üîç [MEMORY_LOG] conversation_history: '{self.conversation_context}'")
        print(f"üîç [MEMORY_LOG] Contient Cocody: {'Cocody' in self.conversation_context}")
        print()
        
        # Appel RAG avec contexte enrichi
        response_data = await self.enhanced_rag.process_query(
            query=query,
            company_id=self.company_id,
            user_id=self.user_id,
            conversation_history=self.conversation_context
        )
        
        # Mise √† jour du contexte
        self._update_conversation_context(query, response_data.get("response", ""))
        
        print(f"ü§ñ R√©ponse: {response_data.get('response', 'VIDE')[:100]}...")
        print(f"üß† Contexte apr√®s: '{self.conversation_context}'")
        print(f"üîç Contient toujours 'Cocody': {'Cocody' in self.conversation_context}")
        print(f"üîç Contient '3 paquets': {'3 paquets' in self.conversation_context}")
        print()
        
    async def _test_3_persistance_contexte(self):
        """Test 3: V√©rifier persistance dans nouvelle requ√™te"""
        print("üß™ TEST 3: PERSISTANCE CONTEXTE")
        print("-" * 40)
        
        query = "√áa co√ªte combien la livraison ?"
        
        print(f"üìù Requ√™te: {query}")
        print(f"üß† Contexte transmis: '{self.conversation_context}'")
        print(f"üîç Informations disponibles:")
        print(f"   - Cocody: {'Cocody' in self.conversation_context}")
        print(f"   - Couches: {'couches' in self.conversation_context.lower()}")
        print(f"   - 3 paquets: {'3 paquets' in self.conversation_context}")
        print()
        
        # üîç LOGS M√âMOIRE CRITIQUE AVANT APPEL RAG #3
        print(f"üîç [MEMORY_LOG] APPEL CRITIQUE RAG #3:")
        print(f"üîç [MEMORY_LOG] conversation_history COMPLET:")
        print(f"üîç [MEMORY_LOG] '{self.conversation_context}'")
        print(f"üîç [MEMORY_LOG] √âl√©ments critiques pr√©sents:")
        print(f"üîç [MEMORY_LOG]   - Cocody: {'Cocody' in self.conversation_context}")
        print(f"üîç [MEMORY_LOG]   - couches: {'couches' in self.conversation_context.lower()}")
        print(f"üîç [MEMORY_LOG]   - 3 paquets: {'3 paquets' in self.conversation_context}")
        print()
        
        # Appel RAG - LE SYST√àME DEVRAIT UTILISER LE CONTEXTE
        response_data = await self.enhanced_rag.process_query(
            query=query,
            company_id=self.company_id,
            user_id=self.user_id,
            conversation_history=self.conversation_context
        )
        
        response = response_data.get("response", "")
        
        print(f"ü§ñ R√©ponse: {response}")
        print()
        print(f"üîç ANALYSE R√âPONSE:")
        print(f"   - Mentionne Cocody: {'Cocody' in response or 'cocody' in response.lower()}")
        print(f"   - Mentionne zone centrale: {'centrale' in response.lower()}")
        print(f"   - Mentionne 1500 FCFA: {'1500' in response}")
        print(f"   - Utilise contexte: {self._analyze_context_usage(response)}")
        print()
        
    async def _test_4_analyse_prompt_llm(self):
        """Test 4: Analyser le prompt envoy√© au LLM"""
        print("üß™ TEST 4: ANALYSE PROMPT LLM")
        print("-" * 40)
        
        # Intercepter le prompt (simulation)
        print(f"üéØ CONTEXTE FINAL TRANSMIS AU RAG:")
        print(f"üìè Taille: {len(self.conversation_context)} caract√®res")
        print(f"üìù Contenu complet:")
        print(f"'{self.conversation_context}'")
        print()
        
        print(f"üîç ANALYSE STRUCTURE:")
        lines = self.conversation_context.split('\n')
        print(f"   - Nombre de lignes: {len(lines)}")
        print(f"   - Messages utilisateur: {self.conversation_context.count('USER:')}")
        print(f"   - R√©ponses assistant: {self.conversation_context.count('ASSISTANT:')}")
        print()
        
        print(f"üéØ √âL√âMENTS CRITIQUES:")
        critical_elements = {
            "Cocody": "Cocody" in self.conversation_context,
            "couches": "couches" in self.conversation_context.lower(),
            "3 paquets": "3 paquets" in self.conversation_context,
            "prix": "prix" in self.conversation_context.lower(),
            "livraison": "livraison" in self.conversation_context.lower()
        }
        
        for element, present in critical_elements.items():
            status = "‚úÖ" if present else "‚ùå"
            print(f"   {status} {element}: {present}")
        
        print()
        
    def _update_conversation_context(self, query: str, response: str):
        """Met √† jour le contexte conversationnel"""
        print(f"üîç [MEMORY_UPDATE] MISE √Ä JOUR CONTEXTE:")
        print(f"üîç [MEMORY_UPDATE] Contexte AVANT: '{self.conversation_context}'")
        
        # Ajouter le message utilisateur
        self.conversation_context += f"USER: {query}\n"
        print(f"üîç [MEMORY_UPDATE] Apr√®s ajout USER: '{self.conversation_context}'")
        
        # Ajouter la r√©ponse (tronqu√©e si trop longue)
        response_truncated = response[:200] + "..." if len(response) > 200 else response
        self.conversation_context += f"ASSISTANT: {response_truncated}\n"
        print(f"üîç [MEMORY_UPDATE] Apr√®s ajout ASSISTANT: '{self.conversation_context}'")
        
        # Limiter la taille totale du contexte
        if len(self.conversation_context) > 1000:
            lines = self.conversation_context.split('\n')
            # Garder les 10 derni√®res lignes
            old_context = self.conversation_context
            self.conversation_context = '\n'.join(lines[-10:])
            print(f"üîç [MEMORY_UPDATE] TRONCATURE APPLIQU√âE:")
            print(f"üîç [MEMORY_UPDATE] Ancien: '{old_context}'")
            print(f"üîç [MEMORY_UPDATE] Nouveau: '{self.conversation_context}'")
        
        print(f"üîç [MEMORY_UPDATE] Contexte FINAL: '{self.conversation_context}'")
        print(f"üîç [MEMORY_UPDATE] Taille finale: {len(self.conversation_context)} chars")
        print()
    
    def _analyze_context_usage(self, response: str) -> str:
        """Analyse si la r√©ponse utilise le contexte"""
        context_indicators = []
        
        if "Cocody" in response or "cocody" in response.lower():
            context_indicators.append("Localisation")
        
        if "1500" in response:
            context_indicators.append("Prix zone centrale")
            
        if "couches" in response.lower():
            context_indicators.append("Produit mentionn√©")
            
        if context_indicators:
            return f"OUI ({', '.join(context_indicators)})"
        else:
            return "NON - R√©ponse g√©n√©rique"

async def main():
    """Point d'entr√©e principal"""
    test = MemoryDiagnosticTest()
    await test.run_memory_diagnostic()
    
    print("üéØ CONCLUSION:")
    print("Ce test r√©v√®le exactement o√π la m√©moire conversationnelle √©choue.")
    print("Analysez les r√©sultats pour identifier les points de d√©faillance.")

if __name__ == "__main__":
    asyncio.run(main())
