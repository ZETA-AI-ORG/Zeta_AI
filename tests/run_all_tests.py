#!/usr/bin/env python3
"""
ğŸš€ LANCEUR DE TESTS COMPLET

ExÃ©cute les 3 scÃ©narios et gÃ©nÃ¨re un rapport comparatif

Usage:
    python tests/run_all_tests.py
"""

import asyncio
import sys
import os
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tests.conversation_simulator import run_scenario
from tests.test_scenarios import SCENARIOS


async def run_all_tests():
    """
    ExÃ©cute tous les scÃ©narios de test
    """
    print("\n" + "="*80)
    print("ğŸ§ª LANCEMENT SUITE COMPLÃˆTE DE TESTS")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    results = {}
    
    for scenario_name in ["light", "medium", "hardcore"]:
        print(f"\nğŸ¯ PrÃ©paration scÃ©nario: {scenario_name.upper()}")
        
        scenario = SCENARIOS[scenario_name]
        
        print(f"ğŸ“‹ Description: {scenario['description']}")
        print(f"ğŸ‘¤ Persona: {scenario['persona']['profil']}")
        print(f"ğŸ­ Nombre de tours: {len(scenario['messages'])}")
        
        input(f"\nâ¸ï¸  Appuyez sur ENTRÃ‰E pour lancer le test {scenario_name.upper()}...")
        
        # ExÃ©cuter
        tracker, report_path = await run_scenario(scenario)
        
        results[scenario_name] = {
            "tracker": tracker,
            "report_path": report_path
        }
        
        print(f"\nâœ… Test {scenario_name.upper()} terminÃ©!")
        print(f"ğŸ“„ Rapport: {report_path}\n")
        
        # Pause entre scÃ©narios
        if scenario_name != "hardcore":
            await asyncio.sleep(3)
    
    # Rapport final comparatif
    print("\n" + "="*80)
    print("ğŸ“Š RAPPORT COMPARATIF FINAL")
    print("="*80 + "\n")
    
    print(f"{'SCENARIO':<15} {'TOURS':<8} {'TEMPS TOTAL':<15} {'COÃ›T':<12} {'CONFIANCE':<12}")
    print("-" * 80)
    
    for scenario_name, result in results.items():
        tracker = result['tracker']
        
        # Confiance finale
        confiance_finale = "N/A"
        if tracker.turns:
            last_thinking = tracker.turns[-1].get('thinking', {})
            confiance = last_thinking.get('confiance', {})
            confiance_finale = f"{confiance.get('score', 0)}%"
        
        print(f"{scenario_name.upper():<15} {len(tracker.turns):<8} "
              f"{tracker.total_time_ms/1000:.1f}s{'':<10} "
              f"${tracker.total_cost:.4f}{'':<4} {confiance_finale:<12}")
    
    print("\n" + "="*80)
    
    # Analyse qualitative
    print("\nğŸ“ˆ ANALYSE QUALITATIVE:\n")
    
    for scenario_name, result in results.items():
        tracker = result['tracker']
        
        print(f"\nğŸ­ {scenario_name.upper()}:")
        
        if tracker.turns:
            last_turn = tracker.turns[-1]
            last_thinking = last_turn.get('thinking', {})
            
            # ComplÃ©tude
            progression = last_thinking.get('progression', {})
            completude = progression.get('completude', 'N/A')
            print(f"   âœ… ComplÃ©tude: {completude}")
            
            # DonnÃ©es collectÃ©es
            deja_collecte = last_thinking.get('deja_collecte', {})
            if deja_collecte:
                collectees = [k for k, v in deja_collecte.items() if v and v != 'null']
                print(f"   ğŸ“‹ DonnÃ©es collectÃ©es: {', '.join(collectees) if collectees else 'Aucune'}")
            
            # Phase atteinte
            strategie = last_thinking.get('strategie_qualification', {})
            phase = strategie.get('phase', 'N/A')
            print(f"   ğŸ¯ Phase finale: {phase}")
        
        print(f"   ğŸ’° CoÃ»t: ${tracker.total_cost:.4f} ({tracker.total_tokens} tokens)")
        print(f"   â±ï¸  Performance: {tracker.total_time_ms/len(tracker.turns) if tracker.turns else 0:.0f}ms/tour")
    
    print("\n" + "="*80)
    print("âœ… TOUS LES TESTS TERMINÃ‰S!")
    print("ğŸ“ Rapports disponibles dans: tests/reports/")
    print("="*80 + "\n")


if __name__ == "__main__":
    asyncio.run(run_all_tests())
