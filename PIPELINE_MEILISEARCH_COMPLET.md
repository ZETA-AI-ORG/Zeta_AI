# üìã PIPELINE MEILISEARCH COMPLET - DE LA QUERY AU LLM

## üéØ FLUX GLOBAL
```
Query Utilisateur
    ‚Üì
[1] Filtrage Stop Words
    ‚Üì
[2] G√©n√©ration N-grams
    ‚Üì
[3] Recherche MeiliSearch Parall√®le
    ‚Üì
[4] D√©duplication Documents
    ‚Üì
[5] Scoring Smart V2
    ‚Üì
[6] Boost N-gram ID
    ‚Üì
[7] Boost Company Keywords
    ‚Üì
[8] Filtrage Dynamique
    ‚Üì
[9] Extraction Contexte
    ‚Üì
[10] Filtrage par Seuil
    ‚Üì
[11] Limitation Documents
    ‚Üì
[12] Formatage Final
    ‚Üì
Envoi au LLM
```

---

## [1] FILTRAGE STOP WORDS

**Fichier**: `vector_store_clean_v2.py` (ligne 286-292)  
**Fonction**: `filter_query_for_meilisearch()` (import√©e de `core.smart_stopwords`)

### Param√®tres:
- **Input**: Query originale
- **Output**: Query filtr√©e (stop words retir√©s)
- **Stop words**: 800+ mots (salutations, pronoms, articles, etc.)

### Exemple:
```
Input:  "bonjour je veux la taille 5"
Output: "veux taille 5"
```

---

## [2] G√âN√âRATION N-GRAMS

**Fichier**: `vector_store_clean_v2.py` (ligne 114-178)  
**Fonction**: `_generate_ngrams(query, max_n=3, min_n=1)`

### Param√®tres:
- **max_n**: 3 (n-grams jusqu'√† 3 mots)
- **min_n**: 1 (n-grams d'au moins 1 mot)
- **Filtrage stop words**: OUI (ligne 118)
- **Mots de liaison autoris√©s**: {"√†", "a", "√°", "√¢", "√§", "√£", "√•"} (uniquement dans n-grams de 3 mots au milieu)

### R√®gles de g√©n√©ration:
1. **N-grams cons√©cutifs** (1, 2, 3 mots)
2. **Combinaisons non-cons√©cutives** (paires de 2 mots)
3. **Filtrage strict**:
   - ‚ùå Chiffres isol√©s (ex: "5")
   - ‚ùå Lettres isol√©es (ex: "a")
   - ‚ùå N-grams identiques (ex: "taille taille")
   - ‚ùå N-grams uniquement num√©riques (ex: "2 3")

### Exemple:
```
Query filtr√©e: "veux taille 5"
N-grams g√©n√©r√©s:
  1. veux taille 5      (n-gram 3)
  2. veux taille        (n-gram 2)
  3. taille 5           (n-gram 2) ‚úÖ IMPORTANT!
  4. veux               (n-gram 1)
  5. taille             (n-gram 1)
  6. veux 5             (combinaison non-cons√©cutive)
```

**‚ö†Ô∏è PROBL√àME IDENTIFI√â**: Le "5" isol√© est filtr√© (ligne 166), mais "taille 5" est conserv√© ‚úÖ

---

## [3] RECHERCHE MEILISEARCH PARALL√àLE

**Fichier**: `vector_store_clean_v2.py` (ligne 311-332)  
**Fonction**: `search_single(ngram, index_name)`

### Param√®tres:
- **Index recherch√©s**: 
  - `products_{company_id}`
  - `delivery_{company_id}`
  - `support_paiement_{company_id}`
  - `localisation_{company_id}`
- **Limite par recherche**: 5 documents
- **Champs r√©cup√©r√©s**: `['content', 'id', 'type', 'searchable_text']`
- **Parall√©lisation**: ThreadPoolExecutor avec 15 workers max
- **Nombre total de t√¢ches**: `len(ngrams) √ó len(indexes)` (ex: 6 n-grams √ó 4 index = 24 recherches)

### Exemple:
```
N-gram "taille 5" ‚Üí Recherche dans 4 index
  ‚Üí products_rue_du_gros: 5 hits
  ‚Üí delivery_rue_du_gros: 0 hits
  ‚Üí support_paiement_rue_du_gros: 0 hits
  ‚Üí localisation_rue_du_gros: 0 hits
```

---

## [4] D√âDUPLICATION DOCUMENTS

**Fichier**: `vector_store_clean_v2.py` (ligne 334-354)

### Param√®tres:
- **Champ prioritaire**: `searchable_text` si plus long que `content`
- **Longueur minimale**: 30 caract√®res
- **D√©duplication**: Par contenu exact (set `seen_contents`)

### Logique:
```python
if len(searchable_text) > len(content):
    content = searchable_text
else:
    content = content or searchable_text

if content and len(content) > 30 and content not in seen_contents:
    # Document ajout√©
```

---

## [5] SCORING SMART V2

**Fichier**: `vector_store_clean_v2.py` (ligne 193-263)  
**Fonction**: `_calculate_smart_score_v2(content, query, all_corpus)`

### Param√®tres de scoring:

#### A. Constants:
- **BONUS_EXACT**: 5 points par mot du n-gram
- **BONUS_FUZZY**: 0.5 points par mot du n-gram
- **Seuil fuzzy**: ratio ‚â• 90%

#### B. Normalisation:
```python
def _normalize(text):
    return unidecode(text.lower().strip())
```
- **Minuscules**: OUI ‚úÖ
- **Suppression accents**: OUI (via unidecode)
- **Trim espaces**: OUI

#### C. G√©n√©ration n-grams internes (pour scoring):
```python
for n in range(1, 4):  # n-grams de 1 √† 3 mots
    ngrams += [' '.join(query_words[i:i+n]) for i in range(len(query_words)-n+1)]
```

#### D. Calcul du score:
```python
content_normalized = _normalize(content)
doc_id_normalized = _normalize(doc_id)

for ng in ngrams:
    n = len(ng.split())  # Nombre de mots dans le n-gram
    
    # Match exact (case-insensitive)
    if ng in content_normalized or (doc_id_normalized and ng in doc_id_normalized):
        score += BONUS_EXACT * n  # 5 √ó n
    else:
        # Fuzzy match (ratio >= 90)
        for doc_word in doc_words:
            if fuzz.ratio(ng, doc_word) >= 90:
                score += BONUS_FUZZY * n  # 0.5 √ó n
                break
```

### Exemples de scoring:

#### Exemple 1: "taille 5" (n-gram 2 mots)
```
Document: "Taille 5 (12-17kg): 25.900 FCFA"

N-grams g√©n√©r√©s du query "veux taille 5":
  - "veux taille 5" (3 mots) ‚Üí pas dans doc ‚Üí 0 points
  - "veux taille" (2 mots) ‚Üí pas dans doc ‚Üí 0 points
  - "taille 5" (2 mots) ‚Üí ‚úÖ MATCH EXACT ‚Üí 5 √ó 2 = 10 points
  - "veux" (1 mot) ‚Üí pas dans doc ‚Üí 0 points
  - "taille" (1 mot) ‚Üí ‚úÖ MATCH EXACT ‚Üí 5 √ó 1 = 5 points
  - "veux 5" (2 mots) ‚Üí pas dans doc ‚Üí 0 points

Score total: 10 + 5 = 15 points ‚úÖ
```

#### Exemple 2: "Taille 1" (autre taille)
```
Document: "Taille 1 (0-4kg): 17.900 FCFA"

N-grams:
  - "veux taille 5" ‚Üí pas dans doc ‚Üí 0 points
  - "veux taille" ‚Üí pas dans doc ‚Üí 0 points
  - "taille 5" ‚Üí pas dans doc (c'est "taille 1") ‚Üí 0 points
  - "veux" ‚Üí pas dans doc ‚Üí 0 points
  - "taille" ‚Üí ‚úÖ MATCH EXACT ‚Üí 5 √ó 1 = 5 points
  - "veux 5" ‚Üí pas dans doc ‚Üí 0 points

Score total: 5 points
```

**‚úÖ CONCLUSION**: Le n-gram "taille 5" (2 mots) donne 10 points, tandis que "taille" seul (1 mot) donne 5 points. Le document avec "Taille 5" aura un score 3√ó sup√©rieur (15 vs 5).

---

## [6] BOOST N-GRAM ID

**Fichier**: `vector_store_clean_v2.py` (ligne 412-423)

### Param√®tres:
- **Bonus par n-gram trouv√© dans l'ID**: +10 points
- **Normalisation**: Suppression des espaces pour comparaison

### Logique:
```python
doc_id = str(doc.get('hit', {}).get('id', '')).lower()
for ng in ngrams:
    ng_norm = ng.lower().replace(' ', '')
    if ng_norm and doc_id and ng_norm in doc_id.replace(' ', ''):
        ngram_bonus += 10
```

### Exemple:
```
Document ID: "taille5_pression"
N-gram: "taille 5"
  ‚Üí ng_norm = "taille5"
  ‚Üí doc_id = "taille5_pression"
  ‚Üí ‚úÖ MATCH ‚Üí +10 points
```

---

## [7] BOOST COMPANY KEYWORDS

**Fichier**: `vector_store_clean_v2.py` (ligne 427-454)  
**Fonction**: `get_company_boosters(company_id)`

### Param√®tres:
- **Boost par keyword**: +2 points
- **Maximum boost**: +5 points
- **Type**: Additif (pas multiplicatif)

### Logique:
```python
boosters_keywords = boosters.get('keywords', [])

for doc in scored_documents:
    content_lower = doc['content'].lower()
    boost = 0
    
    for keyword in boosters_keywords:
        if keyword in query_lower and keyword in content_lower:
            boost += 2
    
    if boost > 0:
        doc['score'] += min(boost, 5)  # Max +5 points
```

---

## [8] FILTRAGE DYNAMIQUE

**Fichier**: `vector_store_clean_v2.py` (ligne 456-476)  
**Fonction**: `filter_by_dynamic_threshold()` (import√©e de `core.smart_metadata_extractor`)

### Param√®tres:
- **Input**: Documents avec scores
- **Output**: Documents filtr√©s selon seuil dynamique
- **Logique**: Calcul automatique du seuil bas√© sur la distribution des scores

---

## [9] EXTRACTION CONTEXTE

**Fichier**: `vector_store_clean_v2.py` (ligne 478-502)  
**Fonction**: `extract_relevant_context()` (import√©e de `core.context_extractor`)

### Param√®tres:
- **Intentions d√©tect√©es**: Via `detect_query_intentions(query)`
- **R√©duction du contenu**: Extraction des parties pertinentes uniquement

---

## [10] FILTRAGE PAR SEUIL

**Fichier**: `vector_store_clean_v2.py` (ligne 504-508)

### Param√®tres:
- **threshold**: 4 points (score minimum pour documents "high score")
- **min_score**: 1 point (score minimum absolu)

### Logique:
```python
filtered_docs = [d for d in scored_documents if d['score'] >= min_score]
high_score_docs = [d for d in filtered_docs if d['score'] >= threshold]
```

---

## [11] LIMITATION DOCUMENTS

**Fichier**: `vector_store_clean_v2.py` (ligne 509-520)

### Param√®tres:
- **max_docs**: 3 documents par index
- **Regroupement**: Par `source_index`
- **Tri**: Par score d√©croissant

### Logique:
```python
docs_by_index = defaultdict(list)
for doc in high_score_docs:
    docs_by_index[doc.get('source_index','?')].append(doc)

scored_documents = []
for idx, docs in docs_by_index.items():
    scored_documents.extend(sorted(docs, key=lambda x: x['score'], reverse=True)[:max_docs])

scored_documents.sort(key=lambda x: x['score'], reverse=True)
```

**‚ö†Ô∏è LIMITATION CRITIQUE**: Maximum 3 documents par index, m√™me si 10 documents ont un score parfait!

---

## [12] FORMATAGE FINAL

**Fichier**: `vector_store_clean_v2.py` (ligne 521-567)

### Format envoy√© au LLM:
```
DOCUMENT #1 (Score: 15.00)
Index de provenance: products_rue_du_gros
N-grams trouv√©s: ['taille 5', 'taille']
Taille 5 (12-17kg): 25.900 FCFA

DOCUMENT #2 (Score: 10.00)
Index de provenance: products_rue_du_gros
N-grams trouv√©s: ['taille']
Taille 1 (0-4kg): 17.900 FCFA
```

---

## üî• R√âSUM√â DES PARAM√àTRES CRITIQUES

### G√©n√©ration N-grams:
- ‚úÖ **max_n**: 3 mots
- ‚úÖ **min_n**: 1 mot
- ‚úÖ **Filtrage stop words**: OUI
- ‚ùå **Probl√®me**: Chiffres isol√©s filtr√©s (mais "taille 5" conserv√©)

### Scoring:
- ‚úÖ **BONUS_EXACT**: 5 points √ó nb_mots_ngram
- ‚úÖ **BONUS_FUZZY**: 0.5 points √ó nb_mots_ngram
- ‚úÖ **Normalisation**: Case-insensitive + sans accents
- ‚úÖ **N-gram 3 mots > N-gram 2 mots > N-gram 1 mot**: OUI (15 > 10 > 5)

### Filtrage:
- ‚ö†Ô∏è **threshold**: 4 points (peut √©liminer des matches partiels)
- ‚ö†Ô∏è **min_score**: 1 point
- ‚ùå **max_docs**: 3 par index (LIMITATION CRITIQUE)

### Boosts:
- ‚úÖ **N-gram dans ID**: +10 points
- ‚úÖ **Company keywords**: +2 √† +5 points (additif)

---

## üéØ V√âRIFICATION: "taille 5" vs "Taille 5"

### Query: "veux taille 5"
### Document: "Taille 5 (12-17kg): 25.900 FCFA"

**√âtape 1**: Filtrage stop words
```
"veux taille 5" ‚Üí "veux taille 5" (aucun stop word)
```

**√âtape 2**: N-grams g√©n√©r√©s
```
1. veux taille 5
2. veux taille
3. taille 5      ‚Üê IMPORTANT
4. veux
5. taille
6. veux 5
```

**√âtape 3**: Scoring
```
content_normalized = "taille 5 (12-17kg): 25.900 fcfa"
query_ngrams = ["veux taille 5", "veux taille", "taille 5", "veux", "taille", "veux 5"]

V√©rification "taille 5" (2 mots):
  ‚Üí "taille 5" in "taille 5 (12-17kg): 25.900 fcfa" ‚Üí ‚úÖ TRUE
  ‚Üí score += 5 √ó 2 = 10 points

V√©rification "taille" (1 mot):
  ‚Üí "taille" in "taille 5 (12-17kg): 25.900 fcfa" ‚Üí ‚úÖ TRUE
  ‚Üí score += 5 √ó 1 = 5 points

Score total: 15 points ‚úÖ
```

**‚úÖ CONFIRMATION**: Le syst√®me fonctionne correctement pour les matches exacts case-insensitive!

---

## ‚ö†Ô∏è PROBL√àMES IDENTIFI√âS

### 1. Limitation max_docs = 3
- **Impact**: Si 10 documents ont "taille 5" avec score 15, seuls 3 seront envoy√©s au LLM
- **Solution propos√©e**: Augmenter √† 10 (mais vous avez refus√©)

### 2. Filtrage chiffres isol√©s
- **Impact**: Le "5" seul est filtr√© des n-grams
- **Contournement**: "taille 5" (2 mots) est conserv√© ‚úÖ

### 3. Threshold = 4
- **Impact**: Documents avec score < 4 sont √©limin√©s
- **Exemple**: Un document avec seulement "taille" (5 points) passe, mais un avec fuzzy match (0.5 points) est √©limin√©

---

## üìä TABLEAU R√âCAPITULATIF DES SCORES

| N-gram | Nb mots | Match exact | Match fuzzy | Score |
|--------|---------|-------------|-------------|-------|
| "veux taille 5" | 3 | 5 √ó 3 = 15 | 0.5 √ó 3 = 1.5 | 15 ou 1.5 |
| "taille 5" | 2 | 5 √ó 2 = 10 | 0.5 √ó 2 = 1.0 | 10 ou 1.0 |
| "taille" | 1 | 5 √ó 1 = 5 | 0.5 √ó 1 = 0.5 | 5 ou 0.5 |

**‚úÖ HI√âRARCHIE RESPECT√âE**: N-gram 3 > N-gram 2 > N-gram 1
