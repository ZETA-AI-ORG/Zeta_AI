"""
Script de test simple pour v√©rifier :
1. Le remplacement des placeholders (context, history, question)
2. Le prompt final envoy√© au LLM
3. La r√©ponse du LLM (parsing de <thinking> et <response>)
"""

import os
from dotenv import load_dotenv

load_dotenv()

# ========================================
# 1. PROMPT SYST√àME (charg√© depuis fichier)
# ========================================
def load_system_prompt():
    """Charge le prompt depuis PROMPT_SYSTEM_FINAL_V2.txt"""
    with open("PROMPT_SYSTEM_FINAL_V2.txt", "r", encoding="utf-8") as f:
        return f.read()

SYSTEM_PROMPT_TEMPLATE = """Tu es Jessica, assistant(e) client de RUEDUGROSSISTE (secteur : Commerce de d√©tail et gros de produits pour b√©b√© - couches).

INFOS CL√âS ENTREPRISE :
- Nom commercial : RUEDUGROSSISTE
- Secteur : Commerce de d√©tail et gros de produits pour b√©b√© (couches)
- Description : Vente en gros et au d√©tail de couches pour b√©b√©
- Adresse : Boutique 100% en ligne (pas de magasin physique)
- Zone(s) de livraison : Abidjan et toutes les grandes villes de C√¥te d'Ivoire
- WhatsApp : +225 0160924560
- Paiement Wave : +225 0787360757
- Horaires : Toujours ouvert (24/7)

---

<context>
{context}
</context>

<history>
{history}
</history>

<question>
{question}
</question>

---

INFOS √Ä RECUEILLIR AVANT VALIDATION COMMANDE (ordre flexible, toutes obligatoires) :
1. ‚òê Produit : Type, Variants, Quantit√©, Prix exact
2. ‚òê Coordonn√©es : Num√©ro de t√©l√©phone
3. ‚òê Livraison : Ville/Commune, Adresse, Frais
4. ‚òê Paiement : Wave, Acompte 2 000 FCFA, Solde livraison
5. ‚òê Validation : R√©capitulatif, Total, Confirmation

---

PROCESSUS DE RAISONNEMENT OBLIGATOIRE :

<thinking>
PHASE 1 - ANALYSE DE LA QUESTION
- Question client : [r√©p√©ter]
- Type : [prix/disponibilit√©/livraison/contact/commande/autre]
- Mots-cl√©s : [liste]
- Infos collect√©es : [liste avec ‚òë]
- Infos manquantes : [liste avec ‚òê]

PHASE 2 - COMPARAISON QUESTION ‚Üî SOURCES
Pour chaque mot-cl√© :
- <context> : [MATCH ligne X / ABSENT]
- <history> : [MATCH / ABSENT]

R√©sultat :
  ‚úì Mot-cl√© 1 : MATCH ‚Üí [citation]
  ‚úó Mot-cl√© 2 : ABSENT

PHASE 3 - VALIDATION STRICTE
- Infos trouv√©es ? [oui/non]
- Donn√©es exactes ? [oui/non]
- V√©rifi√© 2 fois ? [oui/non]
- Certain √† 100% ? [oui/non]
- Confiance : [√âLEV√â/MOYEN/FAIBLE]
</thinking>

<response>
[Ton message clair et professionnel bas√© UNIQUEMENT sur <context> et <history>]
</response>

---

R√àGLES CRITIQUES :
1. Prix/Produits/Livraison ‚Üí <context> UNIQUEMENT
2. T√©l√©phone/Adresse client ‚Üí <history> UNIQUEMENT
3. NE JAMAIS INVENTER : Si absent, demande clarification
4. Ton chaleureux et professionnel
"""


# ========================================
# 2. SIMULATION DES DONN√âES
# ========================================
def simulate_data():
    """Simule les donn√©es qui seraient r√©cup√©r√©es en production"""
    
    # Contexte : documents pertinents r√©cup√©r√©s par RAG
    context = """PRODUIT: Couches √† pression
- Taille 1 (0-4kg): 17 000 FCFA / 300 pi√®ces
- Taille 2 (3-8kg): 18 500 FCFA / 300 pi√®ces
- Taille 3 (6-11kg): 20 500 FCFA / 300 pi√®ces
- Taille 4 (9-14kg): 22 500 FCFA / 300 pi√®ces
- Taille 5 (12-17kg): 24 000 FCFA / 300 pi√®ces
- Taille 6 (17-25kg): 24 500 FCFA / 300 pi√®ces

LIVRAISON:
- Yopougon, Cocody, Plateau: 1 500 FCFA
- Port-Bou√´t, Bingerville: 2 000 FCFA

NOTES: Vendu UNIQUEMENT par lots de 150 ou 300 pi√®ces."""
    
    # Historique : conversation pass√©e (si existe)
    history = """user: Bonjour
assistant: Bonjour ! Je suis Jessica de RUEDUGROSSISTE. Comment puis-je vous aider aujourd'hui ?"""
    
    # Question actuelle
    question = "Prix d'un lot de 300 couches taille 4 pression?"
    
    return context, history, question


# ========================================
# 3. ASSEMBLAGE DU PROMPT FINAL
# ========================================
def build_final_prompt(context: str, history: str, question: str) -> str:
    """Remplace les placeholders et retourne le prompt final"""
    final_prompt = SYSTEM_PROMPT_TEMPLATE.format(
        context=context,
        history=history,
        question=question
    )
    return final_prompt


# ========================================
# 4. PARSING DE LA R√âPONSE LLM
# ========================================
def parse_llm_response(llm_output: str) -> dict:
    """Extrait <thinking> et <response> de la sortie LLM"""
    import re
    
    # Extraire <thinking>
    thinking_match = re.search(r'<thinking>(.*?)</thinking>', llm_output, re.DOTALL)
    thinking = thinking_match.group(1).strip() if thinking_match else "Non trouv√©"
    
    # Extraire <response>
    response_match = re.search(r'<response>(.*?)</response>', llm_output, re.DOTALL)
    response = response_match.group(1).strip() if response_match else llm_output
    
    return {
        "thinking": thinking,
        "response": response
    }


# ========================================
# 5. APPEL LLM (OPTIONNEL - avec Groq)
# ========================================
def call_llm(prompt: str) -> str:
    """Envoie le prompt au LLM et retourne la r√©ponse (optionnel)"""
    try:
        from groq import Groq
        
        client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        
        completion = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": prompt},
            ],
            temperature=0.3,
            max_tokens=2000,
        )
        
        return completion.choices[0].message.content
    
    except Exception as e:
        return f"[ERREUR LLM] {str(e)}"


# ========================================
# 6. TEST PRINCIPAL
# ========================================
def main():
    print("="*80)
    print("üß™ TEST ASSEMBLAGE PROMPT + V√âRIFICATION LLM (VERSION V2)")
    print("="*80)
    
    # Charger le prompt depuis le fichier
    print("\nüìã CHARGEMENT DU PROMPT SYST√àME...")
    print("-"*80)
    global SYSTEM_PROMPT_TEMPLATE
    try:
        SYSTEM_PROMPT_TEMPLATE = load_system_prompt()
        print(f"‚úÖ Prompt charg√© depuis PROMPT_SYSTEM_FINAL_V2.txt ({len(SYSTEM_PROMPT_TEMPLATE)} caract√®res)")
    except FileNotFoundError:
        print("‚ö†Ô∏è Fichier PROMPT_SYSTEM_FINAL_V2.txt non trouv√©, utilisation du prompt par d√©faut")
    
    # √âtape 1 : Simuler les donn√©es
    print("\nüìä √âTAPE 1 : SIMULATION DES DONN√âES")
    print("-"*80)
    context, history, question = simulate_data()
    
    print(f"‚úÖ Context r√©cup√©r√© : {len(context)} caract√®res")
    print(f"‚úÖ History r√©cup√©r√© : {len(history)} caract√®res")
    print(f"‚úÖ Question : {question}")
    
    # √âtape 2 : Assembler le prompt final
    print("\nüîß √âTAPE 2 : ASSEMBLAGE DU PROMPT FINAL")
    print("-"*80)
    final_prompt = build_final_prompt(context, history, question)
    print(f"‚úÖ Prompt final assembl√© : {len(final_prompt)} caract√®res")
    
    # Afficher un extrait du prompt final
    print("\nüìã EXTRAIT DU PROMPT FINAL (premiers 1000 caract√®res) :")
    print("-"*80)
    print(final_prompt[:1000] + "...")
    
    # V√©rifier que les placeholders ont √©t√© remplac√©s
    print("\n‚úÖ V√âRIFICATION DES PLACEHOLDERS :")
    print("-"*80)
    if "{context}" in final_prompt:
        print("‚ùå ERREUR : {context} n'a pas √©t√© remplac√© !")
    else:
        print("‚úÖ {context} remplac√© correctement")
    
    if "{history}" in final_prompt:
        print("‚ùå ERREUR : {history} n'a pas √©t√© remplac√© !")
    else:
        print("‚úÖ {history} remplac√© correctement")
    
    if "{question}" in final_prompt:
        print("‚ùå ERREUR : {question} n'a pas √©t√© remplac√© !")
    else:
        print("‚úÖ {question} remplac√© correctement")
    
    # √âtape 3 : Appeler le LLM (si cl√© API disponible)
    print("\nü§ñ √âTAPE 3 : APPEL LLM (optionnel)")
    print("-"*80)
    
    if os.getenv("GROQ_API_KEY"):
        print("üîÑ Envoi du prompt au LLM...")
        llm_output = call_llm(final_prompt)
        
        # Parser la r√©ponse
        parsed = parse_llm_response(llm_output)
        
        print("\nüìä R√âPONSE LLM BRUTE (premiers 500 caract√®res) :")
        print("-"*80)
        print(llm_output[:500] + "...")
        
        print("\nüß† <thinking> EXTRAIT :")
        print("-"*80)
        print(parsed["thinking"][:500] + "..." if len(parsed["thinking"]) > 500 else parsed["thinking"])
        
        print("\nüí¨ <response> EXTRAIT (√† envoyer au client) :")
        print("-"*80)
        print(parsed["response"])
        
        # V√©rifier si le LLM voit bien les donn√©es
        print("\n‚úÖ V√âRIFICATION : LE LLM VOIT-IL LES DONN√âES ?")
        print("-"*80)
        if "22 500" in llm_output or "22500" in llm_output:
            print("‚úÖ Le LLM a bien vu le prix dans <context> (22 500 FCFA)")
        else:
            print("‚ö†Ô∏è Le LLM n'a pas mentionn√© le prix exact")
        
        if "taille 4" in llm_output.lower():
            print("‚úÖ Le LLM a bien vu 'taille 4' dans <context>")
        else:
            print("‚ö†Ô∏è Le LLM n'a pas mentionn√© 'taille 4'")
        
        if "300" in llm_output:
            print("‚úÖ Le LLM a bien vu '300 pi√®ces' dans <context>")
        else:
            print("‚ö†Ô∏è Le LLM n'a pas mentionn√© '300'")
    
    else:
        print("‚ö†Ô∏è GROQ_API_KEY non trouv√©e dans .env")
        print("   Pour tester l'appel LLM, ajoute GROQ_API_KEY dans .env")
    
    print("\n" + "="*80)
    print("‚úÖ TEST TERMIN√â")
    print("="*80)


if __name__ == "__main__":
    main()
