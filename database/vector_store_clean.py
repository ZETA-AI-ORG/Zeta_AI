"""
üéØ MEILISEARCH PROPRE - DOCUMENTS COMPLETS UNIQUEMENT
Version √©pur√©e qui respecte l'architecture par documents complets
"""
import os
import logging
from typing import List, Optional
try:
    import meilisearch
except ImportError:
    meilisearch = None
from utils import log3

# Import du diagnostic logger
try:
    from core.diagnostic_logger import diagnostic_logger
except ImportError:
    # Fallback si le module n'est pas disponible
    class MockDiagnosticLogger:
        def log_index_detailed_results(self, *args, **kwargs): pass
        def log_final_selection(self, *args, **kwargs): pass
        def log_routing_analysis(self, *args, **kwargs): pass
        def log_document_scoring_process(self, *args, **kwargs): pass
    diagnostic_logger = MockDiagnosticLogger()

logger = logging.getLogger(__name__)


def get_meilisearch_client():
    """Obtenir le client MeiliSearch"""
    try:
        meili_url = os.environ.get("MEILI_URL", "http://127.0.0.1:7700")
        meili_key = os.environ.get("MEILI_API_KEY") or os.environ.get("MEILI_MASTER_KEY", "")
        
        return meilisearch.Client(meili_url, meili_key)
    except Exception as e:
        log3("[MEILI_CLEAN]", f"‚ùå Erreur client: {e}")
        return None


def get_available_indexes(company_id: str) -> List[str]:
    """Obtenir la liste des indexes disponibles pour une entreprise"""
    try:
        client = get_meilisearch_client()
        if not client:
            return []
        
        # Lister tous les indexes
        indexes_response = client.get_indexes()
        
        # G√©rer diff√©rents formats de r√©ponse
        if isinstance(indexes_response, dict):
            indexes = indexes_response.get('results', [])
        else:
            indexes = indexes_response
        
        company_indexes = []
        for index in indexes:
            # Extraire l'uid selon le format
            if isinstance(index, dict):
                index_uid = index.get('uid', '')
            else:
                index_uid = getattr(index, 'uid', '')
            
            # Filtrer par company_id
            if company_id in index_uid:
                company_indexes.append(index_uid)
        
        log3("[MEILI_CLEAN]", f"üìÇ {len(company_indexes)} indexes trouv√©s pour {company_id[:8]}...")
        return company_indexes
        
    except Exception as e:
        log3("[MEILI_CLEAN]", f"‚ùå Erreur indexes: {e}")
        return []


async def search_meili_keywords(query: str, company_id: str, target_indexes: List[str] = None, limit: int = 10) -> str:
    """
    üéØ RECHERCHE MEILISEARCH INTELLIGENTE - FILTRAGE PAR INDEX
    
    Architecture:
    - Recherche par mots-cl√©s dans les documents index√©s
    - Filtrage intelligent par index (garder le meilleur de chaque)
    - Format structur√© pour √©viter les hallucinations LLM
    - Suppression des index majuscules corrompus
    """
    print("\n" + "="*80)
    print("üîç MEILISEARCH - D√âBUT DE LA RECHERCHE")
    print("="*80)
    
    if not query or not query.strip():
        print("‚ùå MEILISEARCH: Query vide")
        return ""
    
    # Initialisation de filtered_query
    filtered_query = query
    
    # FALLBACK de s√©curit√©
    if not filtered_query or len(filtered_query.strip()) < 2:
        print(f"‚ö†Ô∏è MEILISEARCH: Filtrage trop agressif, utilisation query originale")
        filtered_query = query
    else:
        print(f"‚úÖ MEILISEARCH: Query filtr√©e utilis√©e")
    
    # G√âN√âRATION DES REQU√äTES MULTIPLES (STRAT√âGIE INTELLIGENTE)
    print(f"üîÑ MEILISEARCH: G√©n√©ration des combinaisons intelligentes")
    search_queries = _generate_intelligent_queries(filtered_query)
    print(f"üìä MEILISEARCH: {len(search_queries)} requ√™tes g√©n√©r√©es")
    
    # LOGS D√âTAILL√âS - Combinaisons de requ√™tes
    diagnostic_logger.log_query_combinations(search_queries, len(search_queries))
    
    # Obtenir le client
    client = get_meilisearch_client()
    if not client:
        print("‚ùå MEILISEARCH: Client non disponible")
        return ""
    
    print("‚úÖ MEILISEARCH: Client initialis√© avec succ√®s")
    
    # INDEX PRINCIPAUX (PRIORIT√âS) - COMPANY_DOCS EXCLU (fallback seulement)
    main_indexes = [
        f"products_{company_id}",        # PRIORIT√â 1
        f"delivery_{company_id}",        # PRIORIT√â 2  
        f"localisation_{company_id}",    # PRIORIT√â 3
        f"support_paiement_{company_id}" # PRIORIT√â 4
    ]
    
    # INDEX FALLBACK (uniquement si rien trouv√©)
    fallback_index = f"company_docs_{company_id}"
    
    print(f"üìã MEILISEARCH: Index principaux (par priorit√©) = {main_indexes}")
    print(f"üîÑ MEILISEARCH: Index fallback = {fallback_index}")
    
    # Filtrer les target_indexes pour ne garder que les principaux
    if target_indexes:
        indexes_to_search = [idx for idx in target_indexes if idx in main_indexes]
        print(f"üéØ MEILISEARCH: Index cibl√©s = {target_indexes}")
        print(f"‚úÖ MEILISEARCH: Index principaux apr√®s filtrage = {indexes_to_search}")
    else:
        indexes_to_search = main_indexes
        print(f"üîÑ MEILISEARCH: Utilisation de tous les index principaux = {indexes_to_search}")
    
    if not indexes_to_search:
        print("‚ùå MEILISEARCH: Aucun index principal disponible")
        return ""
    
    print(f"üìÇ MEILISEARCH: Recherche dans {len(indexes_to_search)} indexes principaux")
    
    # Dictionnaire pour grouper les r√©sultats par index
    results_by_index = {}
    unique_contents = set()
    
    # RECHERCHE PARALL√àLE AVEC REQU√äTES MULTIPLES
    print(f"\nüöÄ MEILISEARCH: Recherche parall√®le avec {len(search_queries)} requ√™tes")
    
    import asyncio
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    def search_single_query_index(query_text, index_name):
        """Recherche une requ√™te sur un index donn√©"""
        try:
            index = client.index(index_name)
            search_params = {
                'limit': 5,
                'attributesToRetrieve': ['content', 'id', 'type', 'document_id', 'file_name'],
                'attributesToHighlight': [],
                'showMatchesPosition': False,
                'attributesToSearchOn': ['content']
            }
            
            search_results = index.search(query_text, search_params)
            hits = search_results.get('hits', []) if isinstance(search_results, dict) else []
            
            return {
                'query': query_text,
                'index': index_name,
                'hits': hits,
                'success': True
            }
        except Exception as e:
            return {
                'query': query_text,
                'index': index_name,
                'hits': [],
                'success': False,
                'error': str(e)
            }
    
    # Pr√©parer toutes les t√¢ches (requ√™tes √ó indexes)
    all_tasks = []
    for query_text in search_queries:
        for index_name in indexes_to_search:
            all_tasks.append((query_text, index_name))
    
    print(f"‚ö° MEILISEARCH: {len(all_tasks)} requ√™tes parall√®les √† ex√©cuter")
    
    # Ex√©cution parall√®le
    all_results = []
    with ThreadPoolExecutor(max_workers=10) as executor:
        future_to_task = {
            executor.submit(search_single_query_index, query_text, index_name): (query_text, index_name)
            for query_text, index_name in all_tasks
        }
        
        for future in as_completed(future_to_task):
            result = future.result()
            all_results.append(result)
    
    print(f"‚úÖ MEILISEARCH: {len(all_results)} requ√™tes termin√©es")
    
    # Traitement des r√©sultats parall√®les
    hits_by_index = {}
    total_hits_found = 0
    
    for result in all_results:
        if result['success'] and result['hits']:
            index_name = result['index']
            query_text = result['query']
            hits = result['hits']
            
            if index_name not in hits_by_index:
                hits_by_index[index_name] = []
            
            # Ajouter les hits avec info sur la requ√™te qui les a trouv√©s
            for hit in hits:
                hit['_search_query'] = query_text
                hits_by_index[index_name].append(hit)
            
            total_hits_found += len(hits)
            print(f"üìä MEILISEARCH: '{query_text}' ‚Üí {index_name} ‚Üí {len(hits)} hits")
    
    print(f"üéØ MEILISEARCH: Total hits collect√©s = {total_hits_found}")
    
    # PRIORISATION DES INDEX (plus le score est √©lev√©, plus l'index est prioritaire)
    index_priorities = {
        f"products_{company_id}": 4,        # PRIORIT√â 1 (score le plus √©lev√©)
        f"delivery_{company_id}": 3,        # PRIORIT√â 2
        f"localisation_{company_id}": 2,    # PRIORIT√â 3
        f"support_paiement_{company_id}": 1 # PRIORIT√â 4
    }
    
    # Traitement intelligent par index (dans l'ordre de priorit√©)
    sorted_indexes = sorted(hits_by_index.items(), 
                           key=lambda x: index_priorities.get(x[0], 0), 
                           reverse=True)
    
    for index_name, all_hits in sorted_indexes:
        priority_score = index_priorities.get(index_name, 0)
        print(f"\n--- TRAITEMENT INDEX: {index_name} (priorit√©: {priority_score}) ({len(all_hits)} hits) ---")
        
        # D√©duplication par contenu
        unique_hits = {}
        for hit in all_hits:
            content = hit.get('content', '').strip()
            if content and len(content) > 50:
                if content not in unique_hits:
                    unique_hits[content] = hit
                else:
                    # Garder le hit avec la requ√™te la plus compl√®te
                    current_query = unique_hits[content].get('_search_query', '')
                    new_query = hit.get('_search_query', '')
                    if len(new_query) > len(current_query):
                        unique_hits[content] = hit
        
        # Scoring intelligent des hits uniques
        scored_hits = []
        for content, hit in unique_hits.items():
            search_query_used = hit.get('_search_query', filtered_query)
            query_words = [w.lower().strip() for w in search_query_used.split() if len(w.strip()) >= 2]
            content_lower = content.lower()
                    
            # Calculer le score
            keyword_score = 0
            matched_keywords = []
                    
            for keyword in query_words:
                if keyword in content_lower:
                    keyword_score += 1
                    matched_keywords.append(keyword)
                    # Bonus pour r√©p√©titions
                    occurrences = content_lower.count(keyword)
                    if occurrences > 1:
                        keyword_score += min(occurrences - 1, 3) * 0.5
            
            # Score minimum pour tous les documents trouv√©s par MeiliSearch
            base_score = keyword_score if keyword_score > 0 else 0.5
            
            # BONUS DE PRIORIT√â : les index prioritaires obtiennent un bonus
            priority_bonus = index_priorities.get(index_name, 0) * 0.1
            final_score = base_score + priority_bonus
            
            scored_hits.append({
                'hit': hit,
                'content': content,
                'score': final_score,
                'matched_keywords': matched_keywords,
                'search_query': search_query_used
            })
        
        # Trier par score et garder les 2 meilleurs
        scored_hits.sort(key=lambda x: x['score'], reverse=True)
        
        if scored_hits:
            # GARDER MAXIMUM 3 DOCUMENTS PAR INDEX (pour couvrir tous les produits)
            max_docs_per_index = 3
            selected_hits = scored_hits[:max_docs_per_index]
            
            print(f"üèÜ MEILISEARCH: {len(selected_hits)} meilleurs hits pour '{index_name}'")
            
            # LOGS D√âTAILL√âS - Afficher le contenu complet de chaque document
            print(f"\n{'='*80}")
            print(f"üîç D√âTAIL DOCUMENTS TROUV√âS - INDEX: {index_name}")
            print(f"üìä TOTAL DOCUMENTS S√âLECTIONN√âS: {len(selected_hits)}")
            print(f"{'='*80}")
            
            # Traiter chaque document s√©lectionn√©
            for i, best_hit in enumerate(selected_hits, 1):
                content = best_hit['content']
                hit = best_hit['hit']
                
                print(f"\nüìÑ DOCUMENT {i}/{len(selected_hits)}")
                print(f"   üÜî ID: {hit.get('id', 'N/A')}")
                print(f"   ‚≠ê SCORE: {best_hit['score']:.1f}")
                print(f"   üè∑Ô∏è TYPE: {hit.get('type', 'N/A')}")
                print(f"   üîç REQU√äTE ORIGINE: '{best_hit['search_query']}'")
                print(f"   üìè TAILLE CONTENU: {len(content)} caract√®res")
                print(f"   üìù CONTENU COMPLET:")
                print(f"   {'-'*60}")
                print(f"   {content}")
                print(f"   {'-'*60}")
                
                # M√©tadonn√©es additionnelles
                metadata = {k: v for k, v in hit.items() if k not in ['searchable_text', 'content', '_score']}
                if metadata:
                    print(f"   üìã M√âTADONN√âES: {metadata}")
                
                # Analyse de pertinence
                matched_keywords = best_hit.get('matched_keywords', [])
                if matched_keywords:
                    print(f"   üéØ MOTS-CL√âS TROUV√âS: {', '.join(matched_keywords)}")
                else:
                    print(f"   üéØ MOTS-CL√âS TROUV√âS: aucun sp√©cifique")
                    
                if content not in unique_contents:
                    unique_contents.add(content)
                    category = _get_category_from_index(index_name)
                    
                    # Cr√©er une cl√© unique pour chaque document
                    doc_key = f"{index_name}_doc_{i}"
                    
                    results_by_index[doc_key] = {
                        'content': content,
                        'id': hit.get('id', ''),
                        'type': hit.get('type', ''),
                        'document_id': hit.get('document_id', ''),
                        'score': best_hit['score'],
                        'category': category,
                        'search_query': best_hit['search_query'],
                        'index_name': index_name,
                        'doc_rank': i
                    }
                    
                    print(f"   ‚úÖ S√âLECTIONN√â: Document ajout√© aux r√©sultats finaux")
                    print(f"   üéØ RAISON: Score √©lev√© ({best_hit['score']:.1f}) + mots-cl√©s pertinents")
                else:
                    print(f"   ‚ùå REJET√â: Document d√©j√† vu (contenu dupliqu√©)")
                    print(f"   üéØ RAISON: √âviter doublons dans contexte LLM")
            
            print(f"\n{'='*80}\n")
        else:
            print(f"‚ùå MEILISEARCH: Aucun document valide pour '{index_name}'")
    
    # FALLBACK UNIQUEMENT SI AUCUN R√âSULTAT DANS LES INDEX PRINCIPAUX
    if not results_by_index:
        print(f"\nüîÑ MEILISEARCH: Aucun r√©sultat dans les index principaux, fallback vers company_docs")
        try:
            print(f"üîó MEILISEARCH: Connexion au fallback '{fallback_index}'")
            
            # Essayer toutes les requ√™tes sur l'index fallback
            fallback_hits = []
            for query_text in search_queries:
                try:
                    search_results = client.index(fallback_index).search(query_text, {
                        'limit': 3,
                        'attributesToRetrieve': ['content', 'id', 'type', 'document_id', 'file_name'],
                        'attributesToSearchOn': ['content']
                    })
                    
                    hits = search_results.get('hits', []) if isinstance(search_results, dict) else []
                    for hit in hits:
                        hit['_search_query'] = query_text
                        fallback_hits.append(hit)
                    
                    if hits:
                        print(f"üìä MEILISEARCH: Fallback '{query_text}' ‚Üí {len(hits)} hits")
                except Exception as e:
                    print(f"‚ö†Ô∏è MEILISEARCH: Erreur fallback query '{query_text}': {e}")
            
            print(f"üìä MEILISEARCH: Fallback total - {len(fallback_hits)} hits trouv√©s")
            
            if fallback_hits:
                # Prendre le premier r√©sultat le plus long
                best_hit = max(fallback_hits, key=lambda h: len(h.get('content', '')))
                content = best_hit.get('content', '').strip()
                print(f"üìù MEILISEARCH: Fallback - Meilleur contenu = '{content[:100]}...' ({len(content)} chars)")
                
                if content and len(content) > 50:
                    results_by_index[fallback_index] = {
                        'content': content,
                        'id': best_hit.get('id', ''),
                        'type': best_hit.get('type', ''),
                        'document_id': best_hit.get('document_id', ''),
                        'score': 1.0,
                        'category': 'fallback',
                        'search_query': best_hit.get('_search_query', filtered_query)
                    }
                    
                    print(f"‚úÖ MEILISEARCH: Fallback - Document ajout√© ({len(content)} chars)")
                else:
                    print(f"‚ùå MEILISEARCH: Fallback - Document trop court ({len(content)} chars)")
            else:
                print(f"‚ùå MEILISEARCH: Fallback - Aucun hit trouv√©")
                        
        except Exception as e:
            print(f"‚ùå MEILISEARCH: Erreur fallback: {e}")
    
    print(f"\nüìä MEILISEARCH: R√©sultats finaux = {len(results_by_index)} documents")
    for doc_key, doc in results_by_index.items():
        rank_info = f"(rang {doc.get('doc_rank', 1)})" if doc.get('doc_rank') else ""
        print(f"    üìÑ MEILISEARCH: {doc_key} -> {doc['category']} {rank_info} ({len(doc['content'])} chars)")
    
    # LOGS D√âTAILL√âS - Documents finalement s√©lectionn√©s pour le LLM
    if results_by_index:
        print(f"\n{'üéØ'*40}")
        print(f"üì§ DOCUMENTS S√âLECTIONN√âS POUR LE LLM")
        print(f"üìä S√âLECTIONN√âS: {len(results_by_index)} documents au total")
        print(f"{'üéØ'*40}")
        
        for i, (doc_key, doc_data) in enumerate(results_by_index.items(), 1):
            print(f"\n‚úÖ DOCUMENT S√âLECTIONN√â {i}")
            print(f"   üè∑Ô∏è INDEX SOURCE: {doc_data.get('index_name', 'N/A')}")
            print(f"   üÜî ID: {doc_data.get('id', 'N/A')}")
            print(f"   ‚≠ê SCORE FINAL: {doc_data.get('score', 'N/A')}")
            print(f"   üîç REQU√äTE ORIGINE: {doc_data.get('search_query', 'N/A')}")
            print(f"   üìè TAILLE: {len(doc_data.get('content', ''))} chars")
            print(f"   üè∑Ô∏è CAT√âGORIE: {doc_data.get('category', 'N/A')}")
            print(f"   üìù EXTRAIT (100 premiers chars):")
            content = doc_data.get('content', '')[:100]
            print(f"   '{content}...'")
            
            # Analyse de pertinence pour le routing
            category = doc_data.get('category', '')
            score = doc_data.get('score', 0)
            
            if category == 'produits' and score > 3:
                reason = "Pertinent - Produits avec score √©lev√©"
            elif category == 'livraison' and score < 1:
                reason = "‚ö†Ô∏è SUSPECT - Livraison avec score faible (routing √† revoir)"
            elif category == 'localisation' and score < 1:
                reason = "‚ö†Ô∏è SUSPECT - Localisation avec score faible (routing √† revoir)"
            elif category == 'support' and score < 1:
                reason = "‚ö†Ô∏è SUSPECT - Support avec score faible (routing √† revoir)"
            else:
                reason = f"Score acceptable pour {category}"
                
            print(f"   üéØ ANALYSE ROUTING: {reason}")
        
        print(f"\n{'üéØ'*40}\n")
    
    if not results_by_index:
        print("‚ùå MEILISEARCH: Aucun document trouv√©")
        return ""
    
    # Construire le contexte final avec format structur√©
    print(f"\nüîß MEILISEARCH: Construction du contexte final")
    context_parts = []
    
    # Grouper par index pour un formatage coh√©rent
    docs_by_original_index = {}
    for doc_key, doc_data in results_by_index.items():
        original_index = doc_data.get('index_name', doc_key.split('_doc_')[0] if '_doc_' in doc_key else doc_key)
        if original_index not in docs_by_original_index:
            docs_by_original_index[original_index] = []
        docs_by_original_index[original_index].append(doc_data)
    
    # Formater par index avec num√©rotation
    for original_index, docs in docs_by_original_index.items():
        for i, doc_data in enumerate(docs, 1):
            category = doc_data['category']
            content = doc_data['content']
            
            # Format structur√© pour √©viter les hallucinations LLM
            if category == 'fallback':
                if len(docs) > 1:
                    formatted = f"POUR (informations g√©n√©rales) - Index: {original_index} - Document {i}/{len(docs)} :\n{content}"
                else:
                    formatted = f"POUR (informations g√©n√©rales) - Index: {original_index} - voici le document trouv√© :\n{content}"
            else:
                if len(docs) > 1:
                    formatted = f"POUR ({category}) - Index: {original_index} - Document {i}/{len(docs)} :\n{content}"
                else:
                    formatted = f"POUR ({category}) - Index: {original_index} - voici le document trouv√© :\n{content}"
            
            context_parts.append(formatted)
            print(f"    üìù MEILISEARCH: Format√© {category} doc {i} -> {len(formatted)} chars")
    
    final_context = "\n\n".join(context_parts)
    
    print(f"\n‚úÖ MEILISEARCH: Contexte final = {len(final_context)} chars total")
    
    # LOGS D√âTAILL√âS - Analyse globale du routing
    diagnostic_logger.log_routing_analysis(query, docs_by_original_index)
    
    # üß† D√âTECTION D'INTENTION INTELLIGENTE
    try:
        from core.intention_detector import detect_user_intention
        
        # Extraire les mots qui ont surv√©cu au filtrage stop words
        filtered_words = []
        for doc_data in results_by_index.values():
            search_query = doc_data.get('search_query', '')
            if search_query and search_query != query:
                filtered_words.extend(search_query.split())
        
        # D√©tecter l'intention
        intention_result = detect_user_intention(query, docs_by_original_index, filtered_words)
        
        print(f"\nüß† D√âTECTION D'INTENTION INTELLIGENTE")
        print(f"{'üß†'*50}")
        print(f"üéØ SUGGESTION LLM: {intention_result['llm_suggestion']}")
        print(f"üìä CONFIANCE GLOBALE: {intention_result['confidence_score']:.1%}")
        
        if intention_result['primary_intentions']:
            print(f"üîç INTENTIONS D√âTECT√âES:")
            for intent, score in sorted(intention_result['primary_intentions'].items(), key=lambda x: x[1], reverse=True):
                print(f"   ‚Ä¢ {intent}: {score:.1%}")
        
        if intention_result['complex_intentions']:
            print(f"üîó INTENTIONS COMBIN√âES:")
            for intent, score in sorted(intention_result['complex_intentions'].items(), key=lambda x: x[1], reverse=True):
                print(f"   ‚Ä¢ {intent}: {score:.1%}")
        
        print(f"{'üß†'*50}")
        
        # Ajouter la suggestion d'intention au contexte final
        intention_context = f"\n\nüß† ANALYSE D'INTENTION:\n{intention_result['llm_suggestion']}\n"
        final_context += intention_context
        
    except ImportError:
        print("‚ö†Ô∏è D√©tecteur d'intention non disponible")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur d√©tection d'intention: {e}")
    
    # LOGS D√âTAILL√âS - Documents finalement s√©lectionn√©s pour le LLM
    final_docs_list = []
    for doc_key, doc_data in results_by_index.items():
        final_docs_list.append({
            'id': doc_data.get('id', ''),
            'score': doc_data.get('score', 0),
            'content': doc_data.get('content', ''),
            'category': doc_data.get('category', ''),
            'index_name': doc_data.get('index_name', ''),
            'search_query': doc_data.get('search_query', '')
        })
    
    diagnostic_logger.log_final_selection(final_docs_list, len(results_by_index), query)
    
    print("="*80)
    print("üîç MEILISEARCH - FIN DE LA RECHERCHE")
    print("="*80)
    
    return final_context


def _generate_intelligent_queries(query: str) -> List[str]:
    """
    G√©n√®re des requ√™tes multiples selon la STRAT√âGIE INTELLIGENTE :
    1. Requ√™te compl√®te
    2. Mots individuels (filtr√©s)
    3. Bigrammes dans l'ordre (filtr√©s)
    4. Trigrammes dans l'ordre (filtr√©s)
    """
    # STOP WORDS √âTENDUS - Ponctuation et mots g√©n√©riques
    EXTENDED_STOP_WORDS = {
        # Ponctuation & symboles
        '?', '!', '.', ',', ';', ':', '(', ')', '[', ']', '{', '}',
        '"', "'", '`', '-', '_', '+', '=', '*', '/', '\\', '|', '@',
        '#', '$', '%', '^', '&', '~',
        
        # Articles fran√ßais
        'le', 'la', 'les', 'un', 'une', 'des', 'du', 'de', 'des',
        
        # Pr√©positions
        '√†', 'au', 'aux', 'avec', 'dans', 'sur', 'sous', 'pour', 'par', 'sans',
        
        # Conjonctions
        'et', 'ou', 'mais', 'donc', 'or', 'ni', 'car',
        
        # Pronoms
        'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles',
        'me', 'te', 'se', 'lui', 'leur',
        
        # Mots de liaison
        'que', 'qui', 'dont', 'o√π', 'quand', 'comment', 'pourquoi',
        
        # Mots conversationnels
        'salut', 'bonjour', 'bonsoir', 'merci', 'svp', 'stp',
        'dite', 'dis', 'dit', 'alors', 'bon', 'bien', 'voil√†',
        
        # Mots vagues
        'ca', '√ßa', 'cela', 'ceci', 'chose', 'truc', 'machin',
        
        # Interjections
        'ah', 'oh', 'eh', 'hein', 'euh', 'hum', 'ben', 'bah', 'enfin', 'quoi'
    }
    
    def is_meaningful_word(word: str) -> bool:
        """V√©rifie si un mot est significatif (pas un stop word)"""
        return (
            word.lower() not in EXTENDED_STOP_WORDS and
            len(word) > 1 and  # Au moins 2 caract√®res
            not word.isdigit() or len(word) > 2  # Chiffres OK si > 2 chars (ex: "13kg")
        )
    
    def is_meaningful_phrase(phrase: str) -> bool:
        """V√©rifie si une phrase contient au moins un mot significatif"""
        words_in_phrase = phrase.split()
        return any(is_meaningful_word(word) for word in words_in_phrase)
    
    words = [w.strip() for w in query.split() if w.strip()]
    if not words:
        return [query]
    
    queries = []
    
    # 1. REQU√äTE COMPL√àTE (toujours incluse)
    queries.append(query)
    
    # 2. MOTS INDIVIDUELS (filtr√©s)
    for word in words:
        if is_meaningful_word(word) and word not in queries:
            queries.append(word)
    
    # 3. BIGRAMMES DANS L'ORDRE (filtr√©s)
    for i in range(len(words) - 1):
        bigram = f"{words[i]} {words[i+1]}"
        if is_meaningful_phrase(bigram) and bigram not in queries:
            queries.append(bigram)
    
    # 4. TRIGRAMMES DANS L'ORDRE (filtr√©s)
    for i in range(len(words) - 2):
        trigram = f"{words[i]} {words[i+1]} {words[i+2]}"
        if is_meaningful_phrase(trigram) and trigram not in queries:
            queries.append(trigram)
    
    # Filtrage final : supprimer les requ√™tes non significatives
    filtered_queries = []
    for q in queries:
        if q == query or is_meaningful_phrase(q):  # Garder requ√™te originale + phrases significatives
            filtered_queries.append(q)
    
    print(f"üßπ MEILISEARCH: Requ√™tes filtr√©es: {len(queries)} ‚Üí {len(filtered_queries)} (stop words supprim√©s)")
    
    # Limiter √† 20 requ√™tes max pour √©viter la surcharge
    return filtered_queries[:20]


def _get_category_from_index(index_name: str) -> str:
    """D√©termine la cat√©gorie selon le nom de l'index"""
    if 'products' in index_name:
        return 'produits'
    elif 'delivery' in index_name:
        return 'livraison'
    elif 'support' in index_name:
        return 'support'
    elif 'localisation' in index_name:
        return 'localisation'
    elif 'company_docs' in index_name:
        return 'fallback'
    else:
        return 'document'


# === UTILS EXTRACTION/TAGGING RETROACTIF ===
def get_all_documents_for_company(company_id: str, index_name: str, limit: int = 10000) -> list:
    """
    R√©cup√®re tous les documents d'un index MeiliSearch pour une entreprise donn√©e.
    """
    client = get_meilisearch_client()
    if not client:
        return []
    try:
        index = client.index(index_name)
        results = index.get_documents({'limit': limit})
        
        # G√©rer diff√©rents formats de retour MeiliSearch
        if hasattr(results, 'results'):
            # Format DocumentsResults avec attribut results
            docs_list = results.results
        elif isinstance(results, list):
            # Format liste directe
            docs_list = results
        else:
            # Essayer de convertir en liste
            docs_list = list(results)
        
        # Convertir les objets Document en dictionnaires et filtrer par company_id
        docs = []
        for doc in docs_list:
            # Convertir l'objet Document en dictionnaire
            if hasattr(doc, '__dict__'):
                doc_dict = doc.__dict__
            elif hasattr(doc, 'to_dict'):
                doc_dict = doc.to_dict()
            else:
                # Essayer d'acc√©der aux attributs directement
                doc_dict = {
                    'id': getattr(doc, 'id', None),
                    'content': getattr(doc, 'content', ''),
                    'company_id': getattr(doc, 'company_id', None),
                    'type': getattr(doc, 'type', None),
                    'file_name': getattr(doc, 'file_name', None)
                }
            
            # Filtrer par company_id
            if doc_dict.get('company_id') == company_id:
                docs.append(doc_dict)
        
        print(f"[INFO] {len(docs)} documents trouv√©s apr√®s filtrage company_id")
        return docs
    except Exception as e:
        print(f"Erreur r√©cup√©ration documents: {e}")
        return []

def update_document_entities(doc_id: str, entities: dict, index_name: str):
    """
    Met √† jour le champ 'entities' d'un document MeiliSearch par son id.
    """
    client = get_meilisearch_client()
    if not client:
        return
    try:
        index = client.index(index_name)
        index.update_documents([{'id': doc_id, 'entities': entities}])
    except Exception as e:
        print(f"Erreur mise √† jour entit√©s: {e}")
