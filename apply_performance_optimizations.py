"""
ðŸš€ SCRIPT D'APPLICATION DES OPTIMISATIONS PERFORMANCE
Lance ce script pour appliquer les optimisations sans toucher au format LLM
"""
import asyncio
import sys
from pathlib import Path

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent))

from core.performance_optimizer import initialize_performance_optimizations, get_performance_optimizer
from core.global_embedding_cache import GlobalEmbeddingCache
from utils import log3


async def main():
    """Applique les optimisations de performance"""
    
    print("=" * 80)
    print("ðŸš€ APPLICATION DES OPTIMISATIONS PERFORMANCE")
    print("=" * 80)
    print()
    
    # 1. Initialiser l'optimiseur
    print("ðŸ“¦ Ã‰tape 1: Initialisation de l'optimiseur...")
    optimizer = await initialize_performance_optimizations()
    print("âœ… Optimiseur initialisÃ©")
    print()
    
    # 2. VÃ©rifier le cache embedding
    print("ðŸ“¦ Ã‰tape 2: VÃ©rification du cache embedding...")
    cache = GlobalEmbeddingCache()
    stats = cache.get_stats()
    print(f"   - ModÃ¨les chargÃ©s: {stats['models_loaded']}")
    print(f"   - Embeddings en cache: {stats['embeddings_cached']}")
    print(f"   - Taux de hit modÃ¨le: {stats['model_hit_rate']:.1f}%")
    print(f"   - Taux de hit embedding: {stats['embedding_hit_rate']:.1f}%")
    print("âœ… Cache embedding vÃ©rifiÃ©")
    print()
    
    # 3. Afficher les stats de l'optimiseur
    print("ðŸ“¦ Ã‰tape 3: Statistiques de l'optimiseur...")
    opt_stats = optimizer.get_stats()
    print(f"   - ModÃ¨les prÃ©chargÃ©s: {opt_stats['preloaded_models']}")
    print(f"   - Prompts en cache: {opt_stats['prompt_cache_size']}")
    print(f"   - Connexions HTTP max: {opt_stats['http_connections']['max']}")
    print(f"   - HTTP/2 activÃ©: {opt_stats['http_connections']['http2_enabled']}")
    print("âœ… Optimiseur configurÃ©")
    print()
    
    # 4. Test de performance
    print("ðŸ“¦ Ã‰tape 4: Test de performance...")
    import time
    
    # Test embedding
    start = time.time()
    embedding = await cache.get_embedding("test query", "sentence-transformers/all-MiniLM-L6-v2")
    first_time = time.time() - start
    print(f"   - Premier embedding: {first_time*1000:.1f}ms")
    
    # Test cache hit
    start = time.time()
    embedding = await cache.get_embedding("test query", "sentence-transformers/all-MiniLM-L6-v2")
    cached_time = time.time() - start
    print(f"   - Embedding cachÃ©: {cached_time*1000:.1f}ms")
    print(f"   - Gain: {(first_time - cached_time)*1000:.1f}ms ({(1 - cached_time/first_time)*100:.0f}%)")
    print("âœ… Test de performance terminÃ©")
    print()
    
    print("=" * 80)
    print("âœ… OPTIMISATIONS APPLIQUÃ‰ES AVEC SUCCÃˆS!")
    print("=" * 80)
    print()
    print("ðŸ“‹ GAINS ATTENDUS:")
    print("   - Cache embedding: -4s par requÃªte")
    print("   - N-grams optimisÃ©s: -10s par requÃªte")
    print("   - Cache prompt: -2s par requÃªte")
    print("   - Connection pooling: -2s par requÃªte")
    print("   - TOTAL: ~18s â†’ ~5s par requÃªte")
    print()
    print("ðŸš€ Le serveur peut maintenant Ãªtre redÃ©marrÃ© avec ces optimisations!")


if __name__ == "__main__":
    asyncio.run(main())
